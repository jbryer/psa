[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"first encountered propensity score analysis (PSA) late dissertation advisor Robert Pruzek 2006 entered graduate school. notion get reasonable causal estimates without need randomization foreign first, skeptical. Many years later used PSA many projects, convinced possible, believe instances may preferred randomized control trial. Principal Investigator two Federal grants develop test Diagnostic Assessment Achievement College Skills (DAACS) attempted conduct large scale randomized control trials (RCT) involving thousands students. experiences found numerous compromises made delivering intervention generalizability results context concerned RCT make interpretations difficult. Moreover, RCTs assume single effect everyone. PSA, particularly stratification section, possible tease intervention may vary observed covariates.taught PSA many times years. “book” attempt collect notes experiences conducting PSA. part emphasize applied provide links references reader wishes explore theoretical details. psa R package accompanies book available Github can installed using remotes package command . setting dependencies = TRUE parameter ensure R packages used book installed well. psa package contains number datasets utility functions used throughout book. also contains Shiny application designed conduct PSA using graphical user interface. Details using application provided appendix.books much work progress contributions welcome. Please adhere code conduct. page edit link take directly source file Github. also submit feedback using Github Issues tracker.","code":"\nremotes::install(build_vignettes = TRUE, dependencies = 'Enhances')"},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"website created using bookdown hosted Github pages.","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"","code":"\ndevtools::session_info()## ─ Session info ───────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.2.3 (2023-03-15)\n##  os       Ubuntu 22.04.2 LTS\n##  system   x86_64, linux-gnu\n##  ui       X11\n##  language (EN)\n##  collate  C.UTF-8\n##  ctype    C.UTF-8\n##  tz       UTC\n##  date     2023-04-01\n##  pandoc   2.19.2 @ /usr/bin/ (via rmarkdown)\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────\n##  ! package       * version  date (UTC) lib source\n##    abind           1.4-5    2016-07-21 [1] CRAN (R 4.2.3)\n##  P backports       1.4.1    2021-12-13 [?] RSPM (R 4.2.0)\n##    bookdown        0.33     2023-03-06 [1] CRAN (R 4.2.3)\n##  P boot            1.3-28.1 2022-11-22 [3] CRAN (R 4.2.3)\n##  P bslib           0.4.2    2022-12-16 [?] RSPM (R 4.2.0)\n##  P cachem          1.0.7    2023-02-24 [?] RSPM (R 4.2.0)\n##  P callr           3.7.3    2022-11-02 [?] RSPM (R 4.2.0)\n##    car           * 3.1-2    2023-03-30 [1] CRAN (R 4.2.3)\n##    carData       * 3.0-5    2022-01-06 [1] CRAN (R 4.2.3)\n##  P cli             3.6.1    2023-03-23 [?] RSPM (R 4.2.0)\n##  P codetools       0.2-19   2023-02-01 [3] CRAN (R 4.2.3)\n##    coin            1.4-2    2021-10-08 [1] CRAN (R 4.2.3)\n##  P colorspace      2.1-0    2023-01-23 [?] RSPM (R 4.2.0)\n##  P crayon          1.5.2    2022-09-29 [?] RSPM (R 4.2.0)\n##  P devtools        2.4.5    2022-10-11 [?] RSPM (R 4.2.0)\n##  P digest          0.6.31   2022-12-11 [?] RSPM (R 4.2.0)\n##  P downlit         0.4.2    2022-07-05 [?] RSPM (R 4.2.0)\n##  P dplyr           1.1.1    2023-03-22 [?] RSPM (R 4.2.0)\n##  P ellipsis        0.3.2    2021-04-29 [?] RSPM (R 4.2.0)\n##  P evaluate        0.20     2023-01-17 [?] RSPM (R 4.2.0)\n##    ez            * 4.4-0    2016-11-02 [1] CRAN (R 4.2.3)\n##  P fansi           1.0.4    2023-01-22 [?] RSPM (R 4.2.0)\n##  P fastmap         1.1.1    2023-02-24 [?] RSPM (R 4.2.0)\n##  P fs              1.6.1    2023-02-06 [?] RSPM (R 4.2.0)\n##  P generics        0.1.3    2022-07-05 [?] RSPM (R 4.2.0)\n##  P ggplot2       * 3.4.1    2023-02-10 [?] RSPM (R 4.2.0)\n##    ggthemes        4.2.4    2021-01-20 [1] CRAN (R 4.2.3)\n##  P glue            1.6.2    2022-02-24 [?] RSPM (R 4.2.0)\n##    granova       * 2.2      2023-03-22 [1] CRAN (R 4.2.3)\n##  P granovaGG     * 1.4.0    2015-12-18 [?] RSPM (R 4.2.0)\n##  P gridExtra       2.3      2017-09-09 [?] RSPM (R 4.2.0)\n##  P gtable          0.3.3    2023-03-21 [?] RSPM (R 4.2.0)\n##  P htmltools       0.5.5    2023-03-23 [?] RSPM (R 4.2.0)\n##  P htmlwidgets     1.6.2    2023-03-17 [?] RSPM (R 4.2.0)\n##  P httpuv          1.6.9    2023-02-14 [?] RSPM (R 4.2.0)\n##  P jquerylib       0.1.4    2021-04-26 [?] RSPM (R 4.2.0)\n##  P jsonlite        1.8.4    2022-12-06 [?] RSPM (R 4.2.0)\n##  P knitr         * 1.42     2023-01-25 [?] RSPM (R 4.2.0)\n##  P later           1.3.0    2021-08-18 [?] RSPM (R 4.2.0)\n##  P lattice         0.20-45  2021-09-22 [3] CRAN (R 4.2.3)\n##    libcoin         1.0-9    2021-09-27 [1] CRAN (R 4.2.3)\n##  P lifecycle       1.0.3    2022-10-07 [?] RSPM (R 4.2.0)\n##    lme4            1.1-32   2023-03-14 [1] CRAN (R 4.2.3)\n##  P magrittr        2.0.3    2022-03-30 [?] RSPM (R 4.2.0)\n##  P MASS          * 7.3-58.2 2023-01-23 [3] CRAN (R 4.2.3)\n##    Matching      * 4.10-8   2022-11-03 [1] CRAN (R 4.2.3)\n##    MatchIt       * 4.5.2    2023-03-22 [1] CRAN (R 4.2.3)\n##  P Matrix          1.5-3    2022-11-11 [3] CRAN (R 4.2.3)\n##    matrixStats     0.63.0   2022-11-18 [1] CRAN (R 4.2.3)\n##  P memoise         2.0.1    2021-11-26 [?] RSPM (R 4.2.0)\n##  P mgcv            1.8-42   2023-03-02 [3] CRAN (R 4.2.3)\n##  P mime            0.12     2021-09-28 [?] RSPM (R 4.2.0)\n##  P miniUI          0.1.1.1  2018-05-18 [?] RSPM (R 4.2.0)\n##    minqa           1.2.5    2022-10-19 [1] CRAN (R 4.2.3)\n##    mnormt          2.1.1    2022-09-26 [1] CRAN (R 4.2.3)\n##    modeltools      0.2-23   2020-03-05 [1] CRAN (R 4.2.3)\n##    multcomp        1.4-23   2023-03-09 [1] CRAN (R 4.2.3)\n##    multilevelPSA * 1.2.5    2018-03-22 [1] CRAN (R 4.2.3)\n##  P munsell         0.5.0    2018-06-12 [?] RSPM (R 4.2.0)\n##    mvtnorm         1.1-3    2021-10-08 [1] CRAN (R 4.2.3)\n##  P nlme            3.1-162  2023-01-31 [3] CRAN (R 4.2.3)\n##    nloptr          2.0.3    2022-05-26 [1] CRAN (R 4.2.3)\n##    party           1.3-13   2023-03-17 [1] CRAN (R 4.2.3)\n##  P pillar          1.9.0    2023-03-22 [?] RSPM (R 4.2.0)\n##  P pkgbuild        1.4.0    2022-11-27 [?] RSPM (R 4.2.0)\n##  P pkgconfig       2.0.3    2019-09-22 [?] RSPM (R 4.2.0)\n##  P pkgload         1.3.2    2022-11-16 [?] RSPM (R 4.2.0)\n##  P plyr            1.8.8    2022-11-11 [?] RSPM (R 4.2.0)\n##  P prettyunits     1.1.1    2020-01-24 [?] RSPM (R 4.2.0)\n##  P processx        3.8.0    2022-10-26 [?] RSPM (R 4.2.0)\n##  P profvis         0.3.7    2020-11-02 [?] RSPM (R 4.2.0)\n##  P promises        1.2.0.1  2021-02-11 [?] RSPM (R 4.2.0)\n##  P ps              1.7.3    2023-03-21 [?] RSPM (R 4.2.0)\n##    PSAboot       * 1.3.6    2023-03-22 [1] CRAN (R 4.2.3)\n##    PSAgraphics   * 2.1.2    2023-03-21 [1] CRAN (R 4.2.3)\n##    psych           2.3.3    2023-03-18 [1] CRAN (R 4.2.3)\n##  P purrr           1.0.1    2023-01-10 [?] RSPM (R 4.2.0)\n##  P R6              2.5.1    2021-08-19 [?] RSPM (R 4.2.0)\n##    randomForest    4.7-1.1  2022-05-23 [1] CRAN (R 4.2.3)\n##  P RColorBrewer    1.1-3    2022-04-03 [?] RSPM (R 4.2.0)\n##  P Rcpp            1.0.10   2023-01-22 [?] RSPM (R 4.2.0)\n##  P remotes         2.4.2    2021-11-30 [?] RSPM (R 4.2.0)\n##    reshape         0.8.9    2022-04-12 [1] CRAN (R 4.2.3)\n##  P reshape2      * 1.4.4    2020-04-09 [?] RSPM (R 4.2.0)\n##  P rlang           1.1.0    2023-03-14 [?] RSPM (R 4.2.0)\n##  P rmarkdown       2.21     2023-03-26 [?] RSPM (R 4.2.0)\n##  P rpart         * 4.1.19   2022-10-21 [3] CRAN (R 4.2.3)\n##    sandwich        3.0-2    2022-06-15 [1] CRAN (R 4.2.3)\n##  P sass            0.4.5    2023-01-24 [?] RSPM (R 4.2.0)\n##  P scales        * 1.2.1    2022-08-20 [?] RSPM (R 4.2.0)\n##  P sessioninfo     1.2.2    2021-12-06 [?] RSPM (R 4.2.0)\n##  P shiny           1.7.4    2022-12-15 [?] RSPM (R 4.2.0)\n##  P stringi         1.7.12   2023-01-11 [?] RSPM (R 4.2.0)\n##  P stringr         1.5.0    2022-12-02 [?] RSPM (R 4.2.0)\n##    strucchange     1.5-3    2022-06-15 [1] CRAN (R 4.2.3)\n##  P survival        3.5-3    2023-02-12 [3] CRAN (R 4.2.3)\n##    TH.data         1.1-1    2022-04-26 [1] CRAN (R 4.2.3)\n##  P tibble          3.2.1    2023-03-20 [?] RSPM (R 4.2.0)\n##  P tidyselect      1.2.0    2022-10-10 [?] RSPM (R 4.2.0)\n##    TriMatch      * 0.9.9    2017-12-06 [1] CRAN (R 4.2.3)\n##  P urlchecker      1.0.1    2021-11-30 [?] RSPM (R 4.2.0)\n##  P usethis         2.1.6    2022-05-25 [?] RSPM (R 4.2.0)\n##  P utf8            1.2.3    2023-01-31 [?] RSPM (R 4.2.0)\n##  P vctrs           0.6.1    2023-03-22 [?] RSPM (R 4.2.0)\n##  P withr           2.5.0    2022-03-03 [?] RSPM (R 4.2.0)\n##  P xfun            0.38     2023-03-24 [?] RSPM (R 4.2.0)\n##  P xml2            1.3.3    2021-11-30 [?] RSPM (R 4.2.0)\n##  P xtable        * 1.8-4    2019-04-21 [?] RSPM (R 4.2.0)\n##  P yaml            2.3.7    2023-01-23 [?] RSPM (R 4.2.0)\n##    zoo             1.8-11   2022-09-17 [1] CRAN (R 4.2.3)\n## \n##  [1] /home/runner/.cache/R/renv/library/psa-1b3136f9/R-4.2/x86_64-pc-linux-gnu\n##  [2] /home/runner/.cache/R/renv/sandbox/R-4.2/x86_64-pc-linux-gnu/e11edd0e\n##  [3] /opt/R/4.2.3/lib/R/library\n## \n##  P ── Loaded and on-disk path mismatch.\n## \n## ──────────────────────────────────────────────────────────────────────────────"},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"work Jason Bryer licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"use propensity score methods (Rosenbaum Rubin 1983) estimating causal effects observational studies certain kinds quasi-experiments increasing social sciences (Thoemmes Kim 2011) medical research (Austin 2008) last decade. Propensity score analysis (PSA) attempts adjust selection bias occurs due lack randomization. Analysis typically conducted two phases phase , probability placement treatment estimated identify matched pairs clusters phase II, comparisons dependent variable can made matched pairs within clusters. R (R Core Team 2023) ideal conducting PSA given wide availability current statistical methods vis-à-vis add-packages well superior graphics capabilities.book provide theoretical overview propensity score methods well illustrations discussion PSA applications. Methods used phase PSA (.e. models methods estimating propensity scores) include logistic regression, classification trees, matching. Discussions appropriate comparisons estimations effect size confidence intervals phase II also covered. use graphics diagnosing covariate balance well summarizing overall results emphasized. Lastly, extension PSA methods multilevel data also presented.\nFigure 1.1: PSA Citations per Year\n","code":""},{"path":"introduction.html","id":"counterfactual-model-for-causality","chapter":"1 Introduction","heading":"1.1 Counterfactual Model for Causality","text":"order understand propensity score analysis allows us make causal estimates observational data, must first understand basic principals causality, particulary counterfactual model. Figure 1.2 depicts conterfactual model. begin research subject. can student, patient, rat, asteroid, object wish know whether condition effect . Consider two parallel universes: one subject receives condition another receive condition B. Typically one condition treatment whereas condition absense treatment (also referred control). use treatment control throughout book refer two conditions. individual exposed two conditions, outcome measured. difference outcomes true causal effect. However, impossible object exist two universes time, therefore can never actually observe true causal effect. Holland (1986) referred Fundamental Problem Causal Inference.\nFigure 1.2: Causal Model\n","code":""},{"path":"introduction.html","id":"randomized-control-trials-the-gold-standard","chapter":"1 Introduction","heading":"1.2 Randomized Control Trials “The Gold Standard”","text":"randomized experiment goals standard estimating causal effects. Effects can estimated using simple means groups, blocks randomized block design. Randomization presumes unbiasedness balance groups. However, randomization often feasible many reasons, especially educational contexts.strong ignorability assumtion states outcome independent observed unobserved covariates randomization. represented mathematically :\\[\\left( { Y }_{ }\\left( 1 \\right) ,{ Y }_{ }\\left( 0 \\right)  \\right) \\bot { T }_{ }\\]\\({X}_{}\\)Therefore, follows causal effect treatment difference individual’s outcome situation given treatment (referred counterfactual).\\[{\\delta}_{} = { Y }_{ i1 }-{ Y }_{ i0 }\\]However, impossible directly observe \\({}_{}\\) (referred Fundamental Problem Causal Inference, Holland 1986). Rubin framed problem missing data problem.","code":""},{"path":"introduction.html","id":"rubins-causal-model","chapter":"1 Introduction","heading":"1.2.1 Rubin’s Causal Model","text":"","code":""},{"path":"introduction.html","id":"conceptual-steps-for-conducting-propensity-score-analysis","chapter":"1 Introduction","heading":"1.3 Conceptual Steps for Conducting Propensity Score Analysis","text":"","code":""},{"path":"introduction.html","id":"phase-i-modeling-for-selection-bias","chapter":"1 Introduction","heading":"1.3.1 Phase I: Modeling for Selection Bias","text":"","code":""},{"path":"introduction.html","id":"estimate-propensity-scores","chapter":"1 Introduction","heading":"1.3.1.1 Estimate Propensity Scores","text":"goal phase one PSA estimate probability treatment. Since cases binary outcome, logistic regressionis common approach estimating propensity scores.\\[ \\sigma =\\frac { { e }^{ t } }{ { e }^{ t }+1 } =\\frac { 1 }{ 1+{ e }^{ -1 } }  \\]\\[ t={ \\beta  }+{ \\beta  }_{ 1 }x+\\cdots +{ \\beta  }_{ k } \\]\\[ F\\left( x \\right) =\\frac { 1 }{ 1+{ e }^{ -\\left( { \\beta  }+{ \\beta  }_{ 1 }x+\\cdots +{ \\beta  }_{ k } \\right)  } } \\]Figure 1.3 depicts fitted logistic regression along sample matchs connected purple lines.1\nFigure 1.3: Propensity Scores Logistic Regression Sample Matched Pairs\n","code":""},{"path":"introduction.html","id":"evaluate-balance","chapter":"1 Introduction","heading":"1.3.1.2 Evaluate Balance","text":"","code":""},{"path":"introduction.html","id":"phase-ii-estimate-causal-effects","chapter":"1 Introduction","heading":"1.3.2 Phase II: Estimate Causal Effects","text":"","code":""},{"path":"introduction.html","id":"r-primer","chapter":"1 Introduction","heading":"1.4 R Primer","text":"R statistical software language designed extended vis-à-vis packages. April 01, 2023, currently 19,342 packages available CRAN. Given ease R can extended, become tool choice conducting propensity score analysis. book make use number packages matching, multiple imputation missing values, visualize results.MatchIt (Ho et al. 2023) Nonparametric Preprocessing Parametric Causal InferenceMatching (Singh Sekhon Saarinen 2022) Multivariate Propensity Score Matching Software Causal InferencemultilevelPSA (Bryer 2018) Multilevel Propensity Score Analysisparty (Hothorn et al. 2023) Laboratory Recursive PartytioningPSAboot (Bryer 2023) Bootstrapping Propensity Score AnalysisPSAgraphics (Helmreich Pruzek 2023) R Package Support Propensity Score Analysisrbounds (Keele 2022) Overview rebounds: R Package Rosenbaum bounds sensitivity analysis matched data.rpart (Therneau Atkinson 2022) Recursive PartitioningTriMatch (Bryer 2017) Propensity Score Matching Non-Binary TreatmentsThe following command install R packages use book.","code":"\npkgs <- c('granova', 'granovaGG', 'Matching', 'MatchIt', 'mice', \n          'multilevelPSA', 'party', 'PSAboot', 'PSAgraphics', 'rbounds', \n          'TriMatch')\ninstall.packages(pkgs)"},{"path":"introduction.html","id":"datasets","chapter":"1 Introduction","heading":"1.5 Datasets","text":"","code":""},{"path":"introduction.html","id":"lalonde","chapter":"1 Introduction","heading":"1.5.1 National Supported Work Demonstration","text":"lalonde dataset perhaps one used datasets introducing evaluating propensity score methods. data collected Lalonde (1986) became widely used PSA literature Dehejia Wahba (1999) used paper evaluate propensity score matching. dataset originated National Supported Work Demonstration study conducted 1970s. program provided 12 18 months employment people longstanding employment problems. dataset contains 445 observations 13 variables. primary outcome re78 real earnings 1978. Observed covariates used ajdust selection bias include age (age years), edu (number years education), black (black ), hisp (Hispanic ), married (married ), nodegr (whether worker degree , note 1 = degree), re74 (real earnings 1974), re75 (real earnings 1975).","code":"\ndata(lalonde, package='Matching')\nstr(lalonde)## 'data.frame':    445 obs. of  12 variables:\n##  $ age    : int  37 22 30 27 33 22 23 32 22 33 ...\n##  $ educ   : int  11 9 12 11 8 9 12 11 16 12 ...\n##  $ black  : int  1 0 1 1 1 1 1 1 1 0 ...\n##  $ hisp   : int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ married: int  1 0 0 0 0 0 0 0 0 1 ...\n##  $ nodegr : int  1 1 0 1 1 1 0 1 0 0 ...\n##  $ re74   : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ re75   : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ re78   : num  9930 3596 24910 7506 290 ...\n##  $ u74    : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ u75    : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ treat  : int  1 1 1 1 1 1 1 1 1 1 ..."},{"path":"introduction.html","id":"lindner","chapter":"1 Introduction","heading":"1.5.2 Lindner Center","text":"Data observational study 996 patients receiving PCI Ohio Heart Health 1997 followed least 6 months staff Lindner Center. landmark dataset literature propensity score adjustment treatment selection bias due practice evidence based medicine; patients receiving abciximab tended severely diseased receive IIb/IIIa cascade blocker.","code":"\ndata(lindner, package='PSAgraphics')\nstr(lindner)## 'data.frame':    996 obs. of  10 variables:\n##  $ lifepres: num  0 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 ...\n##  $ cardbill: int  14301 3563 4694 7366 8247 8319 8410 8517 8763 8823 ...\n##  $ abcix   : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ stent   : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ height  : int  163 168 188 175 168 178 185 173 152 180 ...\n##  $ female  : int  1 0 0 0 1 0 0 1 1 0 ...\n##  $ diabetic: int  1 0 0 1 0 0 0 0 0 0 ...\n##  $ acutemi : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ ejecfrac: int  56 56 50 50 55 50 58 30 60 60 ...\n##  $ ves1proc: int  1 1 1 1 1 1 1 1 1 1 ..."},{"path":"introduction.html","id":"tutoring","chapter":"1 Introduction","heading":"1.5.3 Tutoring","text":"tutoring dataset originates study conducted online adult serving institution examining effects tutoring services students English 101, English 201, History 310. Tutoring services available students Treatment (treat) operationalized students used tutoring services least course. 19.6% students used tutoring services approximately half using . use dataset dichotomous treatment (used tutoring ) two level treatment (used tutoring services , used tutoring services two times).","code":"\ndata(tutoring, package='TriMatch')\ntutoring$treat2 <- tutoring$treat != 'Control'\nstr(tutoring)## 'data.frame':    1142 obs. of  18 variables:\n##  $ treat     : Factor w/ 3 levels \"Control\",\"Treat1\",..: 1 1 1 1 1 2 1 1 1 1 ...\n##  $ Course    : chr  \"ENG*201\" \"ENG*201\" \"ENG*201\" \"ENG*201\" ...\n##  $ Grade     : int  4 4 4 4 4 3 4 3 0 4 ...\n##  $ Gender    : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Ethnicity : Factor w/ 3 levels \"Black\",\"Other\",..: 2 3 3 3 3 3 3 3 1 3 ...\n##  $ Military  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ ESL       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ EdMother  : int  3 5 1 3 2 3 4 4 3 6 ...\n##  $ EdFather  : int  6 6 1 5 2 3 4 4 2 6 ...\n##  $ Age       : num  48 49 53 52 47 53 54 54 59 40 ...\n##  $ Employment: int  3 3 1 3 1 3 3 3 1 3 ...\n##  $ Income    : num  9 9 5 5 5 9 6 6 1 8 ...\n##  $ Transfer  : num  24 25 39 48 23 ...\n##  $ GPA       : num  3 2.72 2.71 4 3.5 3.55 3.57 3.57 3.43 2.81 ...\n##  $ GradeCode : chr  \"A\" \"A\" \"A\" \"A\" ...\n##  $ Level     : Factor w/ 2 levels \"Lower\",\"Upper\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ ID        : int  377 882 292 215 252 265 1016 282 39 911 ...\n##  $ treat2    : logi  FALSE FALSE FALSE FALSE FALSE TRUE ...\ntable(tutoring$Course, tutoring$treat)##          \n##           Control Treat1 Treat2\n##   ENG*101     349     22     31\n##   ENG*201     518     36     32\n##   HSC*310      51     76     27"},{"path":"introduction.html","id":"pisa","chapter":"1 Introduction","heading":"1.5.4 Programme of International Student Assessment (PISA)","text":"","code":"\ndata(pisana, package='multilevelPSA')\nstr(pisana)## 'data.frame':    66548 obs. of  65 variables:\n##  $ Country : chr  \"Canada\" \"Canada\" \"Canada\" \"Canada\" ...\n##  $ CNT     : chr  \"CAN\" \"CAN\" \"CAN\" \"CAN\" ...\n##  $ SCHOOLID: Factor w/ 1534 levels \"00001\",\"00002\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST01Q01 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ...\n##  $ ST04Q01 : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 2 2 1 1 2 ...\n##  $ ST05Q01 : Factor w/ 3 levels \"No\",\"Yes, more than one year\",..: 2 2 3 2 1 1 3 3 2 3 ...\n##  $ ST06Q01 : num  4 4 4 4 5 5 5 4 4 5 ...\n##  $ ST07Q01 : Factor w/ 3 levels \"No, never\",\"Yes, once\",..: 1 1 1 1 1 1 2 1 1 1 ...\n##  $ ST08Q01 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST08Q02 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 1 2 2 2 ...\n##  $ ST08Q03 : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 1 1 1 2 2 2 ...\n##  $ ST08Q04 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 2 2 2 1 2 1 ...\n##  $ ST08Q05 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST08Q06 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST10Q01 : Factor w/ 5 levels \"<ISCED level 1>\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ ST12Q01 : Factor w/ 4 levels \"Looking for work\",..: 3 3 3 3 3 3 3 3 4 3 ...\n##  $ ST14Q01 : Factor w/ 5 levels \"<ISCED level 1>\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ ST16Q01 : Factor w/ 4 levels \"Looking for work\",..: 3 3 3 3 3 3 3 3 4 3 ...\n##  $ ST19Q01 : Factor w/ 2 levels \"Another language\",..: 2 2 2 1 2 2 2 2 2 1 ...\n##  $ ST20Q01 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q02 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q03 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q04 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q05 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 2 1 ...\n##  $ ST20Q06 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q07 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 1 1 1 1 1 ...\n##  $ ST20Q08 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 2 1 1 1 1 ...\n##  $ ST20Q09 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 1 1 2 ...\n##  $ ST20Q10 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 1 ...\n##  $ ST20Q12 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q13 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 2 2 2 2 2 2 ...\n##  $ ST20Q14 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST21Q01 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 3 3 3 3 4 4 3 3 3 ...\n##  $ ST21Q02 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 3 3 3 4 3 2 3 3 3 ...\n##  $ ST21Q03 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 2 2 3 4 4 4 4 3 3 ...\n##  $ ST21Q04 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 4 4 2 3 4 3 2 4 4 3 ...\n##  $ ST21Q05 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 2 4 4 3 3 4 2 4 4 3 ...\n##  $ ST22Q01 : Factor w/ 6 levels \"0-10 books\",\"101-200 books\",..: 5 5 1 5 4 3 2 5 4 1 ...\n##  $ ST23Q01 : Factor w/ 5 levels \"1 to 2 hours a day\",..: 5 2 4 2 4 4 3 4 3 4 ...\n##  $ ST31Q01 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST31Q02 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 2 ...\n##  $ ST31Q03 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST31Q05 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n##  $ ST31Q06 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n##  $ ST31Q07 : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 1 1 1 1 2 1 ...\n##  $ ST32Q01 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 4 4 5 4 4 4 4 4 4 4 ...\n##  $ ST32Q02 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 5 4 5 4 4 4 4 4 4 5 ...\n##  $ ST32Q03 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 4 4 5 4 4 4 4 4 4 5 ...\n##  $ PV1MATH : num  474 673 348 518 420 ...\n##  $ PV2MATH : num  466 632 372 537 533 ...\n##  $ PV3MATH : num  438 571 397 511 441 ...\n##  $ PV4MATH : num  458 685 445 527 468 ...\n##  $ PV5MATH : num  471 586 375 490 420 ...\n##  $ PV1READ : num  503 613 390 570 407 ...\n##  $ PV2READ : num  492 578 421 542 452 ...\n##  $ PV3READ : num  522 553 442 527 434 ...\n##  $ PV4READ : num  509 594 410 584 423 ...\n##  $ PV5READ : num  460 564 372 518 416 ...\n##  $ PV1SCIE : num  460 686 378 500 417 ...\n##  $ PV2SCIE : num  444 589 399 575 556 ...\n##  $ PV3SCIE : num  484 593 388 505 471 ...\n##  $ PV4SCIE : num  489 643 444 540 436 ...\n##  $ PV5SCIE : num  435 646 385 476 479 ...\n##  $ PUBPRIV : Factor w/ 2 levels \"Private\",\"Public\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ STRATIO : num  14.4 14.4 14.4 14.4 14.4 ..."},{"path":"introduction.html","id":"nmes","chapter":"1 Introduction","heading":"1.5.5 National Medical Expenditure Study","text":"","code":"\ndata(nmes, package='TriMatch')\nstr(nmes)## 'data.frame':    20622 obs. of  29 variables:\n##  $ PIDX     : int  20843014 20836012 20836025 20835019 20832010 20829011 20825018 20821016 20819010 20819023 ...\n##  $ LASTAGE  : int  80 29 28 29 80 71 60 82 32 33 ...\n##  $ MALE     : int  0 0 1 0 1 1 0 0 1 0 ...\n##  $ RACE3    : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 3 3 3 3 3 3 3 3 ...\n##  $ eversmk  : int  0 1 1 1 1 1 1 1 1 0 ...\n##  $ current  : int  NA 1 1 1 1 0 0 0 0 NA ...\n##  $ former   : int  0 0 0 0 0 1 1 1 1 0 ...\n##  $ smoke    : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 2 2 2 2 3 3 3 3 1 ...\n##  $ AGESMOKE : int  NA 15 16 17 21 16 30 16 12 NA ...\n##  $ CIGSSMOK : int  NA 20 18 NA 10 NA 20 2 25 NA ...\n##  $ SMOKENOW : int  NA 1 1 1 1 2 2 2 2 NA ...\n##  $ SMOKED   : int  2 1 1 1 1 1 1 1 1 2 ...\n##  $ CIGSADAY : int  NA 35 19 NA 10 NA NA NA NA NA ...\n##  $ AGESTOP  : int  NA NA NA NA NA 46 38 35 29 NA ...\n##  $ packyears: num  0 15 11.7 NA 30 NA 9 2 22.5 0 ...\n##  $ yearsince: int  0 0 0 0 0 25 22 47 3 0 ...\n##  $ INCALPER : num  5806 9952 10839 13990 8221 ...\n##  $ HSQACCWT : num  6933 10607 11500 14766 9512 ...\n##  $ TOTALEXP : num  298 284 177 750 1369 ...\n##  $ TOTALSP3 : num  20 0 0 0 0 ...\n##  $ lc5      : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ chd5     : int  1 0 0 0 1 0 0 0 0 0 ...\n##  $ beltuse  : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 2 2 1 3 3 3 3 3 ...\n##  $ educate  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ marital  : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 1 1 5 3 1 5 5 1 1 ...\n##  $ SREGION  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ POVSTALB : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 3 3 4 3 5 5 4 5 5 ...\n##  $ flag     : int  0 0 0 2 0 2 0 0 0 0 ...\n##  $ age      : int  1 0 0 0 1 1 1 1 0 0 ..."},{"path":"stratification.html","id":"stratification","chapter":"2 Stratification","heading":"2 Stratification","text":"tutoring example treatment three levels: Treat1, Treat2, Control. ’ll convert two level treatment example.","code":"\ndata(lalonde, package = 'Matching')"},{"path":"stratification.html","id":"estimating-propensity-scores","chapter":"2 Stratification","heading":"2.1 Estimating Propensity Scores","text":"Check distributions propensity scores ensure good overlap","code":"\nlalonde.formu <- treat ~ age + educ + black + hisp + married + nodegr + re74 + re75\nlalonde.glm <- glm(lalonde.formu, \n                   family = binomial(link = 'logit'), \n                   data = lalonde)\nsummary(lalonde.glm)## \n## Call:\n## glm(formula = lalonde.formu, family = binomial(link = \"logit\"), \n##     data = lalonde)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4358  -0.9904  -0.9071   1.2825   1.6946  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  1.178e+00  1.056e+00   1.115  0.26474   \n## age          4.698e-03  1.433e-02   0.328  0.74297   \n## educ        -7.124e-02  7.173e-02  -0.993  0.32061   \n## black       -2.247e-01  3.655e-01  -0.615  0.53874   \n## hisp        -8.528e-01  5.066e-01  -1.683  0.09228 . \n## married      1.636e-01  2.769e-01   0.591  0.55463   \n## nodegr      -9.035e-01  3.135e-01  -2.882  0.00395 **\n## re74        -3.161e-05  2.584e-05  -1.223  0.22122   \n## re75         6.161e-05  4.358e-05   1.414  0.15744   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 587.22  on 436  degrees of freedom\n## AIC: 605.22\n## \n## Number of Fisher Scoring iterations: 4\nlalonde$ps <- fitted(lalonde.glm)\nggplot(lalonde, aes(x = ps, color = as.logical(treat))) + \n    geom_density() +\n    scale_color_brewer('Treatment', type = 'qual', palette = 6)"},{"path":"stratification.html","id":"stratifying","chapter":"2 Stratification","heading":"2.2 Stratifying","text":"Stratification using quintiles.","code":"\nbreaks <- quantile(lalonde$ps, seq(0, 1, 1/5))\nlalonde$strata <- cut(x = lalonde$ps, \n                      breaks = breaks, \n                      include.lowest = TRUE, \n                      labels = letters[1:(length(breaks) - 1)])\ntable(lalonde$strata, useNA = 'ifany')## \n##  a  b  c  d  e \n## 89 89 89 89 89\ntable(lalonde$treat, lalonde$strata, useNA = 'ifany')##    \n##      a  b  c  d  e\n##   0 62 59 56 42 41\n##   1 27 30 33 47 48"},{"path":"stratification.html","id":"checking-balance","chapter":"2 Stratification","heading":"2.3 Checking Balance","text":"","code":"\ncovars <- all.vars(lalonde.formu)\ncovars <- lalonde[,covars[2:length(covars)]]\nPSAgraphics::cv.bal.psa(covariates = covars, \n                        treatment = lalonde$treat,\n                        propensity = lalonde$ps,\n                        strata = lalonde$strata)\nPSAgraphics::box.psa(continuous = lalonde$age, \n                     treatment = lalonde$treat, \n                     strata = lalonde$strata,\n                     xlab = \"Strata\", \n                     balance = FALSE)\nPSAgraphics::cat.psa(categorical = lalonde$nodegr, \n                     treatment = lalonde$treat, \n                     strata = lalonde$strata, \n                     xlab = 'Strata',\n                     balance = FALSE)"},{"path":"stratification.html","id":"estimate-effects","chapter":"2 Stratification","heading":"2.4 Estimate Effects","text":"","code":"\npsa::loess.plot(x = lalonde$ps,\n                response = log(lalonde$re78 + 1),\n                treatment = lalonde$treat == 1)\nPSAgraphics::circ.psa(response = log(lalonde$re78 + 1), \n                      treatment = lalonde$treat == 1, \n                      strata = lalonde$strata, \n                      revc = TRUE)## $summary.strata\n##   n.FALSE n.TRUE means.FALSE means.TRUE\n## a      62     27    5.716641   7.254306\n## b      59     30    5.049745   5.283601\n## c      56     33    5.737915   5.415335\n## d      42     47    5.761898   6.525840\n## e      41     48    5.211647   7.583088\n## \n## $wtd.Mn.TRUE\n## [1] 6.412434\n## \n## $wtd.Mn.FALSE\n## [1] 5.495569\n## \n## $ATE\n## [1] -0.9168647\n## \n## $se.wtd\n## [1] 0.3879182\n## \n## $approx.t\n## [1] -2.363551\n## \n## $df\n## [1] 435\n## \n## $CI.95\n## [1] -1.6792917 -0.1544376"},{"path":"matching.html","id":"matching","chapter":"3 Matching","heading":"3 Matching","text":"","code":""},{"path":"weighting.html","id":"weighting","chapter":"4 Weighting","heading":"4 Weighting","text":"","code":""},{"path":"missing-data.html","id":"missing-data","chapter":"5 Missing Data","heading":"5 Missing Data","text":"Create copy covariates simulate missing random (mar) missing random (nmar).Add missingness existing data. missing random data treatment units twice many missing values control group.proportion missing values first covariateCreate shadow matrix. logical vector cell TRUE value missing original data frame.Change column names include “_miss” name.Impute missing values using mice packageGet imputed data set.Estimate propensity scores using logistic regression.see two indicator columns shadow matrix statistically significant predictors suggesting data missing random.","code":"\nrequire(Matching)\nrequire(mice)\ndata(lalonde, package='Matching')\nTr <- lalonde$treat\nY <- lalonde$re78\nX <- lalonde[,c('age','educ','black','hisp','married','nodegr','re74','re75')]\nlalonde.glm <- glm(treat ~ ., family=binomial, data=cbind(treat=Tr, X))\nsummary(lalonde.glm)## \n## Call:\n## glm(formula = treat ~ ., family = binomial, data = cbind(treat = Tr, \n##     X))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4358  -0.9904  -0.9071   1.2825   1.6946  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  1.178e+00  1.056e+00   1.115  0.26474   \n## age          4.698e-03  1.433e-02   0.328  0.74297   \n## educ        -7.124e-02  7.173e-02  -0.993  0.32061   \n## black       -2.247e-01  3.655e-01  -0.615  0.53874   \n## hisp        -8.528e-01  5.066e-01  -1.683  0.09228 . \n## married      1.636e-01  2.769e-01   0.591  0.55463   \n## nodegr      -9.035e-01  3.135e-01  -2.882  0.00395 **\n## re74        -3.161e-05  2.584e-05  -1.223  0.22122   \n## re75         6.161e-05  4.358e-05   1.414  0.15744   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 587.22  on 436  degrees of freedom\n## AIC: 605.22\n## \n## Number of Fisher Scoring iterations: 4\nlalonde.mar <- X\nlalonde.nmar <- X\n\nmissing.rate <- .2 # What percent of rows will have missing data\nmissing.cols <- c('nodegr', 're75') # The columns we will add missing values to\n\n# Vectors indiciating which rows are treatment and control.\ntreat.rows <- which(lalonde$treat == 1)\ncontrol.rows <- which(lalonde$treat == 0)\nset.seed(2112)\nfor(i in missing.cols) {\n    lalonde.mar[sample(nrow(lalonde), nrow(lalonde) * missing.rate), i] <- NA\n    lalonde.nmar[sample(treat.rows, length(treat.rows) * missing.rate * 2), i] <- NA\n    lalonde.nmar[sample(control.rows, length(control.rows) * missing.rate), i] <- NA\n}\nprop.table(table(is.na(lalonde.mar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                  0          1\n##   FALSE 0.46292135 0.33707865\n##   TRUE  0.12134831 0.07865169\nprop.table(table(is.na(lalonde.nmar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                 0         1\n##   FALSE 0.4674157 0.2494382\n##   TRUE  0.1168539 0.1662921\nshadow.matrix.mar <- as.data.frame(is.na(lalonde.mar))\nshadow.matrix.nmar <- as.data.frame(is.na(lalonde.nmar))\nnames(shadow.matrix.mar) <- names(shadow.matrix.nmar) <- paste0(names(shadow.matrix.mar), '_miss')\nset.seed(2112)\nmice.mar <- mice(lalonde.mar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\nmice.nmar <- mice(lalonde.nmar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\ncomplete.mar <- complete(mice.mar)\ncomplete.nmar <- complete(mice.nmar)\nlalonde.mar.glm <- glm(treat~., data=cbind(treat=Tr, complete.mar, shadow.matrix.mar))\nlalonde.nmar.glm <- glm(treat~., data=cbind(treat=Tr, complete.nmar, shadow.matrix.nmar))\nsummary(lalonde.mar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.mar, \n##     shadow.matrix.mar))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7073  -0.3837  -0.3079   0.5404   0.7881  \n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       8.996e-01  2.504e-01   3.592 0.000366 ***\n## age               9.447e-04  3.395e-03   0.278 0.780957    \n## educ             -2.514e-02  1.706e-02  -1.474 0.141191    \n## black            -3.895e-02  8.746e-02  -0.445 0.656285    \n## hisp             -1.726e-01  1.156e-01  -1.493 0.136068    \n## married           3.008e-02  6.671e-02   0.451 0.652326    \n## nodegr           -2.672e-01  7.475e-02  -3.574 0.000390 ***\n## re74             -1.059e-05  5.681e-06  -1.863 0.063076 .  \n## re75              2.378e-05  1.059e-05   2.246 0.025227 *  \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE  -1.852e-02  5.853e-02  -0.316 0.751797    \n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE    -3.304e-02  5.870e-02  -0.563 0.573823    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2361624)\n## \n##     Null deviance: 108.09  on 444  degrees of freedom\n## Residual deviance: 102.49  on 434  degrees of freedom\n## AIC: 633.48\n## \n## Number of Fisher Scoring iterations: 2\nsummary(lalonde.nmar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.nmar, \n##     shadow.matrix.nmar))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7597  -0.3960  -0.2154   0.4926   0.8693  \n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       7.427e-01  2.319e-01   3.203 0.001459 ** \n## age               7.641e-04  3.254e-03   0.235 0.814441    \n## educ             -2.451e-02  1.584e-02  -1.547 0.122656    \n## black            -1.964e-02  8.493e-02  -0.231 0.817243    \n## hisp             -1.366e-01  1.113e-01  -1.228 0.220246    \n## married           4.426e-02  6.440e-02   0.687 0.492303    \n## nodegr           -2.572e-01  7.143e-02  -3.601 0.000354 ***\n## re74             -3.326e-06  5.324e-06  -0.625 0.532421    \n## re75              4.742e-06  9.893e-06   0.479 0.631935    \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE   2.300e-01  4.957e-02   4.639 4.64e-06 ***\n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE     2.246e-01  4.922e-02   4.563 6.57e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2164914)\n## \n##     Null deviance: 108.090  on 444  degrees of freedom\n## Residual deviance:  93.957  on 434  degrees of freedom\n## AIC: 594.78\n## \n## Number of Fisher Scoring iterations: 2"},{"path":"sensitivity-analysis.html","id":"sensitivity-analysis","chapter":"6 Sensitivity Analysis","heading":"6 Sensitivity Analysis","text":"","code":"\nrequire(rbounds)\ndata(lalonde, package='Matching')\n\nY  <- lalonde$re78   #the outcome of interest\nTr <- lalonde$treat #the treatment of interest\nattach(lalonde)\n# The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n# The covariates we want to obtain balance on\nBalanceMat <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74,\n                    I(re74*re75))\ndetach(lalonde)\n\ngen1 <- GenMatch(Tr=Tr, X=X, BalanceMat=BalanceMat, pop.size=50,\n                  data.type.int=FALSE, print=0, replace=FALSE)\nmgen1 <- Match(Y=Y, Tr=Tr, X=X, Weight.matrix=gen1, replace=FALSE)\nsummary(mgen1)## \n## Estimate...  1613.6 \n## SE.........  721.22 \n## T-stat.....  2.2373 \n## p.val......  0.025266 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185\nrbounds::psens(x = Y[mgen1$index.treated],\n      y =Y[mgen1$index.contro],\n      Gamma = 1.5, \n      GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n##  \n## Unconfounded estimate ....  0.0228 \n## \n##  Gamma Lower bound Upper bound\n##    1.0      0.0228      0.0228\n##    1.1      0.0056      0.0716\n##    1.2      0.0012      0.1640\n##    1.3      0.0002      0.2970\n##    1.4      0.0000      0.4516\n##    1.5      0.0000      0.6030\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## \nrbounds::hlsens(x = Y[mgen1$index.treated],\n       y = Y[mgen1$index.contro],\n       Gamma = 1.5, \n       GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n##  \n## Unconfounded estimate ....  1431.4 \n## \n##  Gamma Lower bound Upper bound\n##    1.0  1.4314e+03      1431.4\n##    1.1  7.9320e+02      1547.1\n##    1.2  4.9780e+02      1901.1\n##    1.3  2.0850e+02      2162.1\n##    1.4 -3.4140e-05      2441.0\n##    1.5 -2.1990e+02      2694.4\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## "},{"path":"bootstrapping.html","id":"bootstrapping","chapter":"7 Bootstrapping","heading":"7 Bootstrapping","text":"","code":""},{"path":"non-binary-treatments.html","id":"non-binary-treatments","chapter":"8 Non-Binary Treatments","heading":"8 Non-Binary Treatments","text":"","code":"\nrequire(TriMatch)\n\ndata(tutoring)\nstr(tutoring)## 'data.frame':    1142 obs. of  17 variables:\n##  $ treat     : Factor w/ 3 levels \"Control\",\"Treat1\",..: 1 1 1 1 1 2 1 1 1 1 ...\n##  $ Course    : chr  \"ENG*201\" \"ENG*201\" \"ENG*201\" \"ENG*201\" ...\n##  $ Grade     : int  4 4 4 4 4 3 4 3 0 4 ...\n##  $ Gender    : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Ethnicity : Factor w/ 3 levels \"Black\",\"Other\",..: 2 3 3 3 3 3 3 3 1 3 ...\n##  $ Military  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ ESL       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ EdMother  : int  3 5 1 3 2 3 4 4 3 6 ...\n##  $ EdFather  : int  6 6 1 5 2 3 4 4 2 6 ...\n##  $ Age       : num  48 49 53 52 47 53 54 54 59 40 ...\n##  $ Employment: int  3 3 1 3 1 3 3 3 1 3 ...\n##  $ Income    : num  9 9 5 5 5 9 6 6 1 8 ...\n##  $ Transfer  : num  24 25 39 48 23 ...\n##  $ GPA       : num  3 2.72 2.71 4 3.5 3.55 3.57 3.57 3.43 2.81 ...\n##  $ GradeCode : chr  \"A\" \"A\" \"A\" \"A\" ...\n##  $ Level     : Factor w/ 2 levels \"Lower\",\"Upper\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ ID        : int  377 882 292 215 252 265 1016 282 39 911 ...\ntable(tutoring$treat)## \n## Control  Treat1  Treat2 \n##     918     134      90\n# Histogram of unadjusted grades\ntmp <- as.data.frame(prop.table(table(tutoring$treat, tutoring$Grade), 1))\nggplot(tmp, aes(x=Var2, y=Freq, fill=Var1)) + \n    geom_bar(position='dodge', stat='identity') +\n    scale_y_continuous(labels = percent_format()) +\n    xlab('Grade') + ylab('Percent') + scale_colour_hue('Treatment')\n## Phase I\n\n# Note that the dependent variable is not included in the formula. The TriMatch\n# functions will replace the dependent variable depending on which pair is\n# being modeled.\ntutoring.formu <- ~ Gender + Ethnicity + Military + ESL + EdMother + EdFather + \n    Age + Employment + Income + Transfer + GPA\n\n# trips will estimate the propensity scores for each pairing of groups\ntutoring.tpsa <- trips(tutoring, tutoring$treat, tutoring.formu)\n\nplot(tutoring.tpsa, sample=c(200))\n# trimatch finds matched triplets.\ntutoring.matched <- trimatch(tutoring.tpsa)\n\n# Partial exact matching\ntutoring.matched2 <- trimatch(tutoring.tpsa, exact=tutoring$Level)\n\n# Plotting the results of trimatch is a subset of the triangle plot with only\n# points that were matched. There is also an additional parameter, rows, that\n# will overlay matched triplets.\nplot(tutoring.matched, rows=1, line.alpha=1, draw.segments=TRUE)\n## Examine the unmatched students\nunmatched <- unmatched(tutoring.matched)\nsummary(unmatched)## 819 (71.7%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 795 (86.6%)  17 (12.7%)   7 (7.78%)\nplot(unmatched)\n## Check balance\nmultibalance.plot(tutoring.tpsa)\nbalance.plot(tutoring.matched, tutoring$Age, label='Age')## \n##  Friedman rank sum test\n## \n## data:  Covariate and Treatment and ID\n## Friedman chi-squared = 4.1498, df = 2, p-value = 0.1256\n## \n##  Repeated measures ANOVA\n## \n##      Effect DFn DFd        F         p p<.05         ges\n## 2 Treatment   2 294 1.707234 0.1831598       0.006613137\nbalance.plot(tutoring.matched, tutoring$Military, label='Military')## \n##  Friedman rank sum test\n## \n## data:  Covariate and Treatment and ID\n## Friedman chi-squared = 0.4, df = 2, p-value = 0.8187\n# Create a grid of figures.\nbplots <- balance.plot(tutoring.matched, tutoring[,all.vars(tutoring.formu)], \n                       legend.position='none', \n                       x.axis.labels=c('C','T1','T1'), x.axis.angle=0)\nbplots[['Military']] # We can plot one at at time.\nsummary(bplots) # Create a data frame with the statistical results##     Covariate   Friedman Friedman.p Friedman.sig    rmANOVA rmANOVA.p\n## 1      Gender 2.16666667 0.33846543                      NA        NA\n## 2   Ethnicity 0.05678233 0.97200807                      NA        NA\n## 3    Military 0.40000000 0.81873075                      NA        NA\n## 4         ESL 4.78571429 0.09136826            .         NA        NA\n## 5    EdMother 1.55974843 0.45846368              0.76509335 0.4662146\n## 6    EdFather 0.02794411 0.98612510              0.06102055 0.9408158\n## 7         Age 4.14982578 0.12556736              1.70723419 0.1831598\n## 8  Employment 2.04048583 0.36050736              1.27194067 0.2818249\n## 9      Income 0.59582543 0.74236614              0.39251642 0.6757086\n## 10   Transfer 3.08717949 0.21361291              0.55080160 0.5770812\n## 11        GPA 1.37542662 0.50272433              0.49589373 0.6095348\n##    rmANOVA.sig\n## 1         <NA>\n## 2         <NA>\n## 3         <NA>\n## 4         <NA>\n## 5             \n## 6             \n## 7             \n## 8             \n## 9             \n## 10            \n## 11\nplot(bplots, cols=3, byrow=FALSE)\n## Phase II\n# The summary function performs a number of statistical tests including Friedman\n# rank sum test, repeated measures ANOVA, and if one or both of those tests have\n# p values less than 0.5 (the default, but configurable), then a pairwise Wilcox\n# test and three paired t-tests will also be performed.\n(sout <- summary(tutoring.matched, tutoring$Grade))## $PercentMatched\n##   Control    Treat1    Treat2 \n## 0.1339869 0.8731343 0.9222222 \n## \n## $friedman.test\n## \n##  Friedman rank sum test\n## \n## data:  Outcome and Treatment and ID\n## Friedman chi-squared = 17.404, df = 2, p-value = 0.0001663\n## \n## \n## $rmanova\n## $rmanova$ANOVA\n##      Effect DFn DFd        F            p p<.05        ges\n## 2 Treatment   2 294 16.66293 1.396209e-07     * 0.06818487\n## \n## $rmanova$`Mauchly's Test for Sphericity`\n##      Effect         W            p p<.05\n## 2 Treatment 0.8668353 2.946934e-05     *\n## \n## $rmanova$`Sphericity Corrections`\n##      Effect       GGe        p[GG] p[GG]<.05       HFe        p[HF] p[HF]<.05\n## 2 Treatment 0.8824842 6.035469e-07         * 0.8923995 5.333417e-07         *\n## \n## \n## $pairwise.wilcox.test\n## \n##  Pairwise comparisons using Wilcoxon signed rank test with continuity correction \n## \n## data:  out$Outcome and out$Treatment \n## \n##             Treat1.out Treat2.out\n## Treat2.out  0.0046     -         \n## Control.out 0.0165     1.9e-06   \n## \n## P value adjustment method: bonferroni \n## \n## $t.tests\n##               Treatments         t  df      p.value sig  mean.diff     ci.min\n## 1  Treat1.out-Treat2.out -3.095689 147 2.351743e-03  ** -0.3378378 -0.5535076\n## 2 Treat1.out-Control.out  2.939953 147 3.813865e-03  **  0.4459459  0.1461816\n## 3 Treat2.out-Control.out  5.443253 147 2.140672e-07 ***  0.7837838  0.4992224\n##       ci.max\n## 1 -0.1221681\n## 2  0.7457103\n## 3  1.0683452\n## \n## attr(,\"class\")\n## [1] \"trimatch.summary\" \"list\"\nls(sout)## [1] \"friedman.test\"        \"pairwise.wilcox.test\" \"PercentMatched\"      \n## [4] \"rmanova\"              \"t.tests\"\n# TODO: boxdiff.plot(tutoring.matched, tutoring$Grade, ordering=c('Treatment2','Treatment1','Control'))\nparallel.plot(tutoring.matched, tutoring$Grade)\n# The Loess plot is imperfect with three sets of propensity scores. There is a\n# model parameter to specify which model to use. Once we a model is selected\n# we have propensity scores for two of the three groups. We impute a propensity\n# score on that model's scale for the third group as the midpoint between\n# the other two propensity scores that unit was matched to.\nloess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method='loess')\n# Turn on 95% confidence interval (see also the level parameter)\nloess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method='loess')\n# We can also pass other parameters to the loess function.\nloess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method='loess', span=1)\n# This is a busy plot, but since all the lines are practically vertical, the\n# distance between each pair of propensity scores is minimal.\nloess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method='loess', \n            plot.connections=TRUE)\n# The merge function will add the outcome to the matched triplet data frame.\n# This is useful for other approaches to analyzing the matched triplets.\ntmatch.out <- merge(tutoring.matched, tutoring$Grade)\nhead(tmatch.out)##   Treat1 Treat2 Control        D.m3        D.m2         D.m1     Dtotal\n## 1    368     39     331 0.007053754 0.001788577 0.0103932229 0.01923555\n## 2    800   1088    1105 0.018477707 0.000736057 0.0001821526 0.01939592\n## 3    286    655     853 0.016859948 0.004237243 0.0019476652 0.02304486\n## 4    158    279     365 0.003373585 0.009530680 0.0107118774 0.02361614\n## 5    899    209     100 0.001929173 0.013633300 0.0091835718 0.02474604\n## 6   1034    791     484 0.010538949 0.008541671 0.0092350273 0.02831565\n##   Treat1.out Treat2.out Control.out\n## 1          4          4           0\n## 2          4          4           3\n## 3          2          4           4\n## 4          4          4           4\n## 5          4          3           4\n## 6          4          4           4"},{"path":"multilevel-psa.html","id":"multilevel-psa","chapter":"9 Multilevel PSA","heading":"9 Multilevel PSA","text":"","code":""},{"path":"psa_shiny.html","id":"psa_shiny","chapter":"A Shiny Application","heading":"A Shiny Application","text":"\nFigure .1: PSA Shiny Application\n","code":"\nlibrary(psa)\npsa_shiny()"},{"path":"propensity-score-ranges.html","id":"propensity-score-ranges","chapter":"B Propensity Score Ranges","heading":"B Propensity Score Ranges","text":"function create data frame three variables (, b, c) two groups.1:10 (100 treatments, 1000 control units)1:20 (100 treatments, 2000 control units)100 treatments, 1000 control units, equal means standard deviations100 treatments, 1000 control units, little overlap100 treat, 1000 control, 10 covariates","code":"\nlibrary(multilevelPSA)\ngetSimulatedData <- function(nvars = 3, ntreat = 100, treat.mean = 0.6, treat.sd = 0.5, \n    ncontrol = 1000, control.mean = 0.4, control.sd = 0.5) {\n    if (length(treat.mean) == 1) {\n        treat.mean = rep(treat.mean, nvars)\n    }\n    if (length(treat.sd) == 1) {\n        treat.sd = rep(treat.sd, nvars)\n    }\n    if (length(control.mean) == 1) {\n        control.mean = rep(control.mean, nvars)\n    }\n    if (length(control.sd) == 1) {\n        control.sd = rep(control.sd, nvars)\n    }\n    \n    df <- c(rep(0, ncontrol), rep(1, ntreat))\n    for (i in 1:nvars) {\n        df <- cbind(df, c(rnorm(ncontrol, mean = control.mean[1], sd = control.sd[1]), \n            rnorm(ntreat, mean = treat.mean[1], sd = treat.sd[1])))\n    }\n    df <- as.data.frame(df)\n    names(df) <- c(\"treat\", letters[1:nvars])\n    return(df)\n}\ntest.df1 <- getSimulatedData(ntreat = 100, ncontrol = 1000)\npsranges1 <- psrange(test.df1, test.df1$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges1)\nsummary(psranges1)##       p ntreat ncontrol ratio   min.mean       min.sd min.median       min.se\n## 1    10    100      100     1 0.15158085 0.0399247330 0.16675914 0.0089274417\n## 21   20    100      200     2 0.08065862 0.0110939596 0.07987970 0.0024806848\n## 41   30    100      300     3 0.05825915 0.0089598303 0.06117074 0.0020034790\n## 61   40    100      400     4 0.03966169 0.0043536309 0.03993927 0.0009735015\n## 81   50    100      500     5 0.03028191 0.0038363450 0.02969747 0.0008578328\n## 101  60    100      600     6 0.02581532 0.0024130983 0.02594087 0.0005395852\n## 121  70    100      700     7 0.02203176 0.0019850490 0.02179919 0.0004438705\n## 141  80    100      800     8 0.01903728 0.0013822504 0.01864631 0.0003090806\n## 161  90    100      900     9 0.01631016 0.0005450121 0.01627551 0.0001218684\n## 181 100    100     1000    10 0.01453930 0.0000000000 0.01453930 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.06536710 0.19336279 0.8598281 0.04676196  0.8501328 0.010456291 0.7861258\n## 21  0.05791970 0.10233899 0.7270197 0.04065439  0.7303578 0.009090597 0.6439753\n## 41  0.03669016 0.07242601 0.6050233 0.05144961  0.5948197 0.011504482 0.5386225\n## 61  0.03200825 0.05010319 0.5606233 0.02683686  0.5606224 0.006000904 0.5089432\n## 81  0.02357618 0.03609465 0.5058454 0.02833751  0.4987880 0.006336460 0.4682137\n## 101 0.02033462 0.02954242 0.4593096 0.02170712  0.4544798 0.004853860 0.4236357\n## 121 0.01906877 0.02678785 0.4255822 0.02072414  0.4292869 0.004634059 0.3811220\n## 141 0.01719583 0.02208829 0.3928887 0.01439171  0.3941605 0.003218084 0.3591961\n## 161 0.01526377 0.01767972 0.3577524 0.01054775  0.3544386 0.002358548 0.3458710\n## 181 0.01453930 0.01453930 0.3306953 0.00000000  0.3306953 0.000000000 0.3306953\n##       max.max\n## 1   0.9361766\n## 21  0.8123514\n## 41  0.7304035\n## 61  0.6127437\n## 81  0.5618276\n## 101 0.5129718\n## 121 0.4548082\n## 141 0.4260502\n## 161 0.3798020\n## 181 0.3306953\ntest.df2 <- getSimulatedData(ncontrol = 2000)\npsranges2 <- psrange(test.df2, test.df2$treat, treat ~ ., samples = seq(100, \n    2000, by = 100), nboot = 20)\nplot(psranges2)\nsummary(psranges2)##       p ntreat ncontrol ratio    min.mean       min.sd  min.median       min.se\n## 1     5    100      100     1 0.108479153 3.478953e-02 0.113600581 7.779176e-03\n## 21   10    100      200     2 0.046371411 1.434092e-02 0.044963287 3.206727e-03\n## 41   15    100      300     3 0.031643058 8.312285e-03 0.030554662 1.858683e-03\n## 61   20    100      400     4 0.017812919 3.780810e-03 0.017660947 8.454147e-04\n## 81   25    100      500     5 0.014711230 2.496677e-03 0.015022455 5.582738e-04\n## 101  30    100      600     6 0.010719571 2.578118e-03 0.009863200 5.764847e-04\n## 121  35    100      700     7 0.009514228 1.588785e-03 0.009334077 3.552632e-04\n## 141  40    100      800     8 0.007691382 1.171925e-03 0.007477029 2.620504e-04\n## 161  45    100      900     9 0.006889727 1.222477e-03 0.006726188 2.733541e-04\n## 181  50    100     1000    10 0.006276889 5.128093e-04 0.006246931 1.146676e-04\n## 201  55    100     1100    11 0.005318209 5.985626e-04 0.005257637 1.338427e-04\n## 221  60    100     1200    12 0.004854049 3.641814e-04 0.004806155 8.143343e-05\n## 241  65    100     1300    13 0.004545931 3.242687e-04 0.004520731 7.250869e-05\n## 261  70    100     1400    14 0.004114851 2.213926e-04 0.004099411 4.950490e-05\n## 281  75    100     1500    15 0.003827730 3.163611e-04 0.003794601 7.074050e-05\n## 301  80    100     1600    16 0.003565950 1.937171e-04 0.003520204 4.331645e-05\n## 321  85    100     1700    17 0.003321481 1.553915e-04 0.003348171 3.474661e-05\n## 341  90    100     1800    18 0.003153977 7.759093e-05 0.003159270 1.734986e-05\n## 361  95    100     1900    19 0.002936686 5.616000e-05 0.002935480 1.255776e-05\n## 381 100    100     2000    20 0.002768276 0.000000e+00 0.002768276 0.000000e+00\n##         min.min     min.max  max.mean      max.sd max.median      max.se\n## 1   0.058393439 0.170711051 0.8819158 0.032569683  0.8876059 0.007282803\n## 21  0.028012247 0.080533570 0.8039199 0.038136538  0.8047723 0.008527589\n## 41  0.019685674 0.053076019 0.7255648 0.038281827  0.7324588 0.008560077\n## 61  0.012532975 0.025846915 0.6973759 0.039330036  0.6887384 0.008794463\n## 81  0.009046485 0.020133140 0.6381940 0.032865186  0.6328090 0.007348879\n## 101 0.008257497 0.019303295 0.6197714 0.029943565  0.6216856 0.006695585\n## 121 0.006840465 0.013738017 0.5788401 0.029811196  0.5822845 0.006665986\n## 141 0.005879615 0.011053336 0.5589409 0.043400845  0.5554842 0.009704724\n## 161 0.005364998 0.011462546 0.5217990 0.027818443  0.5244945 0.006220393\n## 181 0.005345154 0.007100721 0.4896128 0.022943577  0.4909629 0.005130340\n## 201 0.004317742 0.006380071 0.4837967 0.023518184  0.4848799 0.005258826\n## 221 0.004328371 0.005446223 0.4630265 0.022398047  0.4599904 0.005008355\n## 241 0.004046610 0.005226542 0.4414052 0.023000235  0.4404226 0.005143009\n## 261 0.003657161 0.004503838 0.4322617 0.021211389  0.4340543 0.004743011\n## 281 0.003223369 0.004460980 0.4089992 0.015626515  0.4098423 0.003494195\n## 301 0.003307291 0.003902424 0.3932721 0.015087435  0.3914948 0.003373653\n## 321 0.002975258 0.003588740 0.3851770 0.011768615  0.3860391 0.002631542\n## 341 0.003005001 0.003287277 0.3760762 0.009748364  0.3764867 0.002179801\n## 361 0.002838330 0.003063846 0.3659962 0.003577472  0.3653889 0.000799947\n## 381 0.002768276 0.002768276 0.3550118 0.000000000  0.3550118 0.000000000\n##       max.min   max.max\n## 1   0.8071012 0.9270047\n## 21  0.7323388 0.9063155\n## 41  0.6191583 0.7766993\n## 61  0.5878796 0.7694392\n## 81  0.5915755 0.6922287\n## 101 0.5586192 0.6855857\n## 121 0.5269503 0.6250264\n## 141 0.4901740 0.6524009\n## 161 0.4480063 0.5559810\n## 181 0.4391822 0.5360344\n## 201 0.4462260 0.5317380\n## 221 0.4234709 0.5015449\n## 241 0.3962534 0.4878237\n## 261 0.3966437 0.4709061\n## 281 0.3779369 0.4337704\n## 301 0.3746633 0.4216166\n## 321 0.3634942 0.4050216\n## 341 0.3531399 0.3906495\n## 361 0.3578767 0.3711270\n## 381 0.3550118 0.3550118\ntest.df3 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.5, control.mean = 0.5)\npsranges3 <- psrange(test.df3, test.df3$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges3)\nsummary(psranges3)##       p ntreat ncontrol ratio   min.mean      min.sd min.median       min.se\n## 1    10    100      100     1 0.28350601 0.064087988 0.28885238 0.0143305097\n## 21   20    100      200     2 0.16705507 0.031529985 0.16046048 0.0070503191\n## 41   30    100      300     3 0.12344740 0.019802371 0.12273928 0.0044279448\n## 61   40    100      400     4 0.09522531 0.011914719 0.09279009 0.0026642122\n## 81   50    100      500     5 0.07563025 0.008418365 0.07552338 0.0018824037\n## 101  60    100      600     6 0.06076604 0.008521491 0.06124068 0.0019054633\n## 121  70    100      700     7 0.05269255 0.005924987 0.05247310 0.0013248673\n## 141  80    100      800     8 0.04586865 0.002395134 0.04624844 0.0005355682\n## 161  90    100      900     9 0.04048884 0.002059943 0.04014488 0.0004606172\n## 181 100    100     1000    10 0.03634508 0.000000000 0.03634508 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.16192908 0.37939385 0.7132598 0.04361440  0.7129799 0.009752476 0.6394674\n## 21  0.10563283 0.22609654 0.5202766 0.04031786  0.5161472 0.009015347 0.4561985\n## 41  0.09125843 0.16864202 0.4143754 0.02875963  0.4181810 0.006430849 0.3529161\n## 61  0.07550229 0.12048280 0.3532632 0.02511123  0.3510016 0.005615041 0.3137257\n## 81  0.06256572 0.09642339 0.2941406 0.02138895  0.2973524 0.004782715 0.2463536\n## 101 0.05119536 0.08325665 0.2668781 0.01733506  0.2681488 0.003876236 0.2279083\n## 121 0.04216244 0.06796214 0.2363525 0.01411165  0.2348300 0.003155462 0.2141914\n## 141 0.04222852 0.05135097 0.2190724 0.01032624  0.2199282 0.002309018 0.2030472\n## 161 0.03701920 0.04386187 0.1978531 0.00701747  0.1986408 0.001569154 0.1863798\n## 181 0.03634508 0.03634508 0.1835126 0.00000000  0.1835126 0.000000000 0.1835126\n##       max.max\n## 1   0.7816167\n## 21  0.6045684\n## 41  0.4675561\n## 61  0.3965734\n## 81  0.3318606\n## 101 0.2927092\n## 121 0.2619002\n## 141 0.2443409\n## 161 0.2090405\n## 181 0.1835126\ntest.df4 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.25, treat.sd = 0.3, \n    control.mean = 0.75, control.sd = 0.3)\npsranges4 <- psrange(test.df4, test.df4$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges4)\nsummary(psranges4)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 1.525384e-05 2.837735e-05 2.299369e-06\n## 21   20    100      200     2 2.386395e-06 4.263708e-06 6.796030e-07\n## 41   30    100      300     3 7.854093e-07 1.369501e-06 3.322104e-07\n## 61   40    100      400     4 1.139238e-07 9.950192e-08 9.888239e-08\n## 81   50    100      500     5 4.970589e-08 7.085309e-08 1.910629e-08\n## 101  60    100      600     6 9.456041e-08 8.805110e-08 6.876480e-08\n## 121  70    100      700     7 4.562272e-08 3.277532e-08 4.271103e-08\n## 141  80    100      800     8 4.834941e-08 1.732541e-08 4.683829e-08\n## 161  90    100      900     9 3.734849e-08 1.141374e-08 3.473544e-08\n## 181 100    100     1000    10 3.302817e-08 0.000000e+00 3.302817e-08\n##           min.se      min.min      min.max  max.mean       max.sd max.median\n## 1   6.345368e-06 8.242210e-10 1.189959e-04 0.9999919 1.177109e-05  0.9999975\n## 21  9.533941e-07 4.512240e-09 1.459366e-05 0.9999698 4.357035e-05  0.9999873\n## 41  3.062298e-07 9.339904e-09 5.821350e-06 0.9999734 2.437621e-05  0.9999837\n## 61  2.224931e-08 5.006341e-09 3.452425e-07 0.9999718 2.790507e-05  0.9999803\n## 81  1.584323e-08 1.753035e-09 3.004791e-07 0.9999730 3.183032e-05  0.9999857\n## 101 1.968882e-08 1.862385e-08 4.026231e-07 0.9999570 1.988995e-05  0.9999546\n## 121 7.328785e-09 1.690112e-09 1.093181e-07 0.9999481 3.208612e-05  0.9999523\n## 141 3.874079e-09 1.740209e-08 9.088383e-08 0.9999326 1.783491e-05  0.9999357\n## 161 2.552189e-09 1.706567e-08 5.651540e-08 0.9999274 1.839133e-05  0.9999295\n## 181 0.000000e+00 3.302817e-08 3.302817e-08 0.9999211 0.000000e+00  0.9999211\n##           max.se   max.min   max.max\n## 1   2.632095e-06 0.9999539 1.0000000\n## 21  9.742627e-06 0.9998202 0.9999998\n## 41  5.450686e-06 0.9999165 0.9999973\n## 61  6.239764e-06 0.9998793 0.9999976\n## 81  7.117475e-06 0.9998587 0.9999988\n## 101 4.447529e-06 0.9999103 0.9999906\n## 121 7.174675e-06 0.9998882 0.9999962\n## 141 3.988007e-06 0.9998969 0.9999700\n## 161 4.112427e-06 0.9998986 0.9999602\n## 181 0.000000e+00 0.9999211 0.9999211\ntest.df5 <- getSimulatedData(nvars = 10, ntreat = 100, ncontrol = 1000)\npsranges5 <- psrange(test.df5, test.df5$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges5)\nsummary(psranges5)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 0.0101680007 5.571726e-03 0.0087507018\n## 21   20    100      200     2 0.0032786858 1.986855e-03 0.0031712876\n## 41   30    100      300     3 0.0016998723 8.777415e-04 0.0014178615\n## 61   40    100      400     4 0.0012710429 7.476010e-04 0.0010189156\n## 81   50    100      500     5 0.0009891488 3.613101e-04 0.0009233698\n## 101  60    100      600     6 0.0006954162 1.699134e-04 0.0006825701\n## 121  70    100      700     7 0.0005748031 1.098981e-04 0.0005927955\n## 141  80    100      800     8 0.0005241338 7.766948e-05 0.0005085729\n## 161  90    100      900     9 0.0004511794 4.957160e-05 0.0004584537\n## 181 100    100     1000    10 0.0004033341 0.000000e+00 0.0004033341\n##           min.se      min.min      min.max  max.mean      max.sd max.median\n## 1   1.245876e-03 0.0034055594 0.0218076912 0.9797889 0.007606976  0.9810769\n## 21  4.442742e-04 0.0010920758 0.0092490802 0.9667801 0.012485108  0.9700905\n## 41  1.962690e-04 0.0007561575 0.0045260935 0.9539731 0.013264618  0.9562029\n## 61  1.671687e-04 0.0006177711 0.0040260207 0.9394156 0.010526497  0.9424796\n## 81  8.079140e-05 0.0005352006 0.0019389389 0.9221930 0.014294291  0.9217427\n## 101 3.799379e-05 0.0002784764 0.0009768850 0.9113210 0.017183853  0.9104470\n## 121 2.457395e-05 0.0004073415 0.0007375938 0.9032366 0.010021744  0.9045313\n## 141 1.736742e-05 0.0003929492 0.0006895618 0.8848322 0.011660804  0.8843501\n## 161 1.108455e-05 0.0003240626 0.0005327837 0.8735074 0.008826874  0.8708373\n## 181 0.000000e+00 0.0004033341 0.0004033341 0.8639081 0.000000000  0.8639081\n##          max.se   max.min   max.max\n## 1   0.001700972 0.9612616 0.9912510\n## 21  0.002791755 0.9357162 0.9860199\n## 41  0.002966059 0.9106519 0.9720211\n## 61  0.002353796 0.9129030 0.9566536\n## 81  0.003196301 0.8929029 0.9531369\n## 101 0.003842426 0.8850449 0.9491578\n## 121 0.002240930 0.8856435 0.9235878\n## 141 0.002607435 0.8583591 0.9084171\n## 161 0.001973749 0.8641812 0.8983841\n## 181 0.000000000 0.8639081 0.8639081"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
