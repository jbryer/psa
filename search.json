[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Last updated: April 10, 2023I first introduced propensity score analysis (PSA) late dissertation advisor Robert Pruzek 2006 entered graduate school. notion get reasonable causal estimates without need randomization foreign first, skeptical. Many years later used PSA many projects, convinced possible, believe instances may preferred randomized control trial. Principal Investigator two Federal grants develop test Diagnostic Assessment Achievement College Skills (DAACS) attempted conduct large scale randomized control trials (RCT) involving thousands students. found experiences conducting large scale RCTs numerous compromises made delivering intervention compromise generalizability results. Moreover, RCTs assume single, homogenous, causal effect everyone. reality rarely true. interventions equally effective everyone. PSA, particularly stratification section, possible tease intervention may vary observed covariates.taught PSA many times years. “book” attempt collect notes experiences conducting PSA. part emphasize applied provide links references reader wishes explore theoretical details. Additionally, book make extensive use visualizations explain concepts well use presenting results. psa R package accompanies book available Github can installed using remotes package command . setting dependencies = 'Enhances' parameter ensure R packages used book installed well. psa package contains number datasets utility functions used throughout book. also contains Shiny application designed conduct PSA using graphical user interface. Details using application provided appendix.","code":"\nremotes::install_github('jbryer/psa',\n                        build_vignettes = TRUE,\n                        dependencies = 'Enhances')"},{"path":"index.html","id":"contributing","chapter":"Preface","heading":"Contributing","text":"books work progress contributions welcome. Please adhere code conduct. page edit link take directly source file Github. can also submit feedback using Github Issues tracker.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"website created using bookdown hosted Github pages.","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"","code":"\ndevtools::session_info()## ─ Session info ───────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.2.3 (2023-03-15)\n##  os       Ubuntu 22.04.2 LTS\n##  system   x86_64, linux-gnu\n##  ui       X11\n##  language (EN)\n##  collate  C.UTF-8\n##  ctype    C.UTF-8\n##  tz       UTC\n##  date     2023-04-10\n##  pandoc   2.19.2 @ /usr/bin/ (via rmarkdown)\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────\n##  ! package       * version  date (UTC) lib source\n##    abind           1.4-5    2016-07-21 [1] CRAN (R 4.2.3)\n##  P backports       1.4.1    2021-12-13 [?] RSPM (R 4.2.0)\n##    bookdown        0.33     2023-03-06 [1] CRAN (R 4.2.3)\n##  P boot            1.3-28.1 2022-11-22 [3] CRAN (R 4.2.3)\n##  P bslib           0.4.2    2022-12-16 [?] RSPM (R 4.2.0)\n##  P cachem          1.0.7    2023-02-24 [?] RSPM (R 4.2.0)\n##  P callr           3.7.3    2022-11-02 [?] RSPM (R 4.2.0)\n##    car           * 3.1-2    2023-03-30 [1] CRAN (R 4.2.3)\n##    carData       * 3.0-5    2022-01-06 [1] CRAN (R 4.2.3)\n##  P cli             3.6.1    2023-03-23 [?] RSPM (R 4.2.0)\n##  P codetools       0.2-19   2023-02-01 [3] CRAN (R 4.2.3)\n##    coin            1.4-2    2021-10-08 [1] CRAN (R 4.2.3)\n##  P colorspace      2.1-0    2023-01-23 [?] RSPM (R 4.2.0)\n##  P crayon          1.5.2    2022-09-29 [?] RSPM (R 4.2.0)\n##  P devtools        2.4.5    2022-10-11 [?] RSPM (R 4.2.0)\n##  P digest          0.6.31   2022-12-11 [?] RSPM (R 4.2.0)\n##  P downlit         0.4.2    2022-07-05 [?] RSPM (R 4.2.0)\n##  P dplyr         * 1.1.1    2023-03-22 [?] RSPM (R 4.2.0)\n##  P ellipsis        0.3.2    2021-04-29 [?] RSPM (R 4.2.0)\n##  P evaluate        0.20     2023-01-17 [?] RSPM (R 4.2.0)\n##    ez            * 4.4-0    2016-11-02 [1] CRAN (R 4.2.3)\n##  P fansi           1.0.4    2023-01-22 [?] RSPM (R 4.2.0)\n##  P fastmap         1.1.1    2023-02-24 [?] RSPM (R 4.2.0)\n##  P fs              1.6.1    2023-02-06 [?] RSPM (R 4.2.0)\n##  P generics        0.1.3    2022-07-05 [?] RSPM (R 4.2.0)\n##  P ggplot2       * 3.4.2    2023-04-03 [?] RSPM (R 4.2.0)\n##    ggthemes        4.2.4    2021-01-20 [1] CRAN (R 4.2.3)\n##  P glue            1.6.2    2022-02-24 [?] RSPM (R 4.2.0)\n##    granova       * 2.2      2023-03-22 [1] CRAN (R 4.2.3)\n##  P granovaGG     * 1.4.0    2015-12-18 [?] RSPM (R 4.2.0)\n##  P gridExtra       2.3      2017-09-09 [?] RSPM (R 4.2.0)\n##  P gtable          0.3.3    2023-03-21 [?] RSPM (R 4.2.0)\n##  P highr           0.10     2022-12-22 [?] RSPM (R 4.2.0)\n##  P htmltools       0.5.5    2023-03-23 [?] RSPM (R 4.2.0)\n##  P htmlwidgets     1.6.2    2023-03-17 [?] RSPM (R 4.2.0)\n##  P httpuv          1.6.9    2023-02-14 [?] RSPM (R 4.2.0)\n##  P jquerylib       0.1.4    2021-04-26 [?] RSPM (R 4.2.0)\n##  P jsonlite        1.8.4    2022-12-06 [?] RSPM (R 4.2.0)\n##  P knitr         * 1.42     2023-01-25 [?] RSPM (R 4.2.0)\n##  P later           1.3.0    2021-08-18 [?] RSPM (R 4.2.0)\n##  P lattice         0.20-45  2021-09-22 [3] CRAN (R 4.2.3)\n##    libcoin         1.0-9    2021-09-27 [1] CRAN (R 4.2.3)\n##  P lifecycle       1.0.3    2022-10-07 [?] RSPM (R 4.2.0)\n##    lme4            1.1-32   2023-03-14 [1] CRAN (R 4.2.3)\n##  P magrittr        2.0.3    2022-03-30 [?] RSPM (R 4.2.0)\n##  P MASS          * 7.3-58.2 2023-01-23 [3] CRAN (R 4.2.3)\n##    Matching      * 4.10-8   2022-11-03 [1] CRAN (R 4.2.3)\n##    MatchIt       * 4.5.2    2023-03-22 [1] CRAN (R 4.2.3)\n##  P Matrix          1.5-3    2022-11-11 [3] CRAN (R 4.2.3)\n##    matrixStats     0.63.0   2022-11-18 [1] CRAN (R 4.2.3)\n##  P memoise         2.0.1    2021-11-26 [?] RSPM (R 4.2.0)\n##  P mgcv            1.8-42   2023-03-02 [3] CRAN (R 4.2.3)\n##  P mime            0.12     2021-09-28 [?] RSPM (R 4.2.0)\n##  P miniUI          0.1.1.1  2018-05-18 [?] RSPM (R 4.2.0)\n##    minqa           1.2.5    2022-10-19 [1] CRAN (R 4.2.3)\n##    mnormt          2.1.1    2022-09-26 [1] CRAN (R 4.2.3)\n##    modeltools      0.2-23   2020-03-05 [1] CRAN (R 4.2.3)\n##    multcomp        1.4-23   2023-03-09 [1] CRAN (R 4.2.3)\n##    multilevelPSA * 1.2.5    2018-03-22 [1] CRAN (R 4.2.3)\n##  P munsell         0.5.0    2018-06-12 [?] RSPM (R 4.2.0)\n##    mvtnorm         1.1-3    2021-10-08 [1] CRAN (R 4.2.3)\n##  P nlme            3.1-162  2023-01-31 [3] CRAN (R 4.2.3)\n##    nloptr          2.0.3    2022-05-26 [1] CRAN (R 4.2.3)\n##    party           1.3-13   2023-03-17 [1] CRAN (R 4.2.3)\n##  P pillar          1.9.0    2023-03-22 [?] RSPM (R 4.2.0)\n##  P pkgbuild        1.4.0    2022-11-27 [?] RSPM (R 4.2.0)\n##  P pkgconfig       2.0.3    2019-09-22 [?] RSPM (R 4.2.0)\n##  P pkgload         1.3.2    2022-11-16 [?] RSPM (R 4.2.0)\n##  P plyr            1.8.8    2022-11-11 [?] RSPM (R 4.2.0)\n##  P prettyunits     1.1.1    2020-01-24 [?] RSPM (R 4.2.0)\n##  P processx        3.8.0    2022-10-26 [?] RSPM (R 4.2.0)\n##  P profvis         0.3.7    2020-11-02 [?] RSPM (R 4.2.0)\n##  P promises        1.2.0.1  2021-02-11 [?] RSPM (R 4.2.0)\n##  P ps              1.7.4    2023-04-02 [?] RSPM (R 4.2.0)\n##    PSAboot       * 1.3.6    2023-03-22 [1] CRAN (R 4.2.3)\n##    PSAgraphics   * 2.1.2    2023-03-21 [1] CRAN (R 4.2.3)\n##    psych           2.3.3    2023-03-18 [1] CRAN (R 4.2.3)\n##  P purrr           1.0.1    2023-01-10 [?] RSPM (R 4.2.0)\n##  P R6              2.5.1    2021-08-19 [?] RSPM (R 4.2.0)\n##  P randomForest    4.7-1.1  2022-05-23 [?] RSPM (R 4.2.0)\n##  P RColorBrewer    1.1-3    2022-04-03 [?] RSPM (R 4.2.0)\n##  P Rcpp            1.0.10   2023-01-22 [?] RSPM (R 4.2.0)\n##  P remotes         2.4.2    2021-11-30 [?] RSPM (R 4.2.0)\n##    reshape         0.8.9    2022-04-12 [1] CRAN (R 4.2.3)\n##  P reshape2      * 1.4.4    2020-04-09 [?] RSPM (R 4.2.0)\n##  P rlang           1.1.0    2023-03-14 [?] RSPM (R 4.2.0)\n##  P rmarkdown       2.21     2023-03-26 [?] RSPM (R 4.2.0)\n##  P rpart         * 4.1.19   2022-10-21 [3] CRAN (R 4.2.3)\n##    sandwich        3.0-2    2022-06-15 [1] CRAN (R 4.2.3)\n##  P sass            0.4.5    2023-01-24 [?] RSPM (R 4.2.0)\n##  P scales        * 1.2.1    2022-08-20 [?] RSPM (R 4.2.0)\n##  P sessioninfo     1.2.2    2021-12-06 [?] RSPM (R 4.2.0)\n##  P shiny           1.7.4    2022-12-15 [?] RSPM (R 4.2.0)\n##  P stringi         1.7.12   2023-01-11 [?] RSPM (R 4.2.0)\n##  P stringr         1.5.0    2022-12-02 [?] RSPM (R 4.2.0)\n##    strucchange     1.5-3    2022-06-15 [1] CRAN (R 4.2.3)\n##  P survival        3.5-3    2023-02-12 [3] CRAN (R 4.2.3)\n##    TH.data         1.1-1    2022-04-26 [1] CRAN (R 4.2.3)\n##  P tibble          3.2.1    2023-03-20 [?] RSPM (R 4.2.0)\n##  P tidyselect      1.2.0    2022-10-10 [?] RSPM (R 4.2.0)\n##    TriMatch      * 0.9.9    2017-12-06 [1] CRAN (R 4.2.3)\n##  P urlchecker      1.0.1    2021-11-30 [?] RSPM (R 4.2.0)\n##  P usethis         2.1.6    2022-05-25 [?] RSPM (R 4.2.0)\n##  P utf8            1.2.3    2023-01-31 [?] RSPM (R 4.2.0)\n##  P vctrs           0.6.1    2023-03-22 [?] RSPM (R 4.2.0)\n##  P withr           2.5.0    2022-03-03 [?] RSPM (R 4.2.0)\n##  P xfun            0.38     2023-03-24 [?] RSPM (R 4.2.0)\n##  P xml2            1.3.3    2021-11-30 [?] RSPM (R 4.2.0)\n##  P xtable        * 1.8-4    2019-04-21 [?] RSPM (R 4.2.0)\n##  P yaml            2.3.7    2023-01-23 [?] RSPM (R 4.2.0)\n##    zoo             1.8-11   2022-09-17 [1] CRAN (R 4.2.3)\n## \n##  [1] /home/runner/.cache/R/renv/library/psa-1b3136f9/R-4.2/x86_64-pc-linux-gnu\n##  [2] /home/runner/.cache/R/renv/sandbox/R-4.2/x86_64-pc-linux-gnu/e11edd0e\n##  [3] /opt/R/4.2.3/lib/R/library\n## \n##  P ── Loaded and on-disk path mismatch.\n## \n## ──────────────────────────────────────────────────────────────────────────────"},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"work Jason Bryer licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"chapter-introduction.html","id":"chapter-introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"use propensity score methods (Rosenbaum Rubin 1983) estimating causal effects observational studies certain kinds quasi-experiments increasing last couple decades (see Figure 1.1), especially social sciences (Thoemmes Kim 2011) medical research (Austin 2008). Propensity score analysis (PSA) attempts adjust selection bias occurs due lack randomization. Analysis typically conducted three phases phase , probability placement treatment estimated identify matched pairs clusters phase II, comparisons dependent variable can made matched pairs within clusters. Lastly, phase III involves testing robustness estimates unobserved confounders. R (R Core Team 2023) ideal conducting PSA given wide availability current statistical methods vis-à-vis add-packages well superior graphics capabilities.book provide theoretical overview propensity score methods well illustrations discussion implementing PSA methods R. Chapter 1 provides overview three phases PSA minimal R code. Chapters 2, 3, 4 discuss details implementing three major approaches PSA. Chapter 5 provides strategies conducting PSA missing data. Chapters 6 7 provide details phase III PSA using sensitivity analysis bootstrapping, respectively. Lastly, chapter 8 provides methods implementing PSA non-binary treatments chapter 9 discusses methods PSA cluster, Hierarchical, data. appendices contain additional details regarding PSA Shiny application (Appendix ), limitations interpreting fitted values logistic regression (Appendix B), additional methods packages estimating propensity scores (Appendix C).\nFigure 1.1: PSA Citations per year\n","code":""},{"path":"chapter-introduction.html","id":"counterfactual-model-for-causality","chapter":"1 Introduction","heading":"1.1 Counterfactual Model for Causality","text":"order understand propensity score analysis allows us make causal estimates observational data, must first understand basic principals causality, particularly counterfactual model. Figure 1.2 depicts conterfactual model. begin research subject. can student, patient, mouse, asteroid, object wish know whether condition effect . Consider two parallel universes: one subject receives condition another receive condition B. Typically one condition treatment whereas condition absence treatment (also referred control). use treatment control throughout book refer two conditions. individual exposed two conditions, outcome measured. difference outcomes true causal effect. However, unless Dr. Strange living Marvell multiverse, impossible object exist two universes time, therefore can never actually observe true causal effect. Holland (1986) referred Fundamental Problem Causal Inference.\nFigure 1.2: Theoretical Causal Model\n","code":""},{"path":"chapter-introduction.html","id":"randomized-control-trials-the-gold-standard","chapter":"1 Introduction","heading":"1.2 Randomized Control Trials: “The Gold Standard”","text":"randomized control trials (RCT) gold standard estimating causal effects. Effects can estimated using simple means groups, blocks randomized block design. Randomization presumes unbiasedness balance groups. However, randomization often feasible many reasons, especially educational contexts. Although RCT gold standard, important recognize estimates causal effect. look example RCT can wrong average provides good estimates true causal effect can build model closely mimick RCT non-randomized data.Intelligence Quotient (IQ) common measure intelligence. designed mean 100 standard deviation 15. Consider developed intervention known increase anyone’s IQ 4.5 points (standardized effect size 0.3). Figure 1.3 represents scenario 30 individuals. left panel individual’s outcome assigned control condition (blue) treatment condition (red). distance red blue points individual 4.5, stipulated counterfactual difference. RCTs ever get observe one outcome individual. right pane represents one possible set outcomes RCT. , randomly selected one outcome individual left pane.\nFigure 1.3: Example conterfactuals (left panel) one possible randomized control trial.\nFigure 1.4 includes mean differences treatment control vertical lines blue red, respectively. left observe true counterfactuals difference treatment (red) control (blue) vertical lines 4.5. However, right difference treatment control -5.3!\nFigure 1.4: Estimated differences full counterfactual model one RCT.\nexample RCT estimate true effect, estimated wrong direction. However, Figure 1.5 represents distribution effects conducting 1,000 RCTs 30 individuals . point RCT already compromise estimating true counterfactual (.e. causal effect). consider gold standard many trials nearly approximate true counterfactual.\nFigure 1.5: Distribution differences across many RCTs\nRCT works probability anyone treatment 50%. Statistically, call strong ignorability assumption. strong ignorability assumption states outcome independent observed unobserved covariates1 randomization. represented mathematically :\\[\\begin{equation}\n\\begin{aligned}\n\\left( { Y }_{ }\\left( 1 \\right) ,{ Y }_{ }\\left( 0 \\right)  \\right) \\bot { T }_{ }\n\\end{aligned}\n\\tag{1.1}\n\\end{equation}\\]\\({X}_{}\\) , \\(Y\\) outcome interest individual response \\(Y_i(1)\\) outcome subject assigned treatment group \\(Y_i(0)\\) outcome subject assigned control group. \\(\\bot\\) means independent \\(T_i\\) assignment indicator subject . Therefore, follows causal effect treatment difference individual’s outcome situation given treatment (referred counterfactual).\\[\\begin{equation}\n\\begin{aligned}\n{\\delta}_{} = { Y }_{ i1 }-{ Y }_{ i0 }\n\\end{aligned}\n\\tag{1.2}\n\\end{equation}\\]However, impossible directly observe \\({}_{}\\) (referred Fundamental Problem Causal Inference, Holland 1986). Rubin framed problem missing data problem details discussed next section.","code":""},{"path":"chapter-introduction.html","id":"rubins-causal-model","chapter":"1 Introduction","heading":"1.2.1 Rubin’s Causal Model","text":"Returning Figure 1.2, problem getting true causal effect observe outcome outcome B, never . result, missing data estimate causal effect. Neyman (1923) first coined term potential outcomes referring randomized trials. However, Donald Rubin extended Neyman’s idea include observational experimental data. Rubin’s student Holland (1986) later coined Rubin Causal Model.Rubin (1974) discussed example effect aspirin headache:“Intuitively, causal effect one treatment, E, another, C, particular unit interval time \\(t_{1}\\)\n\\(t_{2}\\) difference happened time \\(t_{2}\\) unit exposed E initiated \\(t_{1}\\) happened \\(t_{2}\\) unit exposed C initiated \\(t_{1}\\): ‘hour ago taken two aspirins instead just glass water, headache now gone,’ ‘hour ago took two aspirins instead just glass water, headache now gone.’ definition causal effect E versus C treatment reflect intuitive meaning.”Rubin Causal Model, whether headache cause whether took aspirin one hour ago, can observe one. key estimating causal effect understanding mechanism selecting whether take aspirin. Imagine get chronic headaches need decide many times whether take aspirin. Let’s also stimulate aspirin likely effective take morning afternoon. decide flip coin decide whether take aspirin balance observed headaches morning afternoon. , even though difference morning afternoon, influence observed outcomes. However, decide take aspirin 50 degrees outside. Since likely warmer afternoon morning, comparing outcomes provide bias estimate, part deciding whether take aspirin longer 50%. observed weather can potentially determine probability taking aspirin . enough observations, compare situations probability taking aspirin low, observations without aspirin way across spectrum high probability fo taking aspirin.","code":""},{"path":"chapter-introduction.html","id":"propensity-scores","chapter":"1 Introduction","heading":"1.2.2 Propensity Scores","text":"Propensity scores first introduced Rosenbaum Rubin (1983). defined propensity scores “conditional probability assignment particular treatment given vector observed covariates.” Rosenbaum Rubin showed seminal 1983 paper, Central Role Propensity Score Observational Studies Causal Effects “scalar propensity score sufficient remove bias due observed covariates.” Propensity scores can used variety ways including matching, stratification, weighting.Mathematically can define probability treatment group :\\[\\begin{equation}\n\\begin{aligned}\n\\pi(X_i) = Pr(T_i = 1 \\; | \\; X_i)\n\\end{aligned}\n\\tag{1.3}\n\\end{equation}\\]\\(X\\) matrix observed covariates \\(\\pi(X_i)\\) propensity score. balancing property exogeneity states ,\\[\\begin{equation}\n\\begin{aligned}\n\nT_i \\; \\mathrel{\\unicode{x2AEB}} \\; X_i \\; | \\; \\pi (X_i)\n\\end{aligned}\n\\tag{1.4}\n\\end{equation}\\]Ti treatment indicator subject . case randomized experiments, strong ignobility assumption states,\\[\\begin{equation}\n\\begin{aligned}\n\nY_i(1), \\; Y_i(0)) \\; \\mathrel{\\unicode{x2AEB}} \\; T_i \\; | \\; X_i\n\\end{aligned}\n\\tag{1.5}\n\\end{equation}\\]\\(X_i\\). , treatment independent covariates, observed otherwise. However, strong ignorability assumption can restated propensity score ,\\[\\begin{equation}\n\\begin{aligned}\n\n({ Y }_{ }(1),{ Y }_{ }(0)) \\; \\mathrel{\\unicode{x2AEB}} \\; { T }_{ } \\; | \\; \\pi({ X }_{ })\n\\end{aligned}\n\\tag{1.6}\n\\end{equation}\\]treatment placement ignorable given propensity score presuming sufficient balance2 achieved.average treatment effect (ATE) defined \\(E(r_1) - E(r_0)\\) \\(E(.)\\) expected value population. Given set covariates, \\(X\\), outcomes \\(Y\\), 0 denotes control group 1 denotes treatment group, ATE defined :\\[\\begin{equation}\n\\begin{aligned}\nATE \\; = \\; E(Y_1 - Y_0 \\; | \\; X) \\; = \\; E(Y_1 \\; | \\; X) - E(Y_0 \\; | \\; X)\n\\end{aligned}\n\\tag{1.7}\n\\end{equation}\\]difference treatment control groups given set observed covariates. section 1.3.2 discuss ATE addition causal estimators detail.Simply put, Rosenbaum Rubin (1983) proved observations similar propensity scores roughly equivalent (balanced) across observed covariates. see rest chapter, scalar summarizes many variables convenient finding matches, stratifying, applying regression weights. Although verify balance achieved methods estimating propensity scores better others.","code":""},{"path":"chapter-introduction.html","id":"phases-of-propensity-score-analysis","chapter":"1 Introduction","heading":"1.3 Phases of Propensity Score Analysis","text":"Propensity score analysis typically conducted three phases, namely:Model selection bias (.e. estimate propensity scores).Estimate causal effects.Check sensitivity unobserved confounders.following sections provide overview phases details implementing phase using one three main methods conducting PSA, stratification (chapter 2), matching (chapter 3), weighting (chapter 4).","code":""},{"path":"chapter-introduction.html","id":"phase-i-estimate-propensity-scores","chapter":"1 Introduction","heading":"1.3.1 Phase I: Estimate Propensity Scores","text":"Phase one propensity score analysis cyclical process propensity scores estimated using statistical model, balance observed covariates checked, modifications model modified sufficient balance achieved. simplicity use logistic regression estimate propensity scores throughout book. However, introduce classification trees chapter 2 given uniquely applicable stratification methods appendix C outlines additional statistical methods, R code, estimating propensity scores.Propensity scores conditional probability treatment given set observed covaraites. practice use statistical models dependent variable dichotomous (treatment control). often logistic regression used, advances predictive models ever increasing number model choices including classification trees, Bayesian models, ensemble random forests, many . demonstrate main features propensity score analysis use simulated dataset two pre-treatment covariates, x1 x2, treatment indicator, outcome variable treatment effect 2. Figure 1.6 scatter plot simulated data.3\nFigure 1.6: Scatterplot simulated datatset\nFigure 1.7 pairs plot (Schloerke et al. 2021) showing relationship covariates (.e. x1 x2) outcome grouped treatment. statistically significant correlation covariates outcome suggesting selection bias bias causal estimate. Indeed simple null hypothesis test resulted difference 4.55 (\\(t_{959} = -32.75\\), p < 0.01), however setup simulation mean difference 2!\nFigure 1.7: Pairs plot showing relationships covariates, treatment, outcome\ngoal adjust selection bias using propensity scores. example used logistic regression estimate propensity scores. Figure 1.8 histogram showing distribution propensity scores treatment group greee control group orange . Note distributions skewed; treatment group negatively skewed control group positively skewed. hopefully make intuitive sense. probability treatment increases, see number treatment observations increase number control observations decrease.\nFigure 1.8: Distribution propensity scores\n","code":"\nt.test(outcome ~ treatment, data = dat)## \n##  Welch Two Sample t-test\n## \n## data:  outcome by treatment\n## t = -32.748, df = 959.11, p-value < 2.2e-16\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -4.821017 -4.275883\n## sample estimates:"},{"path":"chapter-introduction.html","id":"evaluate-balance","chapter":"1 Introduction","heading":"1.3.1.1 Evaluate Balance","text":"propensity scores estimated important verify balance observed covariates achieved. number ways . matching methods treatment control units paired, dependent sample tests can usedd (e.g. t-tests continuous variables \\(\\chi^2\\) tests categorical variables). However, significance testing generally problematic. Given number covariates, hence null hypothesis tests conducted, likelihood committing type type II errors high. Moreover, many observational studies wish use PSA large sample sizes , else equal, shrink standard error estimate often resulting small p-values. Instead utilizing standardized effect sizes graphical representations provide better evidence whether balance achieved. PSAgraphics package (Helmreich Pruzek 2023) provides number functions assist. Figure 1.9 provides two assessment plots. left plot multiple covariate balance assessment plot. x-axis standardized effect size y-axis covariate. red line effect propensity score adjustment blue effect propensity score adjustment. Unfortunately conventional adjusted effect size threshold achieving sufficient balance literature. Cohen (1988) frequently cited indicated effect size 0.2 0.3 small. general, recommend trying achieve adjdusted effect sizes less 0.1.\nFigure 1.9: Multiple (left) continuous (right) covariate balance assessment plots\nplot right figure 1.9 balance assessment plot continuous variable. exact procedures stratification discussed chapter 2 short, divided propensity scores five strata using quintiles stratum number observations. yellow bars control group orange bars treatment group. looking center spread roughly equivalent within stratum. example can see stratum 5 higher values stratum 1. counterpart plot categorical data using box plots provided chapter 2.see many choices estimating propensity scores remainder book. practice find phase PSA occupy time. robustness causal estimates rely achieving good balance observed covariates.propensity score method use?Whichever one gives best balance!","code":""},{"path":"chapter-introduction.html","id":"introduction-effects","chapter":"1 Introduction","heading":"1.3.2 Phase II: Estimate Causal Effects","text":"Now sufficient balance achieved observed covariates, time estimate causal effect. randomized control trials typically conduct null hypothesis test differences means treatment control groups (defined equation (1.7) ). PSA often done, important recognize observations counted equal causal estimation. moreover, average treatment effect causal estimate measure can calculate. section defines four different causal estimates. presented context propensity score weighting (see 4) conceptually apply stratification matching.begin, often helpful plot propensity scores outcome. Figure 1.10 scatter plot propensity scores (x-axis) outcome (y-axis), grouped/colored treatment, along Loess regression line (Cleveland 1979). number features observe . First, see propensity score increases outcome increases. direct representation selection bias. Second, Loess regression lines approximate 95% confidence intervals (grey) overlap across entire range propensity scores. Additionally distance two Loess regression lines rougly equal. indication treatment effect homogeneous (.e. units). see later chapters often case. become important feature PSA detecting heterogeneous, uneven, treatments based upon different “profiles.”\nFigure 1.10: Scatter plot propensity scores outcome Loess regression lines\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-ate","chapter":"1 Introduction","heading":"1.3.2.1 Average Treatment Effect (ATE)","text":"average treatment effect (ATE) understood estimate given direct analog RCTs. estimate ATE RCT using approach simply assuming everyone propensity score 0.5 since 50% treatment. , assume every treatment unit interchangeable control unit. PSA though, unit different propensity score. goal compare units similar propensity scores. saw figure 1.8 distributions treatment control . Figure @ref{fig:ate-hist} depicts ATE works practice, particular different units weighted less towards ATE estimate move across propensity score range. darker color represents propensity score distribution estimated , light bars represent distribution used ATE calculation. treatment units lower propensity scores (fewer ) weighted ATE calculation. move right across propensity score range control units large propensity scores wieghed range.\\[\\begin{equation}\n\\begin{aligned}\nATE = E(Y_1 - Y_0 | X) = E(Y_1|X) - E(Y_0|X)\n\\end{aligned}\n\\tag{1.8}\n\\end{equation}\\]\nFigure 1.11: Histogram average treatement effect\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-treated-att","chapter":"1 Introduction","heading":"1.3.2.2 Average Treatment Effect Among the Treated (ATT)","text":"average treatment effect among treated (ATT) uses treated units primary focus. figure 1.12 see entire treatment group ise used weighting . However, control group weight (grey bars) values lower end propensity score range match distribution treatment group. Conversely, control group observations weighted right side propensity score range, , closely match distribution treatment group. context matching wish pair treatment control units, goal use treatment observations, therefore possible use control observations smaller propensity scores whereas control observations larger propensity scores may reused order find match every treatment observation.Mathematically, ATT defined equation (1.9). important difference ATE calculating expected value given \\(X = 1\\), indicates placement treatment.\\[\\begin{equation}\n\\begin{aligned}\nATT = E(Y_1 - Y_0 | X = 1) = E(Y_1 | X = 1) - E(Y_0 | X = 1)\n\\end{aligned}\n\\tag{1.9}\n\\end{equation}\\]\nFigure 1.12: Histogram average treatement among treated\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-control-atc","chapter":"1 Introduction","heading":"1.3.2.3 Average Treatment Effect Among the Control (ATC)","text":"average treatment effect among control (ATC) exactly opposte ATT. , wish use every control observation means treatment observations larger propensity scores used (case matching) weighted (case weighting stratification) represented grey. Conversely, treatment observations smaller propensity scores may match multiple control observations (case matching) weighted (case weighting stratification).Mathematically, ATC defined equation (1.10). important difference ATE calculating expected value given \\(X = 1\\), indicates placement control\\[\\begin{equation}\n\\begin{aligned}\nATC = E(Y_1 - Y_0 | X = 0) = E(Y_1 | X = 0) - E(Y_0 | X = 0)\n\\end{aligned}\n\\tag{1.10}\n\\end{equation}\\]\nFigure 1.13: Histogram average treatement among control\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-evenly-matched-atm","chapter":"1 Introduction","heading":"1.3.2.4 Average Treatment Effect Among the Evenly Matched (ATM)","text":"average treatment effect among evenly matched (ATM) relatively new estimate developed specifically propensity score weighting closely related estimated conducting one--one matching. Unlike ATT ATC observations weighted equally, calculation ATM observations included estimation equal weight. depicted figure 1.14 control observations small propensity scores used treatment observations large propensity scores used (represented grey bars). closely mimics occurs one--one matching. one--one matching observation can used can matched one observation group. Hence, tends work observations near mean propensity score range included. See Li Greene (2013), McGowan (2018), Samuels (2017) details.\\[\\begin{equation}\n\\begin{aligned}\nATM_d = E(Y_1 - Y_0 | M_d = 1)\n\\end{aligned}\n\\tag{1.11}\n\\end{equation}\\]\nFigure 1.14: Histogram average treatment effect among evenly matched\n","code":""},{"path":"chapter-introduction.html","id":"phase-iii-sensitivity-analysis","chapter":"1 Introduction","heading":"1.3.3 Phase III: Sensitivity Analysis","text":"final phase propensity score analysis evaluate robustness causal estimates. discuss two approaches test robustness: sensitivity analysis (covered detail chapter 6) bootstrapping (covered detail chapter 7). Sensitivity analysis procedure results tested increasing factors unmeasured confounder changing randomization process. , tests much another variable change prediction treatment result non rejecting null hypothesis.Sensitivity analysis well defined matching methods. Rosenbaum (2012) proposed testing null hypothesis , part, also test sensitivity chosen method. spirit testing null hypothesis , PSAboot R package (Bryer 2023) developed conducting bootstrapping propensity score analysis. framework addresses issues sensitivity method choice, also provides framework addressing issues imbalance treatment placement. Bootstrapping (Efron 1979) become effective approach estimating parameters. approach discussed chapter 7 avoids issues multiple hypothesis testing increased type error rates using bootstrap samples estimate standard errors confidence intervals.","code":""},{"path":"chapter-introduction.html","id":"r-packages","chapter":"1 Introduction","heading":"1.4 R Packages","text":"R statistical software language designed extended vis-à-vis packages. April 10, 2023, currently 19,317 packages available CRAN. Given ease R can extended, become tool choice conducting propensity score analysis. new R highly recommend R Data Science (Wickham Grolemund 2016) excellent introduction R. book make use number RMatchIt (Ho et al. 2023) Nonparametric Preprocessing Parametric Causal InferenceMatching (Singh Sekhon Saarinen 2022) Multivariate Propensity Score Matching Software Causal InferencemultilevelPSA (Bryer 2018) Multilevel Propensity Score Analysisparty (Hothorn et al. 2023) Laboratory Recursive PartytioningPSAboot (Bryer 2023) Bootstrapping Propensity Score AnalysisPSAgraphics (Helmreich Pruzek 2023) R Package Support Propensity Score Analysisrbounds (Keele 2022) Overview rebounds: R Package Rosenbaum bounds sensitivity analysis matched data.rpart (Therneau Atkinson 2022) Recursive PartitioningTriMatch (Bryer 2017) Propensity Score Matching Non-Binary TreatmentsThe psa R package specifically designed accompany book including utility functions assist conducting propensity score analysis. following command install psa R package along R packages use book.","code":"\nremotes::install_github('jbryer/psa', dependencies = 'Enhances')"},{"path":"chapter-introduction.html","id":"datasets","chapter":"1 Introduction","heading":"1.5 Datasets","text":"section provides description datasets used throughout book.","code":""},{"path":"chapter-introduction.html","id":"lalonde","chapter":"1 Introduction","heading":"1.5.1 National Supported Work Demonstration (lalonde)","text":"lalonde dataset perhaps one used datasets introducing evaluating propensity score methods. data collected Lalonde (1986) became widely used PSA literature Dehejia Wahba (1999) used paper evaluate propensity score matching. dataset originated National Supported Work Demonstration study conducted 1970s. program provided 12 18 months employment people longstanding employment problems. dataset contains 445 observations 12 variables. primary outcome re78 real earnings 1978. Observed covariates used ajdust selection bias include age (age years), edu (number years education), black (black ), hisp (Hispanic ), married (married ), nodegr (whether worker degree , note 1 = degree), re74 (real earnings 1974), re75 (real earnings 1975).age: Integer mean = 25 SD = 7.1educ: Integer mean = 10 SD = 1.8black: Integer mean = 0.83 SD = 0.37hisp: Integer mean = 0.088 SD = 0.28married: Integer mean = 0.17 SD = 0.37nodegr: Integer mean = 0.78 SD = 0.41re74: Numeric mean = 2,102 SD = 5,364re75: Numeric mean = 1,377 SD = 3,151re78: Numeric mean = 5,301 SD = 6,631u74: Integer mean = 0.73 SD = 0.44u75: Integer mean = 0.65 SD = 0.48treat: Integer mean = 0.42 SD = 0.49","code":"\ndata(lalonde, package='Matching')"},{"path":"chapter-introduction.html","id":"lindner","chapter":"1 Introduction","heading":"1.5.2 Lindner Center (lindner)","text":"Data observational study 996 patients receiving PCI Ohio Heart Health 1997 followed least 6 months staff Lindner Center. landmark dataset literature propensity score adjustment treatment selection bias due practice evidence based medicine; patients receiving abciximab tended severely diseased receive IIb/IIIa cascade blocker.lifepres: Numeric mean = 11 SD = 1.9cardbill: Integer mean = 15,674 SD = 11,182abcix: Integer mean = 0.7 SD = 0.46stent: Integer mean = 0.67 SD = 0.47height: Integer mean = 171 SD = 11female: Integer mean = 0.35 SD = 0.48diabetic: Integer mean = 0.22 SD = 0.42acutemi: Integer mean = 0.14 SD = 0.35ejecfrac: Integer mean = 51 SD = 10ves1proc: Integer mean = 1.4 SD = 0.66","code":"\ndata(lindner, package='PSAgraphics')"},{"path":"chapter-introduction.html","id":"tutoring","chapter":"1 Introduction","heading":"1.5.3 Tutoring (tutoring)","text":"tutoring dataset originates study conducted online adult serving institution examining effects tutoring services students English 101, English 201, History 310. Tutoring services available students Treatment (treat) operationalized students used tutoring services least course. 19.6% students used tutoring services approximately half using . use dataset dichotomous treatment (used tutoring ) two level treatment (used tutoring services , used tutoring services two times).treat: Factor 3 levels: Control; Treat1; Treat2Course: Character 3 unique valuesGrade: Integer mean = 2.9 SD = 1.5Gender: Factor 2 levels: FEMALE; MALEEthnicity: Factor 3 levels: ; White; BlackMilitary: Logical 31% TRUE 69% FALSEESL: Logical 8.1% TRUE 92% FALSEEdMother: Integer mean = 3.8 SD = 1.5EdFather: Integer mean = 3.7 SD = 1.7Age: Numeric mean = 37 SD = 9Employment: Integer mean = 2.7 SD = 0.68Income: Numeric mean = 5.1 SD = 2.3Transfer: Numeric mean = 52 SD = 25GPA: Numeric mean = 3.2 SD = 0.57GradeCode: Character 6 unique valuesLevel: Factor 2 levels: Lower; UpperID: Integer mean = 572 SD = 330treat2: Logical 20% TRUE 80% FALSE","code":"\ndata(tutoring, package='TriMatch')\ntutoring$treat2 <- tutoring$treat != 'Control'\ntable(tutoring$Course, tutoring$treat)##          \n##           Control Treat1 Treat2\n##   ENG*101     349     22     31\n##   ENG*201     518     36     32\n##   HSC*310      51     76     27"},{"path":"chapter-introduction.html","id":"pisa","chapter":"1 Introduction","heading":"1.5.4 Programme of International Student Assessment (pisana)","text":"Programme International Student Assessment (PISA) study conducted OECD every three years measure 15-year-olds’ academic abilities reading, mathematics, science along rich set demographic background information. pisana dataset included multilevelPSA package contains results 2009 study North America (.e. Canada, Mexico, United States).Country: Character 3 unique valuesCNT: Character 3 unique valuesSCHOOLID: Factor 1,534 levelsST01Q01: Factor 0 levels: NA (66,548 missing values)ST04Q01: Factor 2 levels: Female; MaleST05Q01: Factor 3 levels: Yes, one year; Yes, one year less; NoST06Q01: Numeric mean = 5.7 SD = 0.81ST07Q01: Factor 3 levels: , never; Yes, ; Yes, twice moreST08Q01: Factor 2 levels: Yes; NoST08Q02: Factor 2 levels: Yes; NoST08Q03: Factor 2 levels: ; YesST08Q04: Factor 2 levels: Yes; NoST08Q05: Factor 2 levels: ; YesST08Q06: Factor 2 levels: ; YesST10Q01: Factor 5 levels: <ISCED level 3A>; <ISCED level 2>; <ISCED level 3B, 3C>; complete <ISCED level 1>; <ISCED level 1>ST12Q01: Factor 4 levels: Working Full-time; Working Part-Time; ; Looking workST14Q01: Factor 5 levels: <ISCED level 3A>; <ISCED level 2>; <ISCED level 1>; Complete <ISCED level 1>; <ISCED level 3B, 3C>ST16Q01: Factor 4 levels: Working Full-time; Working Part-Time; Looking work; OtherST19Q01: Factor 2 levels: Language test; Another languageST20Q01: Factor 2 levels: Yes; NoST20Q02: Factor 2 levels: Yes; NoST20Q03: Factor 2 levels: Yes; NoST20Q04: Factor 2 levels: Yes; NoST20Q05: Factor 2 levels: Yes; NoST20Q06: Factor 2 levels: Yes; NoST20Q07: Factor 2 levels: Yes; NoST20Q08: Factor 2 levels: Yes; NoST20Q09: Factor 2 levels: Yes; NoST20Q10: Factor 2 levels: Yes; NoST20Q12: Factor 2 levels: Yes; NoST20Q13: Factor 2 levels: Yes; NoST20Q14: Factor 2 levels: Yes; NoST21Q01: Factor 4 levels: Three ; Two; One; NoneST21Q02: Factor 4 levels: Three ; Two; One; NoneST21Q03: Factor 4 levels: Three ; One; Two; NoneST21Q04: Factor 4 levels: Two; One; Three ; NoneST21Q05: Factor 4 levels: One; Two; Three ; NoneST22Q01: Factor 6 levels: 26-100 books; 0-10 books; 201-500 books; 11-25 books; 101-200 books; 500 booksST23Q01: Factor 5 levels: 2 hours day; 30 minutes less day; don’t read enjoyment; 30 60 minutes; 1 2 hours dayST31Q01: Factor 2 levels: ; YesST31Q02: Factor 2 levels: ; YesST31Q03: Factor 2 levels: ; YesST31Q05: Factor 2 levels: ; YesST31Q06: Factor 2 levels: ; YesST31Q07: Factor 2 levels: Yes; NoST32Q01: Factor 5 levels: attend; Less 2 hours week; 2 4 Hours week; 6 hours week; 4 6 hours per weekST32Q02: Factor 5 levels: Less 2 hours week; attend; 6 hours week; 2 4 Hours week; 4 6 hours per weekST32Q03: Factor 5 levels: attend; Less 2 hours week; 4 6 hours per week; 2 4 Hours week; 6 hours weekPV1MATH: Numeric mean = 461 SD = 92PV2MATH: Numeric mean = 461 SD = 92PV3MATH: Numeric mean = 461 SD = 92PV4MATH: Numeric mean = 461 SD = 92PV5MATH: Numeric mean = 461 SD = 92PV1READ: Numeric mean = 465 SD = 94PV2READ: Numeric mean = 465 SD = 94PV3READ: Numeric mean = 465 SD = 94PV4READ: Numeric mean = 465 SD = 94PV5READ: Numeric mean = 465 SD = 94PV1SCIE: Numeric mean = 460 SD = 94PV2SCIE: Numeric mean = 460 SD = 94PV3SCIE: Numeric mean = 460 SD = 94PV4SCIE: Numeric mean = 460 SD = 94PV5SCIE: Numeric mean = 460 SD = 94PUBPRIV: Factor 2 levels: Public; PrivateSTRATIO: Numeric mean = 26 SD = 31 (8,576 missing values)","code":"\ndata(pisana, package='multilevelPSA')"},{"path":"chapter-introduction.html","id":"nmes","chapter":"1 Introduction","heading":"1.5.5 National Medical Expenditure Study (nmes)","text":"National Medical Expenditure Study dataset used Imai Dyk (2004) evaluating method non-binary treatments. study examined relationship smoking status medical expenditures.PIDX: Integer mean = 2.9e+07 SD = 5,107,973LASTAGE: Integer mean = 46 SD = 19MALE: Integer mean = 0.44 SD = 0.5RACE3: Factor 3 levels: 3; 1; 2eversmk: Integer mean = 0.52 SD = 0.5current: Integer mean = 0.55 SD = 0.5 (9,872 missing values)former: Integer mean = 0.23 SD = 0.42smoke: Factor 3 levels: 0; 1; 2AGESMOKE: Integer mean = 18 SD = 5.4 (10,382 missing values)CIGSSMOK: Integer mean = 18 SD = 12 (11,362 missing values)SMOKENOW: Integer mean = 1.4 SD = 0.5 (9,872 missing values)SMOKED: Integer mean = 1.5 SD = 0.5CIGSADAY: Integer mean = 19 SD = 12 (14,990 missing values)AGESTOP: Integer mean = 39 SD = 16 (16,242 missing values)packyears: Numeric mean = 12 SD = 21 (1,119 missing values)yearsince: Integer mean = 3 SD = 8.2 (416 missing values)INCALPER: Numeric mean = 7,171 SD = 3,560HSQACCWT: Numeric mean = 7,850 SD = 3,796TOTALEXP: Numeric mean = 1,947 SD = 6,207TOTALSP3: Numeric mean = 494 SD = 3,418lc5: Integer mean = 0.011 SD = 0.1chd5: Integer mean = 0.053 SD = 0.23beltuse: Factor 3 levels: 3; 2; 1educate: Factor 4 levels: 1; 2; 3; 4marital: Factor 5 levels: 2; 1; 5; 3; 4; NA (76 missing values)SREGION: Factor 4 levels: 1; 2; 3; 4POVSTALB: Factor 5 levels: 1; 3; 4; 5; 2; NA (85 missing values)flag: Integer mean = 0.15 SD = 0.53age: Integer mean = 0.56 SD = 0.5","code":"\ndata(nmes, package='TriMatch')"},{"path":"chapter-stratification.html","id":"chapter-stratification","chapter":"2 Stratification","heading":"2 Stratification","text":"stratifyverb: stratify; 3rd person present: stratifies; past tense: stratified; past participle: stratified; gerund present participle: stratifying\n1. arrange classify.\n2. form arrange strata.Propensity score stratification leverages propensity scores can define strata (groups) roughly equivalent observed covariates. Although reasonable start chapter 3 matching, stratification important method even prefer use matching method, stratification often used order evaluate balance.","code":""},{"path":"chapter-stratification.html","id":"phase-i-estimate-propensity-scores-logistic-regression","chapter":"2 Stratification","heading":"2.1 Phase I: Estimate Propensity Scores (Logistic regression)","text":"begin let’s estimate propensity scores using logistic regression National Supported Work Demonostration (lalonde) dataset (Lalonde 1986). , using final model specification used Dehejia Wahba (1999).Check distributions propensity scores ensure good overlap","code":"\ndata(lalonde, package = 'Matching')\nlalonde_formu <- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\nlr_out <- glm(formula = lalonde_formu,\n              data = lalonde,\n              family = binomial(link = 'logit'))\nsummary(lr_out)## \n## Call:\n## glm(formula = lalonde_formu, family = binomial(link = \"logit\"), \n##     data = lalonde)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6632  -0.9699  -0.9043   1.2406   1.7627  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)  4.008e+00  2.155e+00   1.860   0.0629 .\n## age          1.372e-02  8.929e-02   0.154   0.8779  \n## I(age^2)    -2.535e-04  1.472e-03  -0.172   0.8632  \n## educ        -8.612e-01  4.154e-01  -2.073   0.0382 *\n## I(educ^2)    4.482e-02  2.334e-02   1.920   0.0549 .\n## black       -2.933e-01  3.679e-01  -0.797   0.4253  \n## hisp        -9.472e-01  5.127e-01  -1.847   0.0647 .\n## married      1.730e-01  2.826e-01   0.612   0.5404  \n## nodegr      -4.280e-01  3.917e-01  -1.093   0.2745  \n## re74         4.815e-06  5.689e-05   0.085   0.9326  \n## I(re74^2)   -1.692e-09  1.999e-09  -0.846   0.3974  \n## re75         1.273e-04  8.310e-05   1.532   0.1256  \n## I(re75^2)   -4.623e-09  4.384e-09  -1.055   0.2916  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 581.07  on 432  degrees of freedom\n## AIC: 607.07\n## \n## Number of Fisher Scoring iterations: 4\nlalonde$lr_ps <- fitted(lr_out)\nggplot(lalonde, aes(x = lr_ps, color = as.logical(treat))) + \n    geom_density() +\n    scale_color_manual('Treatment', values = palette2) +\n    xlab('Propensity Score')"},{"path":"chapter-stratification.html","id":"stratifying","chapter":"2 Stratification","heading":"2.1.1 Stratifying","text":"Stratification using quintiles.\nFigure 2.1: Distribution propensity scores strata breaks\n\nFigure 2.2: Scatter plot propensity scores log real earnings 1978 treatment strata breaks\n","code":"\nbreaks5 <- psa::get_strata_breaks(lalonde$lr_ps)\nbreaks5## $breaks\n##         0%        20%        40%        60%        80%       100% \n## 0.08491513 0.34032233 0.35943734 0.40797660 0.51119006 0.83047510 \n## \n## $labels\n##     strata       xmin      xmax      xmid\n## 0%       A 0.08491513 0.3403223 0.2126187\n## 20%      B 0.34032233 0.3594373 0.3498798\n## 40%      C 0.35943734 0.4079766 0.3837070\n## 60%      D 0.40797660 0.5111901 0.4595833\n## 80%      E 0.51119006 0.8304751 0.6708326\nlalonde$lr_strata5 <- cut(x = lalonde$lr_ps, \n                          breaks = breaks5$breaks, \n                          include.lowest = TRUE, \n                          labels = breaks5$labels$strata)\ntable(lalonde$treat, lalonde$lr_strata5)##    \n##      A  B  C  D  E\n##   0 66 61 51 42 40\n##   1 23 28 38 47 49"},{"path":"chapter-stratification.html","id":"stratification-balance","chapter":"2 Stratification","heading":"2.1.2 Checking Balance","text":"\nFigure 2.3: Covariate balance plots categorical variables\n\nFigure 2.4: Covariate balance plots numeric variables\n","code":"\ncovars <- all.vars(lalonde.formu)\ncovars <- lalonde[,covars[-1]]\nPSAgraphics::cv.bal.psa(covariates = covars, \n                        treatment = lalonde$treat,\n                        propensity = lalonde$lr_ps,\n                        strata = lalonde$lr_strata)\nPSAgraphics::box.psa(continuous = lalonde$age, \n                     treatment = lalonde$treat, \n                     strata = lalonde$lr_strata,\n                     xlab = \"Strata\", \n                     balance = FALSE)\nPSAgraphics::cat.psa(categorical = lalonde$nodegr, \n                     treatment = lalonde$treat, \n                     strata = lalonde$lr_strata, \n                     xlab = 'Strata',\n                     balance = FALSE)"},{"path":"chapter-stratification.html","id":"phase-ii-estimate-effects","chapter":"2 Stratification","heading":"2.2 Phase II: Estimate Effects","text":"","code":"\nPSAgraphics::loess.psa(response = log(lalonde$re78 + 1),\n                       treatment = lalonde$treat,\n                       propensity = lalonde$lr_ps)## $ATE\n## [1] 0.9008386\n## \n## $se.wtd\n## [1] 0.3913399\n## \n## $CI95\n## [1] 0.1181588 1.6835185\n## \n## $summary.strata\n##    counts.0 counts.1  means.0  means.1 diff.means\n## 1        34       11 6.268705 6.474912  0.2062076\n## 2        32       12 5.491717 5.659280  0.1675631\n## 3        31       14 5.467712 5.703584  0.2358722\n## 4        30       14 5.425593 5.747613  0.3220194\n## 5        27       18 5.397146 5.831117  0.4339703\n## 6        24       20 5.302660 6.339721  1.0370619\n## 7        21       23 5.125331 6.607936  1.4826043\n## 8        21       24 5.036908 6.594808  1.5578999\n## 9        22       22 5.182703 6.981383  1.7986801\n## 10       18       27 6.047529 7.820786  1.7732573\npsa::loess.plot(x = lalonde$lr_ps,\n                response = log(lalonde$re78 + 1),\n                treatment = lalonde$treat == 1,\n                responseTitle = 'log(re78)',\n                plot.strata = 5,\n                points.treat.alpha = 0.5,\n                points.control.alpha = 0.5,\n                percentPoints.treat = 1,\n                percentPoints.control = 1,\n                se = FALSE, \n                formula = y ~ x,\n                method = 'loess')\nPSAgraphics::circ.psa(response = log(lalonde$re78 + 1), \n                      treatment = lalonde$treat == 1, \n                      strata = lalonde$lr_strata5)## $summary.strata\n##   n.FALSE n.TRUE means.FALSE means.TRUE\n## A      66     23    6.280406   6.600537\n## B      61     28    4.409935   5.129193\n## C      51     38    6.212981   6.455034\n## D      42     47    4.705981   6.208840\n## E      40     49    5.783529   7.576461\n## \n## $wtd.Mn.FALSE\n## [1] 5.478567\n## \n## $wtd.Mn.TRUE\n## [1] 6.394013\n## \n## $ATE\n## [1] 0.9154463\n## \n## $se.wtd\n## [1] 0.394155\n## \n## $approx.t\n## [1] 2.322554\n## \n## $df\n## [1] 435\n## \n## $CI.95\n## [1] 0.1407612 1.6901314"},{"path":"chapter-stratification.html","id":"phase-iii-sensitivity-analysis-1","chapter":"2 Stratification","heading":"2.3 Phase III: Sensitivity Analysis","text":"Now established statistically significant effect intervention adjusting selection bias using propensity scores want evaluate robustness effect. Sensitivity analysis one approach well defined matching methods. chapter 7 introduce bootstrapping method can help test robustness. Rosenbaum (2012) suggest another approach test sensitivity test null hypothesis twice. using classification tree approach estimating propensity scores strata.","code":""},{"path":"chapter-stratification.html","id":"estimate-propensity-scores-classification-tree","chapter":"2 Stratification","heading":"2.3.1 Estimate Propensity Scores (classification tree)","text":"\n(#fig:tree_plot)Classification tree\n","code":"\nlibrary(tree)\ntree_out <- tree::tree(lalonde_formu,\n                       data = lalonde)\nplot(tree_out); text(tree_out)\nlalonde$tree_ps <- predict(tree_out)\ntable(lalonde$tree_ps, lalonde$treat, useNA = 'ifany')##                    \n##                       0   1\n##   0.332             167  83\n##   0.344827586206897  19  10\n##   0.351851851851852  35  19\n##   0.612903225806452  24  38\n##   0.659090909090909  15  29\n##   1                   0   6\nlalonde$tree_strata <- predict(tree_out, type = 'where')\ntable(lalonde$tree_strata, lalonde$treat, useNA = 'ifany')##     \n##        0   1\n##   3  167  83\n##   5   15  29\n##   6   35  19\n##   9   24  38\n##   10  19  10\n##   11   0   6"},{"path":"chapter-matching.html","id":"chapter-matching","chapter":"3 Matching","heading":"3 Matching","text":"matchverb\n1. correspond cause correspond essential respect; make harmonious.\n2. equal (something) quality strength.name suggests, propensity score matching concerned matching treatment control observations…\nFigure 3.1: Propensity Scores Logistic Regression Sample Matched Pairs\n","code":"\nlr_out <- glm(lalonde.formu, \n              data = lalonde,\n              family = binomial(link = logit))\nlalonde$lr_ps <- fitted(lr_out)  # Propensity scores"},{"path":"chapter-matching.html","id":"one-to-one-matching","chapter":"3 Matching","heading":"3.1 One-to-One Matching","text":"One--one matching replacement (M = 1 option). Estimating treatment effect treated (default ATT).","code":"\nrr_att <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                estimand='ATT')\nsummary(rr_att) # The default estimate is ATT here## \n## Estimate...  2153.3 \n## AI SE......  825.4 \n## T-stat.....  2.6088 \n## p.val......  0.0090858 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  346"},{"path":"chapter-matching.html","id":"checking-balance","chapter":"3 Matching","heading":"3.1.1 Checking Balance","text":"","code":"\nrr.ate <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                estimand = 'ATE')\nsummary(rr.ate)## \n## Estimate...  2013.3 \n## AI SE......  817.76 \n## T-stat.....  2.4619 \n## p.val......  0.013819 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  445 \n## Matched number of observations  (unweighted).  756\nls(rr.ate)##  [1] \"caliper\"           \"ecaliper\"          \"est\"              \n##  [4] \"est.noadj\"         \"estimand\"          \"exact\"            \n##  [7] \"index.control\"     \"index.dropped\"     \"index.treated\"    \n## [10] \"MatchLoopC\"        \"mdata\"             \"ndrops\"           \n## [13] \"ndrops.matches\"    \"nobs\"              \"orig.nobs\"        \n## [16] \"orig.treated.nobs\" \"orig.wnobs\"        \"se\"               \n## [19] \"se.cond\"           \"se.standard\"       \"version\"          \n## [22] \"weights\"           \"wnobs\"\nrr2 <- Match(Y = lalonde$re78,      \n             Tr = lalonde$treat, \n             X = lalonde$lr_ps,\n             M = 1, \n             ties = TRUE, \n             replace = TRUE,\n             estimand = 'ATT')\nsummary(rr2) # The default estimate is ATT here## \n## Estimate...  2153.3 \n## AI SE......  825.4 \n## T-stat.....  2.6088 \n## p.val......  0.0090858 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  346\nlength(unique(rr2$index.control))## 173.00\nls(rr2)##  [1] \"caliper\"           \"ecaliper\"          \"est\"              \n##  [4] \"est.noadj\"         \"estimand\"          \"exact\"            \n##  [7] \"index.control\"     \"index.dropped\"     \"index.treated\"    \n## [10] \"MatchLoopC\"        \"mdata\"             \"ndrops\"           \n## [13] \"ndrops.matches\"    \"nobs\"              \"orig.nobs\"        \n## [16] \"orig.treated.nobs\" \"orig.wnobs\"        \"se\"               \n## [19] \"se.cond\"           \"se.standard\"       \"version\"          \n## [22] \"weights\"           \"wnobs\"\n## Using the Matchit package\nmatchit.out <- matchit(lalonde.formu, data = lalonde)\nsummary(matchit.out)## \n## Call:\n## matchit(formula = lalonde.formu, data = lalonde)\n## \n## Summary of Balance for All Data:\n##           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance         0.4468        0.3936          0.4533     1.2101    0.1340\n## age             25.8162       25.0538          0.1066     1.0278    0.0254\n## I(age^2)       717.3946      677.3154          0.0929     1.0115    0.0254\n## educ            10.3459       10.0885          0.1281     1.5513    0.0287\n## I(educ^2)      111.0595      104.3731          0.1701     1.6625    0.0287\n## black            0.8432        0.8269          0.0449          .    0.0163\n## hisp             0.0595        0.1077         -0.2040          .    0.0482\n## married          0.1892        0.1538          0.0902          .    0.0353\n## nodegr           0.7081        0.8346         -0.2783          .    0.1265\n## re74          2095.5740     2107.0268         -0.0023     0.7381    0.0192\n## I(re74^2) 28141433.9907 36667413.1577         -0.0747     0.5038    0.0192\n## re75          1532.0556     1266.9092          0.0824     1.0763    0.0508\n## I(re75^2) 12654752.6909 11196530.0057          0.0260     1.4609    0.0508\n## u74              0.7081        0.7500         -0.0921          .    0.0419\n## u75              0.6000        0.6846         -0.1727          .    0.0846\n##           eCDF Max\n## distance    0.2244\n## age         0.0652\n## I(age^2)    0.0652\n## educ        0.1265\n## I(educ^2)   0.1265\n## black       0.0163\n## hisp        0.0482\n## married     0.0353\n## nodegr      0.1265\n## re74        0.0471\n## I(re74^2)   0.0471\n## re75        0.1075\n## I(re75^2)   0.1075\n## u74         0.0419\n## u75         0.0846\n## \n## Summary of Balance for Matched Data:\n##           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance         0.4468        0.4277          0.1627     1.2887    0.0400\n## age             25.8162       25.7514          0.0091     1.0691    0.0151\n## I(age^2)       717.3946      710.7568          0.0154     0.9977    0.0151\n## educ            10.3459       10.2054          0.0699     1.2231    0.0162\n## I(educ^2)      111.0595      107.4378          0.0921     1.3309    0.0162\n## black            0.8432        0.8324          0.0297          .    0.0108\n## hisp             0.0595        0.0811         -0.0914          .    0.0216\n## married          0.1892        0.1946         -0.0138          .    0.0054\n## nodegr           0.7081        0.7676         -0.1308          .    0.0595\n## re74          2095.5740     2168.6782         -0.0150     1.0398    0.0125\n## I(re74^2) 28141433.9907 27544255.1664          0.0052     1.4319    0.0125\n## re75          1532.0556     1482.8937          0.0153     1.1270    0.0181\n## I(re75^2) 12654752.6909 11344693.3166          0.0234     2.2884    0.0181\n## u74              0.7081        0.7027          0.0119          .    0.0054\n## u75              0.6000        0.5946          0.0110          .    0.0054\n##           eCDF Max Std. Pair Dist.\n## distance    0.1189          0.1627\n## age         0.0378          0.9912\n## I(age^2)    0.0378          0.9340\n## educ        0.0595          0.8442\n## I(educ^2)   0.0595          0.8353\n## black       0.0108          0.6244\n## hisp        0.0216          0.2743\n## married     0.0054          0.6211\n## nodegr      0.0595          0.5588\n## re74        0.0324          0.6787\n## I(re74^2)   0.0324          0.4400\n## re75        0.0486          0.7713\n## I(re75^2)   0.0486          0.4068\n## u74         0.0054          0.8204\n## u75         0.0054          0.7613\n## \n## Sample Sizes:\n##           Control Treated\n## All           260     185\n## Matched       185     185\n## Unmatched      75       0\n## Discarded       0       0\n# Same as above but calculate average treatment effect\nrr.ate <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                ties = FALSE, \n                replace = FALSE, \n                estimand='ATE')\nsummary(rr.ate) # Here the estimate is ATE## \n## Estimate...  2125.5 \n## SE.........  493.96 \n## T-stat.....  4.3029 \n## p.val......  1.6859e-05 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  370 \n## Matched number of observations  (unweighted).  370\n## Genetic Matching\nrr.gen <- GenMatch(Tr = lalonde$treat, \n                   X = lalonde$lr_ps, \n                   BalanceMatrix = lalonde[,all.vars(lalonde.formu)[-1]],\n                   estimand = 'ATE', \n                   M = 1, \n                   pop.size = 16)## \n## \n## Mon Apr 10 16:17:13 2023\n## Domains:\n##  0.000000e+00   <=  X1   <=    1.000000e+03 \n## \n## Data Type: Floating Point\n## Operators (code number, name, population) \n##  (1) Cloning...........................  1\n##  (2) Uniform Mutation..................  2\n##  (3) Boundary Mutation.................  2\n##  (4) Non-Uniform Mutation..............  2\n##  (5) Polytope Crossover................  2\n##  (6) Simple Crossover..................  2\n##  (7) Whole Non-Uniform Mutation........  2\n##  (8) Heuristic Crossover...............  2\n##  (9) Local-Minimum Crossover...........  0\n## \n## SOFT Maximum Number of Generations: 100\n## Maximum Nonchanging Generations: 4\n## Population size       : 16\n## Convergence Tolerance: 1.000000e-03\n## \n## Not Using the BFGS Derivative Based Optimizer on the Best Individual Each Generation.\n## Not Checking Gradients before Stopping.\n## Using Out of Bounds Individuals.\n## \n## Maximization Problem.\n## GENERATION: 0 (initializing the population)\n## Lexical Fit..... 9.235231e-02  1.757822e-01  2.857138e-01  2.857138e-01  3.950411e-01  3.950411e-01  4.483849e-01  7.409596e-01  7.643859e-01  7.643859e-01  7.647535e-01  7.691713e-01  7.691713e-01  8.592751e-01  8.592751e-01  9.160976e-01  9.160976e-01  9.681165e-01  9.786492e-01  9.823810e-01  \n## #unique......... 16, #Total UniqueCount: 16\n## var 1:\n## best............ 3.324648e+01\n## mean............ 4.638550e+02\n## variance........ 8.779735e+04\n## \n## GENERATION: 1\n## Lexical Fit..... 1.067431e-01  2.553994e-01  2.553994e-01  3.549996e-01  3.913630e-01  3.913630e-01  4.747623e-01  5.084810e-01  6.420853e-01  6.420853e-01  6.665787e-01  7.967582e-01  8.111685e-01  8.111685e-01  8.592751e-01  8.592751e-01  9.160976e-01  9.160976e-01  9.264048e-01  9.941933e-01  \n## #unique......... 10, #Total UniqueCount: 26\n## var 1:\n## best............ 4.206134e+00\n## mean............ 2.117860e+02\n## variance........ 4.296951e+04\n## \n## GENERATION: 2\n## Lexical Fit..... 1.134648e-01  2.173453e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.939824e-01  5.024479e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.112194e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.794830e-01  9.805886e-01  \n## #unique......... 8, #Total UniqueCount: 34\n## var 1:\n## best............ 2.699388e+00\n## mean............ 7.152003e+01\n## variance........ 2.503090e+04\n## \n## GENERATION: 3\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 9, #Total UniqueCount: 43\n## var 1:\n## best............ 2.608864e+00\n## mean............ 2.299597e+01\n## variance........ 2.361439e+03\n## \n## GENERATION: 4\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 8, #Total UniqueCount: 51\n## var 1:\n## best............ 2.608864e+00\n## mean............ 9.058378e+01\n## variance........ 4.078986e+04\n## \n## GENERATION: 5\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 6, #Total UniqueCount: 57\n## var 1:\n## best............ 2.608864e+00\n## mean............ 1.344191e+02\n## variance........ 7.663447e+04\n## \n## GENERATION: 6\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 6, #Total UniqueCount: 63\n## var 1:\n## best............ 2.608864e+00\n## mean............ 4.415709e+01\n## variance........ 1.768958e+04\n## \n## GENERATION: 7\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 6, #Total UniqueCount: 69\n## var 1:\n## best............ 2.608864e+00\n## mean............ 4.706867e+01\n## variance........ 7.517895e+03\n## \n## GENERATION: 8\n## Lexical Fit..... 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## #unique......... 7, #Total UniqueCount: 76\n## var 1:\n## best............ 2.608864e+00\n## mean............ 7.701794e+01\n## variance........ 4.010500e+04\n## \n## 'wait.generations' limit reached.\n## No significant improvement in 4 generations.\n## \n## Solution Lexical Fitness Value:\n## 1.134648e-01  2.188211e-01  2.857138e-01  2.857138e-01  3.432276e-01  3.432276e-01  4.959769e-01  5.104434e-01  6.832321e-01  6.832321e-01  7.316137e-01  7.643859e-01  7.643859e-01  8.126852e-01  9.055825e-01  9.055825e-01  9.160976e-01  9.160976e-01  9.797994e-01  9.805886e-01  \n## \n## Parameters at the Solution:\n## \n##  X[ 1] : 2.608864e+00\n## \n## Solution Found Generation 3\n## Number of Generations Run 8\n## \n## Mon Apr 10 16:17:14 2023\n## Total run time : 0 hours 0 minutes and 1 seconds\nrr.gen.mout <- Match(Y = lalonde$re78, \n                     Tr = lalonde$treat, \n                     X = lalonde$lr_ps,\n                     estimand = 'ATE',\n                     Weight.matrix = rr.gen)\nsummary(rr.gen.mout)## \n## Estimate...  2153.5 \n## AI SE......  815.34 \n## T-stat.....  2.6412 \n## p.val......  0.0082608 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  445 \n## Matched number of observations  (unweighted).  653\n## Partial exact matching\nrr2 <- Matchby(Y = lalonde$re78, \n               Tr = lalonde$treat, \n               X = lalonde$lr_ps, \n               by = factor(lalonde$nodegr))## 1 of 2 groups\n## 2 of 2 groups\nsummary(rr2)## \n## Estimate...  2332.5 \n## SE.........  688.25 \n## T-stat.....  3.3891 \n## p.val......  0.0007012 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185\n## Partial exact matching on two covariates\nrr3 <- Matchby(Y = lalonde$re78, \n               Tr = lalonde$treat, \n               X = lalonde$lr_ps, \n               by = lalonde[,c('nodegr','married')])## 1 of 4 groups\n## 2 of 4 groups\n## 3 of 4 groups\n## 4 of 4 groups\nsummary(rr3)## \n## Estimate...  2017.6 \n## SE.........  713.91 \n## T-stat.....  2.8262 \n## p.val......  0.0047104 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185"},{"path":"chapter-weighting.html","id":"chapter-weighting","chapter":"4 Weighting","heading":"4 Weighting","text":"weightverb\n1. hold (something) placing heavy object top .\n2. attach importance value .Propensity score weighting approach using propensity scores weights statistical models regression ANOVA. Like stratification (see Chapter 2), propensity score weighting advantage observations. section 1.3.2 introduced four different treatment estimators. histograms used conceptually explain observations included, included, calculation used propensity score weights. chapter discuss mathematical details weights calculated applied, include R code generate estimates.present formula treatment effects wish estimate. formulas define weights. weights can use statistical model using following formula estimate treatment effect.\\[\\begin{equation}\n\\begin{aligned}\nTreatment\\ Effect = \\frac{\\sum Y_{}Z_{}w_{}}{\\sum Z_{} w_{}} - \\frac{\\sum Y_{}(1 - Z_{}) w_{}}{\\sum (1 - Z_{}) w_{} }\n\\end{aligned}\n\\tag{4.1}\n\\end{equation}\\]equation (4.1), \\(w\\) weight (defined following sections), \\(Z_i\\) treatment assignment \\(Z = 1\\) treatment \\(Z = 0\\) control, \\(Y_i\\) outcome.","code":""},{"path":"chapter-weighting.html","id":"estimate-propensity-scores","chapter":"4 Weighting","heading":"4.1 Estimate Propensity Scores","text":"begin, estimate propensity scores, using logistic regression.","code":"\ndata(\"lalonde\", package = 'Matching')\nlr_out <- glm(formula = lalonde.formu,\n              data = lalonde,\n              family = binomial(link = 'logit'))\nlalonde$lr_ps <- fitted(lr_out)"},{"path":"chapter-weighting.html","id":"checking-balance-1","chapter":"4 Weighting","heading":"4.2 Checking Balance","text":"Checking balance propensity score weighting stratification. Figure 4.1 multiple covariate balance assessment plot. See section 2.1.2 stratification chapter details can check balance individual covariates.\nFigure 4.1: Multiple covariate balance assessment plot Lalonde data estimating propensity scores logistic regression\none additional balance check can done propensity score weights. can run propensity score estimation model estimated propensity score weights. result covariates non-statistically significant effect treatment. explore details four treatment effects discussed next section.","code":"\nPSAgraphics::cv.bal.psa(covariates = lalonde[,all.vars(lalonde.formu)[-1]],\n                        treatment = lalonde$treat,\n                        propensity = lalonde$lr_ps,\n                        strata = 5)"},{"path":"chapter-weighting.html","id":"average-treatment-effect-ate-1","chapter":"4 Weighting","heading":"4.3 Average Treatment Effect (ATE)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATE} = \\frac{Z_i}{\\pi_i} + \\frac{1 - Z_i}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.2}\n\\end{equation}\\]","code":"\nate_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps,                          \n                                         estimand = 'ATE')"},{"path":"chapter-weighting.html","id":"check-balance-with-ate-weights","chapter":"4 Weighting","heading":"4.3.1 Check Balance with ATE Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = ate_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = ate_weights)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -2.217  -1.498  -1.405   1.720   2.562  \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -2.000e-01  1.977e+00  -0.101    0.919\n## age          2.686e-02  8.513e-02   0.316    0.753\n## I(age^2)    -4.695e-04  1.397e-03  -0.336    0.737\n## educ        -6.326e-02  4.024e-01  -0.157    0.875\n## I(educ^2)    3.510e-03  2.259e-02   0.155    0.877\n## black       -3.695e-03  3.714e-01  -0.010    0.992\n## hisp         2.232e-02  4.904e-01   0.046    0.964\n## married     -8.664e-03  2.784e-01  -0.031    0.975\n## nodegr       4.154e-02  3.889e-01   0.107    0.915\n## re74         2.291e-05  7.493e-05   0.306    0.760\n## I(re74^2)   -9.734e-10  2.337e-09  -0.416    0.677\n## re75         5.001e-06  1.015e-04   0.049    0.961\n## I(re75^2)   -3.543e-10  5.032e-09  -0.070    0.944\n## u74          8.163e-02  4.449e-01   0.183    0.854\n## u75         -4.153e-04  3.566e-01  -0.001    0.999\n## \n## (Dispersion parameter for quasibinomial family taken to be 2.067727)\n## \n##     Null deviance: 1232.6  on 444  degrees of freedom\n## Residual deviance: 1231.9  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-ate","chapter":"4 Weighting","heading":"4.3.2 Estimate ATE","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = ate_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = ate_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     4556        450  10.125   <2e-16 ***\n## treat           1558        637   2.446   0.0148 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9497 on 443 degrees of freedom\n## Multiple R-squared:  0.01333,    Adjusted R-squared:  0.0111 \n## F-statistic: 5.983 on 1 and 443 DF,  p-value: 0.01483\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = ate_weights)## 1558.09"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-treated-att-1","chapter":"4 Weighting","heading":"4.4 Average Treatment Effect Among the Treated (ATT)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATT} = \\frac{\\pi_i Z_i}{\\pi_i} + \\frac{\\pi_i (1 - Z_i)}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.3}\n\\end{equation}\\]","code":"\natt_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATT')"},{"path":"chapter-weighting.html","id":"check-balance-with-att-weights","chapter":"4 Weighting","heading":"4.4.1 Check Balance with ATT Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = att_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = att_weights)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.9694  -0.9382  -0.8097   1.1718   1.2765  \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  1.122e-01  1.754e+00   0.064    0.949\n## age          2.350e-02  8.362e-02   0.281    0.779\n## I(age^2)    -4.382e-04  1.352e-03  -0.324    0.746\n## educ        -1.279e-01  3.424e-01  -0.374    0.709\n## I(educ^2)    7.725e-03  1.931e-02   0.400    0.689\n## black       -5.090e-02  3.388e-01  -0.150    0.881\n## hisp        -7.925e-02  5.202e-01  -0.152    0.879\n## married     -2.667e-02  2.691e-01  -0.099    0.921\n## nodegr       1.449e-01  3.623e-01   0.400    0.689\n## re74         9.327e-06  7.444e-05   0.125    0.900\n## I(re74^2)   -9.597e-11  2.521e-09  -0.038    0.970\n## re75        -1.340e-05  9.575e-05  -0.140    0.889\n## I(re75^2)    7.444e-10  4.817e-09   0.155    0.877\n## u74         -6.362e-02  4.320e-01  -0.147    0.883\n## u75          8.469e-02  3.390e-01   0.250    0.803\n## \n## (Dispersion parameter for quasibinomial family taken to be 0.8614842)\n## \n##     Null deviance: 513.54  on 444  degrees of freedom\n## Residual deviance: 513.06  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-att","chapter":"4 Weighting","heading":"4.4.2 Estimate ATT","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = att_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = att_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4557.4      454.2  10.033  < 2e-16 ***\n## treat         1791.7      642.8   2.787  0.00554 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6186 on 443 degrees of freedom\n## Multiple R-squared:  0.01724,    Adjusted R-squared:  0.01502 \n## F-statistic:  7.77 on 1 and 443 DF,  p-value: 0.00554\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = att_weights)## 1791.72"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-control-atc-1","chapter":"4 Weighting","heading":"4.5 Average Treatment Effect Among the Control (ATC)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATC} = \\frac{(1 - \\pi_i) Z_i}{\\pi_i} + \\frac{(1 - e_i)(1 - Z_i)}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.4}\n\\end{equation}\\]","code":"\natc_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATC')"},{"path":"chapter-weighting.html","id":"check-balance-with-atc-weights","chapter":"4 Weighting","heading":"4.5.1 Check Balance with ATC Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = atc_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = atc_weights)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.261  -1.182  -1.151   1.229   2.260  \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -6.598e-01  2.390e+00  -0.276    0.783\n## age          3.097e-02  8.728e-02   0.355    0.723\n## I(age^2)    -5.201e-04  1.449e-03  -0.359    0.720\n## educ         4.722e-02  4.975e-01   0.095    0.924\n## I(educ^2)   -3.225e-03  2.766e-02  -0.117    0.907\n## black        3.598e-02  4.033e-01   0.089    0.929\n## hisp         7.912e-02  4.941e-01   0.160    0.873\n## married      7.290e-03  2.868e-01   0.025    0.980\n## nodegr      -7.488e-02  4.205e-01  -0.178    0.859\n## re74         2.763e-05  7.658e-05   0.361    0.718\n## I(re74^2)   -1.296e-09  2.319e-09  -0.559    0.577\n## re75         2.037e-05  1.073e-04   0.190    0.849\n## I(re75^2)   -1.341e-09  5.282e-09  -0.254    0.800\n## u74          1.831e-01  4.577e-01   0.400    0.689\n## u75         -7.234e-02  3.730e-01  -0.194    0.846\n## \n## (Dispersion parameter for quasibinomial family taken to be 1.206136)\n## \n##     Null deviance: 719.09  on 444  degrees of freedom\n## Residual deviance: 717.84  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-atc","chapter":"4 Weighting","heading":"4.5.2 Estimate ATC","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = atc_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = atc_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4554.8      446.8  10.195   <2e-16 ***\n## treat         1391.0      632.6   2.199   0.0284 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7204 on 443 degrees of freedom\n## Multiple R-squared:  0.0108, Adjusted R-squared:  0.008564 \n## F-statistic: 4.835 on 1 and 443 DF,  p-value: 0.0284\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = atc_weights)## 1391.02"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-evenly-matched-atm-1","chapter":"4 Weighting","heading":"4.6 Average Treatment Effect Among the Evenly Matched (ATM)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATM} = \\frac{min\\{\\pi_i, 1 - \\pi_i\\}}{Z_i \\pi_i (1 - Z_i)(1 - \\pi_i)}\n\\end{aligned}\n\\tag{4.5}\n\\end{equation}\\]","code":"\natm_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATM')"},{"path":"chapter-weighting.html","id":"check-balance-with-atm-weights","chapter":"4 Weighting","heading":"4.6.1 Check Balance with ATM Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = atm_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = atm_weights)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.2349  -0.9347  -0.8207   1.1588   1.2256  \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  1.657e-01  2.129e+00   0.078    0.938\n## age         -2.418e-02  8.893e-02  -0.272    0.786\n## I(age^2)     4.192e-04  1.465e-03   0.286    0.775\n## educ         6.308e-02  4.304e-01   0.147    0.884\n## I(educ^2)   -3.347e-03  2.420e-02  -0.138    0.890\n## black        1.480e-02  3.577e-01   0.041    0.967\n## hisp        -3.495e-02  5.203e-01  -0.067    0.946\n## married     -3.486e-03  2.736e-01  -0.013    0.990\n## nodegr      -4.659e-02  3.825e-01  -0.122    0.903\n## re74        -2.333e-05  7.526e-05  -0.310    0.757\n## I(re74^2)    8.298e-10  2.510e-09   0.331    0.741\n## re75        -7.419e-06  9.758e-05  -0.076    0.939\n## I(re75^2)    7.957e-10  4.775e-09   0.167    0.868\n## u74         -6.630e-02  4.360e-01  -0.152    0.879\n## u75         -3.051e-02  3.436e-01  -0.089    0.929\n## \n## (Dispersion parameter for quasibinomial family taken to be 0.7850194)\n## \n##     Null deviance: 467.94  on 444  degrees of freedom\n## Residual deviance: 467.73  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 3"},{"path":"chapter-weighting.html","id":"estimate-atm","chapter":"4 Weighting","heading":"4.6.2 Estimate ATM","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = atm_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = atm_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4504.6      459.8   9.797  < 2e-16 ***\n## treat         1707.7      648.8   2.632  0.00878 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5960 on 443 degrees of freedom\n## Multiple R-squared:  0.0154, Adjusted R-squared:  0.01318 \n## F-statistic: 6.928 on 1 and 443 DF,  p-value: 0.008783\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = atm_weights)## 1707.69"},{"path":"chapter-missing.html","id":"chapter-missing","chapter":"5 Missing Data","heading":"5 Missing Data","text":"Create copy covariates simulate missing random (mar) missing random (nmar).Add missingness existing data. missing random data treatment units twice many missing values control group.proportion missing values first covariateCreate shadow matrix. logical vector cell TRUE value missing original data frame.Change column names include “_miss” name.Impute missing values using mice packageGet imputed data set.Estimate propensity scores using logistic regression.see two indicator columns shadow matrix statistically significant predictors suggesting data missing random.","code":"\nrequire(Matching)\nrequire(mice)\ndata(lalonde, package='Matching')\nTr <- lalonde$treat\nY <- lalonde$re78\nX <- lalonde[,c('age','educ','black','hisp','married','nodegr','re74','re75')]\nlalonde.glm <- glm(treat ~ ., family=binomial, data=cbind(treat=Tr, X))\nsummary(lalonde.glm)## \n## Call:\n## glm(formula = treat ~ ., family = binomial, data = cbind(treat = Tr, \n##     X))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4358  -0.9904  -0.9071   1.2825   1.6946  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  1.178e+00  1.056e+00   1.115  0.26474   \n## age          4.698e-03  1.433e-02   0.328  0.74297   \n## educ        -7.124e-02  7.173e-02  -0.993  0.32061   \n## black       -2.247e-01  3.655e-01  -0.615  0.53874   \n## hisp        -8.528e-01  5.066e-01  -1.683  0.09228 . \n## married      1.636e-01  2.769e-01   0.591  0.55463   \n## nodegr      -9.035e-01  3.135e-01  -2.882  0.00395 **\n## re74        -3.161e-05  2.584e-05  -1.223  0.22122   \n## re75         6.161e-05  4.358e-05   1.414  0.15744   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 587.22  on 436  degrees of freedom\n## AIC: 605.22\n## \n## Number of Fisher Scoring iterations: 4\nlalonde.mar <- X\nlalonde.nmar <- X\n\nmissing.rate <- .2 # What percent of rows will have missing data\nmissing.cols <- c('nodegr', 're75') # The columns we will add missing values to\n\n# Vectors indiciating which rows are treatment and control.\ntreat.rows <- which(lalonde$treat == 1)\ncontrol.rows <- which(lalonde$treat == 0)\nset.seed(2112)\nfor(i in missing.cols) {\n    lalonde.mar[sample(nrow(lalonde), nrow(lalonde) * missing.rate), i] <- NA\n    lalonde.nmar[sample(treat.rows, length(treat.rows) * missing.rate * 2), i] <- NA\n    lalonde.nmar[sample(control.rows, length(control.rows) * missing.rate), i] <- NA\n}\nprop.table(table(is.na(lalonde.mar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                  0          1\n##   FALSE 0.46292135 0.33707865\n##   TRUE  0.12134831 0.07865169\nprop.table(table(is.na(lalonde.nmar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                 0         1\n##   FALSE 0.4674157 0.2494382\n##   TRUE  0.1168539 0.1662921\nshadow.matrix.mar <- as.data.frame(is.na(lalonde.mar))\nshadow.matrix.nmar <- as.data.frame(is.na(lalonde.nmar))\nnames(shadow.matrix.mar) <- names(shadow.matrix.nmar) <- paste0(names(shadow.matrix.mar), '_miss')\nset.seed(2112)\nmice.mar <- mice(lalonde.mar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\nmice.nmar <- mice(lalonde.nmar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\ncomplete.mar <- complete(mice.mar)\ncomplete.nmar <- complete(mice.nmar)\nlalonde.mar.glm <- glm(treat~., data=cbind(treat=Tr, complete.mar, shadow.matrix.mar))\nlalonde.nmar.glm <- glm(treat~., data=cbind(treat=Tr, complete.nmar, shadow.matrix.nmar))\nsummary(lalonde.mar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.mar, \n##     shadow.matrix.mar))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7073  -0.3837  -0.3079   0.5404   0.7881  \n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       8.996e-01  2.504e-01   3.592 0.000366 ***\n## age               9.447e-04  3.395e-03   0.278 0.780957    \n## educ             -2.514e-02  1.706e-02  -1.474 0.141191    \n## black            -3.895e-02  8.746e-02  -0.445 0.656285    \n## hisp             -1.726e-01  1.156e-01  -1.493 0.136068    \n## married           3.008e-02  6.671e-02   0.451 0.652326    \n## nodegr           -2.672e-01  7.475e-02  -3.574 0.000390 ***\n## re74             -1.059e-05  5.681e-06  -1.863 0.063076 .  \n## re75              2.378e-05  1.059e-05   2.246 0.025227 *  \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE  -1.852e-02  5.853e-02  -0.316 0.751797    \n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE    -3.304e-02  5.870e-02  -0.563 0.573823    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2361624)\n## \n##     Null deviance: 108.09  on 444  degrees of freedom\n## Residual deviance: 102.49  on 434  degrees of freedom\n## AIC: 633.48\n## \n## Number of Fisher Scoring iterations: 2\nsummary(lalonde.nmar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.nmar, \n##     shadow.matrix.nmar))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7597  -0.3960  -0.2154   0.4926   0.8693  \n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       7.427e-01  2.319e-01   3.203 0.001459 ** \n## age               7.641e-04  3.254e-03   0.235 0.814441    \n## educ             -2.451e-02  1.584e-02  -1.547 0.122656    \n## black            -1.964e-02  8.493e-02  -0.231 0.817243    \n## hisp             -1.366e-01  1.113e-01  -1.228 0.220246    \n## married           4.426e-02  6.440e-02   0.687 0.492303    \n## nodegr           -2.572e-01  7.143e-02  -3.601 0.000354 ***\n## re74             -3.326e-06  5.324e-06  -0.625 0.532421    \n## re75              4.742e-06  9.893e-06   0.479 0.631935    \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE   2.300e-01  4.957e-02   4.639 4.64e-06 ***\n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE     2.246e-01  4.922e-02   4.563 6.57e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2164914)\n## \n##     Null deviance: 108.090  on 444  degrees of freedom\n## Residual deviance:  93.957  on 434  degrees of freedom\n## AIC: 594.78\n## \n## Number of Fisher Scoring iterations: 2"},{"path":"chapter-sensitivity.html","id":"chapter-sensitivity","chapter":"6 Sensitivity Analysis","heading":"6 Sensitivity Analysis","text":"","code":"\nrequire(rbounds)\ndata(lalonde, package='Matching')\n\nY  <- lalonde$re78   #the outcome of interest\nTr <- lalonde$treat #the treatment of interest\nattach(lalonde)\n# The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n# The covariates we want to obtain balance on\nBalanceMat <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74,\n                    I(re74*re75))\ndetach(lalonde)\n\ngen1 <- GenMatch(Tr=Tr, X=X, BalanceMat=BalanceMat, pop.size=50,\n                  data.type.int=FALSE, print=0, replace=FALSE)\nmgen1 <- Match(Y=Y, Tr=Tr, X=X, Weight.matrix=gen1, replace=FALSE)\nsummary(mgen1)## \n## Estimate...  1613.6 \n## SE.........  721.22 \n## T-stat.....  2.2373 \n## p.val......  0.025266 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185\nrbounds::psens(x = Y[mgen1$index.treated],\n      y =Y[mgen1$index.contro],\n      Gamma = 1.5, \n      GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n##  \n## Unconfounded estimate ....  0.0228 \n## \n##  Gamma Lower bound Upper bound\n##    1.0      0.0228      0.0228\n##    1.1      0.0056      0.0716\n##    1.2      0.0012      0.1640\n##    1.3      0.0002      0.2970\n##    1.4      0.0000      0.4516\n##    1.5      0.0000      0.6030\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## \nrbounds::hlsens(x = Y[mgen1$index.treated],\n       y = Y[mgen1$index.contro],\n       Gamma = 1.5, \n       GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n##  \n## Unconfounded estimate ....  1431.4 \n## \n##  Gamma Lower bound Upper bound\n##    1.0  1.4314e+03      1431.4\n##    1.1  7.9320e+02      1547.1\n##    1.2  4.9780e+02      1901.1\n##    1.3  2.0850e+02      2162.1\n##    1.4 -3.4140e-05      2441.0\n##    1.5 -2.1990e+02      2694.4\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## "},{"path":"chapter-bootstrapping.html","id":"chapter-bootstrapping","chapter":"7 Bootstrapping","heading":"7 Bootstrapping","text":"\nFigure 7.1: Mean difference across bootstrap samples method\nDetails available within returned object","code":"\nrequire(PSAboot)\n\nboot.matching.1to3 <- function(Tr, Y, X, X.trans, formu, ...) {\n    return(boot.matching(Tr=Tr, Y=Y, X=X, X.trans=X.trans, formu=formu, M=3, ...))\n}\n\n\nboot_out <- PSAboot(Tr = lalonde$treat == 1, \n                    Y = lalonde$re78, \n                    X = lalonde[,all.vars(lalonde.formu)[-1]], \n                    seed = 2112,\n                    methods=c('Stratification' = boot.strata,\n                              'ctree' = boot.ctree,\n                              'rpart' = boot.rpart,\n                              'Matching' = boot.matching,\n                              'Matching-1-to-3' = boot.matching.1to3,\n                              'MatchIt' = boot.matchit) )\nsummary(boot_out)## Stratification Results:\n##    Complete estimate = 1658\n##    Complete CI = [242, 3074]\n##    Bootstrap pooled estimate = 1476\n##    Bootstrap weighted pooled estimate = 1461\n##    Bootstrap pooled CI = [66.5, 2885]\n##    59% of bootstrap samples have confidence intervals that do not span zero.\n##       59% positive.\n##       0% negative.\n## ctree Results:\n##    Complete estimate = 1598\n##    Complete CI = [-6.62, 3203]\n##    Bootstrap pooled estimate = 1465\n##    Bootstrap weighted pooled estimate = 1472\n##    Bootstrap pooled CI = [172, 2758]\n##    38.1% of bootstrap samples have confidence intervals that do not span zero.\n##       38.1% positive.\n##       0% negative.\n## rpart Results:\n##    Complete estimate = 1332\n##    Complete CI = [-295, 2959]\n##    Bootstrap pooled estimate = 1429\n##    Bootstrap weighted pooled estimate = 1442\n##    Bootstrap pooled CI = [-136, 2993]\n##    32% of bootstrap samples have confidence intervals that do not span zero.\n##       32% positive.\n##       0% negative.\n## Matching Results:\n##    Complete estimate = 861\n##    Complete CI = [234, 1487]\n##    Bootstrap pooled estimate = 1429\n##    Bootstrap weighted pooled estimate = 1396\n##    Bootstrap pooled CI = [-339, 3196]\n##    86% of bootstrap samples have confidence intervals that do not span zero.\n##       85% positive.\n##       1% negative.\n## Matching-1-to-3 Results:\n##    Complete estimate = 1536\n##    Complete CI = [1089, 1983]\n##    Bootstrap pooled estimate = 1450\n##    Bootstrap weighted pooled estimate = 1400\n##    Bootstrap pooled CI = [-135, 3034]\n##    90% of bootstrap samples have confidence intervals that do not span zero.\n##       90% positive.\n##       0% negative.\n## MatchIt Results:\n##    Complete estimate = 2004\n##    Complete CI = [701, 3307]\n##    Bootstrap pooled estimate = 1653\n##    Bootstrap weighted pooled estimate = 1634\n##    Bootstrap pooled CI = [180, 3125]\n##    68% of bootstrap samples have confidence intervals that do not span zero.\n##       68% positive.\n##       0% negative.\nplot(boot_out)\nboxplot(boot_out)\nmatrixplot(boot_out)\nboot_balance <- balance(boot_out)\nboot_balance## Unadjusted balance: 0.122354110755457## 0.04\n\n## 0.08\n\n## 0.05\n\n## 0.06\n\n## 0.01\n\n## 0.05\n\n## 0.04\n\n##  NA\n\n## 0.07\n\n## 0.07\n\n## 0.05\n\n## 0.08\nplot(boot_balance)\nboxplot(boot_balance) + geom_hline(yintercept=.1, color='red')\nboot_balance$unadjusted## 0.11\n\n## 0.14\n\n## 0.04\n\n## 0.17\n\n## 0.09\n\n## 0.31\n\n## 0.00\n\n## 0.08\n\n## 0.09\n\n## 0.18\nboot_balance$complete## 0.04\n\n## 0.00\n\n## 0.00\n\n## 0.09\n\n## 0.01\n\n## 0.02\n\n## 0.03\n\n## 0.16\n\n## 0.05\n\n## 0.04\n\n## 0.00\n\n## 0.05\n\n## 0.09\n\n## 0.15\n\n## 0.14\n\n## 0.02\n\n## 0.05\n\n## 0.01\n\n## 0.04\n\n## 0.05\n\n## 0.08\n\n## 0.04\n\n## 0.01\n\n## 0.02\n\n## 0.02\n\n## 0.08\n\n## 0.10\n\n## 0.06\n\n## 0.00\n\n## 0.04\n\n## 0.03\n\n## 0.08\n\n## 0.02\n\n## 0.01\n\n## 0.00\n\n## 0.19\n\n## 0.00\n\n## 0.09\n\n## 0.05\n\n## 0.10\n\n## 0.00\n\n## 0.05\n\n## 0.03\n\n## 0.09\n\n## 0.01\n\n## 0.07\n\n## 0.01\n\n## 0.05\n\n## 0.06\n\n## 0.06\n\n## 0.06\n\n## 0.09\n\n## 0.02\n\n## 0.04\n\n## 0.03\n\n## 0.03\n\n## 0.01\n\n## 0.04\n\n## 0.02\n\n## 0.07\nboot_balance$pooled |> head()## 0.02\n\n## 0.03\n\n## 0.04\n\n## 0.03\n\n## 0.04\n\n## 0.05\n\n## 0.07\n\n## 0.09\n\n## 0.12\n\n##  NA\n\n## 0.06\n\n## 0.12\n\n## 0.05\n\n## 0.09\n\n## 0.08\n\n## 0.11\n\n## 0.08\n\n## 0.07\n\n## 0.06\n\n## 0.07\n\n## 0.03\n\n## 0.07\n\n## 0.05\n\n## 0.06\n\n## 0.04\n\n## 0.03\n\n## 0.03\n\n## 0.08\n\n## 0.03\n\n## 0.05\n\n## 0.08\n\n## 0.09\n\n## 0.07\n\n## 0.05\n\n## 0.10\n\n## 0.10"},{"path":"chapter-non-binary.html","id":"chapter-non-binary","chapter":"8 Non-Binary Treatments","heading":"8 Non-Binary Treatments","text":"","code":"\nrequire(TriMatch)\n\ndata(tutoring)\nstr(tutoring)## 'data.frame':    1142 obs. of  17 variables:\n##  $ treat     : Factor w/ 3 levels \"Control\",\"Treat1\",..: 1 1 1 1 1 2 1 1 1 1 ...\n##  $ Course    : chr  \"ENG*201\" \"ENG*201\" \"ENG*201\" \"ENG*201\" ...\n##  $ Grade     : int  4 4 4 4 4 3 4 3 0 4 ...\n##  $ Gender    : Factor w/ 2 levels \"FEMALE\",\"MALE\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Ethnicity : Factor w/ 3 levels \"Black\",\"Other\",..: 2 3 3 3 3 3 3 3 1 3 ...\n##  $ Military  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ ESL       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ EdMother  : int  3 5 1 3 2 3 4 4 3 6 ...\n##  $ EdFather  : int  6 6 1 5 2 3 4 4 2 6 ...\n##  $ Age       : num  48 49 53 52 47 53 54 54 59 40 ...\n##  $ Employment: int  3 3 1 3 1 3 3 3 1 3 ...\n##  $ Income    : num  9 9 5 5 5 9 6 6 1 8 ...\n##  $ Transfer  : num  24 25 39 48 23 ...\n##  $ GPA       : num  3 2.72 2.71 4 3.5 3.55 3.57 3.57 3.43 2.81 ...\n##  $ GradeCode : chr  \"A\" \"A\" \"A\" \"A\" ...\n##  $ Level     : Factor w/ 2 levels \"Lower\",\"Upper\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ ID        : int  377 882 292 215 252 265 1016 282 39 911 ...\ntable(tutoring$treat)## \n## Control  Treat1  Treat2 \n##     918     134      90\n# Histogram of unadjusted grades\ntmp <- as.data.frame(prop.table(table(tutoring$treat, tutoring$Grade), 1))\nggplot(tmp, aes(x=Var2, y=Freq, fill=Var1)) + \n    geom_bar(position='dodge', stat='identity') +\n    scale_y_continuous(labels = percent_format()) +\n    xlab('Grade') + ylab('Percent') + scale_colour_hue('Treatment')\n## Phase I\n\n# Note that the dependent variable is not included in the formula. The TriMatch\n# functions will replace the dependent variable depending on which pair is\n# being modeled.\ntutoring.formu <- ~ Gender + Ethnicity + Military + ESL + EdMother + EdFather + \n    Age + Employment + Income + Transfer + GPA\n\n# trips will estimate the propensity scores for each pairing of groups\ntutoring.tpsa <- trips(tutoring, tutoring$treat, tutoring.formu)\n\nplot(tutoring.tpsa, sample=c(200))\n# trimatch finds matched triplets.\ntutoring.matched <- trimatch(tutoring.tpsa)\n\n# Partial exact matching\ntutoring.matched2 <- trimatch(tutoring.tpsa, exact=tutoring$Level)\n\n# Plotting the results of trimatch is a subset of the triangle plot with only\n# points that were matched. There is also an additional parameter, rows, that\n# will overlay matched triplets.\nplot(tutoring.matched, rows=1, line.alpha=1, draw.segments=TRUE)\n## Examine the unmatched students\nunmatched <- unmatched(tutoring.matched)\nsummary(unmatched)## 819 (71.7%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 795 (86.6%)  17 (12.7%)   7 (7.78%)\nplot(unmatched)\n## Check balance\nmultibalance.plot(tutoring.tpsa)\nbalance.plot(tutoring.matched, tutoring$Age, label='Age')## \n##  Friedman rank sum test\n## \n## data:  Covariate and Treatment and ID\n## Friedman chi-squared = 4.1498, df = 2, p-value = 0.1256\n## \n##  Repeated measures ANOVA\n## \n##      Effect DFn DFd        F         p p<.05         ges\n## 2 Treatment   2 294 1.707234 0.1831598       0.006613137\nbalance.plot(tutoring.matched, tutoring$Military, label='Military')## \n##  Friedman rank sum test\n## \n## data:  Covariate and Treatment and ID\n## Friedman chi-squared = 0.4, df = 2, p-value = 0.8187\n# Create a grid of figures.\nbplots <- balance.plot(tutoring.matched, tutoring[,all.vars(tutoring.formu)], \n                       legend.position='none', \n                       x.axis.labels=c('C','T1','T1'), x.axis.angle=0)\nbplots[['Military']] # We can plot one at at time.\nsummary(bplots) # Create a data frame with the statistical results##     Covariate   Friedman Friedman.p Friedman.sig    rmANOVA rmANOVA.p\n## 1      Gender 2.16666667 0.33846543                      NA        NA\n## 2   Ethnicity 0.05678233 0.97200807                      NA        NA\n## 3    Military 0.40000000 0.81873075                      NA        NA\n## 4         ESL 4.78571429 0.09136826            .         NA        NA\n## 5    EdMother 1.55974843 0.45846368              0.76509335 0.4662146\n## 6    EdFather 0.02794411 0.98612510              0.06102055 0.9408158\n## 7         Age 4.14982578 0.12556736              1.70723419 0.1831598\n## 8  Employment 2.04048583 0.36050736              1.27194067 0.2818249\n## 9      Income 0.59582543 0.74236614              0.39251642 0.6757086\n## 10   Transfer 3.08717949 0.21361291              0.55080160 0.5770812\n## 11        GPA 1.37542662 0.50272433              0.49589373 0.6095348\n##    rmANOVA.sig\n## 1         <NA>\n## 2         <NA>\n## 3         <NA>\n## 4         <NA>\n## 5             \n## 6             \n## 7             \n## 8             \n## 9             \n## 10            \n## 11\nplot(bplots, cols=3, byrow=FALSE)\n## Phase II\n# The summary function performs a number of statistical tests including Friedman\n# rank sum test, repeated measures ANOVA, and if one or both of those tests have\n# p values less than 0.5 (the default, but configurable), then a pairwise Wilcox\n# test and three paired t-tests will also be performed.\n(sout <- summary(tutoring.matched, tutoring$Grade))## $PercentMatched\n##   Control    Treat1    Treat2 \n## 0.1339869 0.8731343 0.9222222 \n## \n## $friedman.test\n## \n##  Friedman rank sum test\n## \n## data:  Outcome and Treatment and ID\n## Friedman chi-squared = 17.404, df = 2, p-value = 0.0001663\n## \n## \n## $rmanova\n## $rmanova$ANOVA\n##      Effect DFn DFd        F            p p<.05        ges\n## 2 Treatment   2 294 16.66293 1.396209e-07     * 0.06818487\n## \n## $rmanova$`Mauchly's Test for Sphericity`\n##      Effect         W            p p<.05\n## 2 Treatment 0.8668353 2.946934e-05     *\n## \n## $rmanova$`Sphericity Corrections`\n##      Effect       GGe        p[GG] p[GG]<.05       HFe        p[HF] p[HF]<.05\n## 2 Treatment 0.8824842 6.035469e-07         * 0.8923995 5.333417e-07         *\n## \n## \n## $pairwise.wilcox.test\n## \n##  Pairwise comparisons using Wilcoxon signed rank test with continuity correction \n## \n## data:  out$Outcome and out$Treatment \n## \n##             Treat1.out Treat2.out\n## Treat2.out  0.0046     -         \n## Control.out 0.0165     1.9e-06   \n## \n## P value adjustment method: bonferroni \n## \n## $t.tests\n##               Treatments         t  df      p.value sig  mean.diff     ci.min\n## 1  Treat1.out-Treat2.out -3.095689 147 2.351743e-03  ** -0.3378378 -0.5535076\n## 2 Treat1.out-Control.out  2.939953 147 3.813865e-03  **  0.4459459  0.1461816\n## 3 Treat2.out-Control.out  5.443253 147 2.140672e-07 ***  0.7837838  0.4992224\n##       ci.max\n## 1 -0.1221681\n## 2  0.7457103\n## 3  1.0683452\n## \n## attr(,\"class\")\n## [1] \"trimatch.summary\" \"list\"\nls(sout)## [1] \"friedman.test\"        \"pairwise.wilcox.test\" \"PercentMatched\"      \n## [4] \"rmanova\"              \"t.tests\"\n# TODO: boxdiff.plot(tutoring.matched, tutoring$Grade, ordering=c('Treatment2','Treatment1','Control'))\nparallel.plot(tutoring.matched, tutoring$Grade)\n# The Loess plot is imperfect with three sets of propensity scores. There is a\n# model parameter to specify which model to use. Once we a model is selected\n# we have propensity scores for two of the three groups. We impute a propensity\n# score on that model's scale for the third group as the midpoint between\n# the other two propensity scores that unit was matched to.\nloess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method='loess')\n# Turn on 95% confidence interval (see also the level parameter)\nloess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method='loess')\n# We can also pass other parameters to the loess function.\nloess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method='loess', span=1)\n# This is a busy plot, but since all the lines are practically vertical, the\n# distance between each pair of propensity scores is minimal.\nloess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method='loess', \n            plot.connections=TRUE)\n# The merge function will add the outcome to the matched triplet data frame.\n# This is useful for other approaches to analyzing the matched triplets.\ntmatch.out <- merge(tutoring.matched, tutoring$Grade)\nhead(tmatch.out)##   Treat1 Treat2 Control        D.m3        D.m2         D.m1     Dtotal\n## 1    368     39     331 0.007053754 0.001788577 0.0103932229 0.01923555\n## 2    800   1088    1105 0.018477707 0.000736057 0.0001821526 0.01939592\n## 3    286    655     853 0.016859948 0.004237243 0.0019476652 0.02304486\n## 4    158    279     365 0.003373585 0.009530680 0.0107118774 0.02361614\n## 5    899    209     100 0.001929173 0.013633300 0.0091835718 0.02474604\n## 6   1034    791     484 0.010538949 0.008541671 0.0092350273 0.02831565\n##   Treat1.out Treat2.out Control.out\n## 1          4          4           0\n## 2          4          4           3\n## 3          2          4           4\n## 4          4          4           4\n## 5          4          3           4\n## 6          4          4           4"},{"path":"chapter-multilevelpsa.html","id":"chapter-multilevelpsa","chapter":"9 Multilevel PSA","heading":"9 Multilevel PSA","text":"\nFigure 9.1: Annotated multilevel PSA assessment plot. plot compares private schools (x- axis) public schools (y-axis) North America Programme International Student Assessment.\nPhase IUse conditional inference trees party package\nFigure 9.2: Tree heat map showing relative importance covariates used tree\nPhase II\nFigure 9.3: Multilevel PSA assessment plot\n\nFigure 9.4: Multilevel PSA difference plot\n","code":"\nlibrary(multilevelPSA)\nlibrary(grid)\n\ndata(pisana)\ndata(pisa.colnames)\ndata(pisa.psa.cols)\nstr(pisana)## 'data.frame':    66548 obs. of  65 variables:\n##  $ Country : chr  \"Canada\" \"Canada\" \"Canada\" \"Canada\" ...\n##  $ CNT     : chr  \"CAN\" \"CAN\" \"CAN\" \"CAN\" ...\n##  $ SCHOOLID: Factor w/ 1534 levels \"00001\",\"00002\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST01Q01 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ...\n##  $ ST04Q01 : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 2 2 1 1 2 ...\n##  $ ST05Q01 : Factor w/ 3 levels \"No\",\"Yes, more than one year\",..: 2 2 3 2 1 1 3 3 2 3 ...\n##  $ ST06Q01 : num  4 4 4 4 5 5 5 4 4 5 ...\n##  $ ST07Q01 : Factor w/ 3 levels \"No, never\",\"Yes, once\",..: 1 1 1 1 1 1 2 1 1 1 ...\n##  $ ST08Q01 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST08Q02 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 2 1 2 2 2 ...\n##  $ ST08Q03 : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 1 1 1 2 2 2 ...\n##  $ ST08Q04 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 2 2 2 1 2 1 ...\n##  $ ST08Q05 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST08Q06 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST10Q01 : Factor w/ 5 levels \"<ISCED level 1>\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ ST12Q01 : Factor w/ 4 levels \"Looking for work\",..: 3 3 3 3 3 3 3 3 4 3 ...\n##  $ ST14Q01 : Factor w/ 5 levels \"<ISCED level 1>\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ ST16Q01 : Factor w/ 4 levels \"Looking for work\",..: 3 3 3 3 3 3 3 3 4 3 ...\n##  $ ST19Q01 : Factor w/ 2 levels \"Another language\",..: 2 2 2 1 2 2 2 2 2 1 ...\n##  $ ST20Q01 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q02 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q03 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q04 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q05 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 2 1 ...\n##  $ ST20Q06 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q07 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 1 1 1 1 1 ...\n##  $ ST20Q08 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 2 1 1 1 1 ...\n##  $ ST20Q09 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 1 1 2 ...\n##  $ ST20Q10 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 1 ...\n##  $ ST20Q12 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST20Q13 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 2 2 2 2 2 2 ...\n##  $ ST20Q14 : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ ST21Q01 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 3 3 3 3 4 4 3 3 3 ...\n##  $ ST21Q02 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 3 3 3 4 3 2 3 3 3 ...\n##  $ ST21Q03 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 3 2 2 3 4 4 4 4 3 3 ...\n##  $ ST21Q04 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 4 4 2 3 4 3 2 4 4 3 ...\n##  $ ST21Q05 : Factor w/ 4 levels \"None\",\"One\",\"Three or more\",..: 2 4 4 3 3 4 2 4 4 3 ...\n##  $ ST22Q01 : Factor w/ 6 levels \"0-10 books\",\"101-200 books\",..: 5 5 1 5 4 3 2 5 4 1 ...\n##  $ ST23Q01 : Factor w/ 5 levels \"1 to 2 hours a day\",..: 5 2 4 2 4 4 3 4 3 4 ...\n##  $ ST31Q01 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST31Q02 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 2 ...\n##  $ ST31Q03 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ST31Q05 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n##  $ ST31Q06 : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n##  $ ST31Q07 : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 1 1 1 1 2 1 ...\n##  $ ST32Q01 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 4 4 5 4 4 4 4 4 4 4 ...\n##  $ ST32Q02 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 5 4 5 4 4 4 4 4 4 5 ...\n##  $ ST32Q03 : Factor w/ 5 levels \"2 up to 4 Hours a week\",..: 4 4 5 4 4 4 4 4 4 5 ...\n##  $ PV1MATH : num  474 673 348 518 420 ...\n##  $ PV2MATH : num  466 632 372 537 533 ...\n##  $ PV3MATH : num  438 571 397 511 441 ...\n##  $ PV4MATH : num  458 685 445 527 468 ...\n##  $ PV5MATH : num  471 586 375 490 420 ...\n##  $ PV1READ : num  503 613 390 570 407 ...\n##  $ PV2READ : num  492 578 421 542 452 ...\n##  $ PV3READ : num  522 553 442 527 434 ...\n##  $ PV4READ : num  509 594 410 584 423 ...\n##  $ PV5READ : num  460 564 372 518 416 ...\n##  $ PV1SCIE : num  460 686 378 500 417 ...\n##  $ PV2SCIE : num  444 589 399 575 556 ...\n##  $ PV3SCIE : num  484 593 388 505 471 ...\n##  $ PV4SCIE : num  489 643 444 540 436 ...\n##  $ PV5SCIE : num  435 646 385 476 479 ...\n##  $ PUBPRIV : Factor w/ 2 levels \"Private\",\"Public\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ STRATIO : num  14.4 14.4 14.4 14.4 14.4 ...\ntable(pisana$CNT, pisana$PUBPRIV, useNA='ifany')##      \n##       Private Public\n##   CAN    1645  21484\n##   MEX    4048  34138\n##   USA     345   4888\nprop.table(table(pisana$CNT, pisana$PUBPRIV, useNA='ifany'), 1) * 100##      \n##         Private    Public\n##   CAN  7.112283 92.887717\n##   MEX 10.600744 89.399256\n##   USA  6.592777 93.407223\nmlctree <- mlpsa.ctree(pisana[,c('CNT','PUBPRIV',pisa.psa.cols)], \n                       formula=PUBPRIV ~ ., level2='CNT')## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\npisana.party <- getStrata(mlctree, pisana, level2='CNT')\ntree.plot(mlctree, level2Col=pisana$CNT, \n          colLabels=pisa.colnames[,c('Variable','ShortDesc')])\n#NOTE: This is not entirely correct but is sufficient for visualization purposes\n#      See mitools package for combining multiple plausible values.\npisana.party$mathscore <- apply(pisana.party[,paste0('PV',1:5,'MATH')],1,sum)/5\npisana.party$readscore <- apply(pisana.party[,paste0('PV',1:5,'READ')],1,sum)/5\npisana.party$sciescore <- apply(pisana.party[,paste0('PV',1:5,'SCIE')],1,sum)/5\nresults.psa.math <- mlpsa(response = pisana.party$mathscore, \n                          treatment = pisana.party$PUBPRIV, \n                          strata = pisana.party$strata, \n                          level2 = pisana.party$CNT, minN=5)\n# summary(results.psa.math)\nresults.psa.math$level2.summary[,c('level2','n','Private','Private.n','Public',\n                                   'Public.n','diffwtd','ci.min','ci.max','df')]##   level2     n  Private Private.n   Public Public.n    diffwtd    ci.min\n## 1    CAN 22718 578.6262      1625 512.7997    21093 -65.826528 -72.08031\n## 2    MEX 38134 429.5247      4044 422.9746    34090  -6.550102 -10.04346\n## 3    USA  5233 505.2189       345 484.8212     4888 -20.397746 -32.03916\n##       ci.max    df\n## 1 -59.572751 22652\n## 2  -3.056743 38050\n## 3  -8.756334  5213\n# Confidence interval\nresults.psa.math$overall.ci## -31.30\n\n## -24.75\n# Effect Size\nresults.psa.math$overall.ci / sd(pisana.party$mathscore)## -0.35\n\n## -0.28\nplot(results.psa.math)\nmlpsa.difference.plot(results.psa.math)"},{"path":"appendix-shiny.html","id":"appendix-shiny","chapter":"A Shiny Application","heading":"A Shiny Application","text":"\nFigure .1: PSA Shiny Application\n","code":"\nlibrary(psa)\npsa_shiny()"},{"path":"appendix-psranges.html","id":"appendix-psranges","chapter":"B Propensity Score Ranges","heading":"B Propensity Score Ranges","text":"regard propensity score ranges, range tends shrink ratio treatment--control increases. Figure 20 depicts range distribution propensity scores (using logistic regression) varying treatment--control ratios. data used create figure simulated available Appendix K. psrange plot.psrange functions included multilevelPSA R package. Propensity scores estimated single covariate mean treatment control 0.6 0.4, respectively. standard deviation 0.4. 100 treatment units 1,000 control units simulated. goal choosing means standard deviations separation treatment control. row figure represents percentage control units sampled estimating propensity scores, starting 100% (.e. 1,000 control units) 10% (100 control units). figure shows, ratio decreases equal treatment control units, range propensity scores becomes normal. calculate ranges, sampling step bootstrapped green bar black points represent 20 bootstrap samples taken. bars represent mean minimum mean maximum step.“shrinking” propensity score ranges ratio treatment--control increases implications interpretation propensity scores. Typically, propensity scores interpreted probability treatment. studies number treatment control units roughly equal, interpretation valid. However, cases ratio treatment--control large, best simply interpret propensity scores adjustment scores probabilities. Since matching stratification procedures utilize standard scores (.e. propensity score divided standard deviation propensity scores), impact interpretation propensity scores impact estimated treatment e↵ects. appears issue explored either PSA logistic regression literature additional exploration topic appears warranted.1:10 (100 treatments, 1000 control units)1:20 (100 treatments, 2000 control units)100 treatments, 1000 control units, equal means standard deviations100 treatments, 1000 control units, little overlap100 treat, 1000 control, 10 covariates","code":"\nlibrary(multilevelPSA)\ngetSimulatedData <- function(nvars = 3, ntreat = 100, treat.mean = 0.6, treat.sd = 0.5, \n    ncontrol = 1000, control.mean = 0.4, control.sd = 0.5) {\n    if (length(treat.mean) == 1) {\n        treat.mean = rep(treat.mean, nvars)\n    }\n    if (length(treat.sd) == 1) {\n        treat.sd = rep(treat.sd, nvars)\n    }\n    if (length(control.mean) == 1) {\n        control.mean = rep(control.mean, nvars)\n    }\n    if (length(control.sd) == 1) {\n        control.sd = rep(control.sd, nvars)\n    }\n    \n    df <- c(rep(0, ncontrol), rep(1, ntreat))\n    for (i in 1:nvars) {\n        df <- cbind(df, c(rnorm(ncontrol, mean = control.mean[1], sd = control.sd[1]), \n            rnorm(ntreat, mean = treat.mean[1], sd = treat.sd[1])))\n    }\n    df <- as.data.frame(df)\n    names(df) <- c(\"treat\", letters[1:nvars])\n    return(df)\n}\ntest.df1 <- getSimulatedData(ntreat = 100, ncontrol = 1000)\npsranges1 <- psrange(test.df1, test.df1$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges1)\nsummary(psranges1)##       p ntreat ncontrol ratio   min.mean       min.sd min.median       min.se\n## 1    10    100      100     1 0.15158085 0.0399247330 0.16675914 0.0089274417\n## 21   20    100      200     2 0.08065862 0.0110939596 0.07987970 0.0024806848\n## 41   30    100      300     3 0.05825915 0.0089598303 0.06117074 0.0020034790\n## 61   40    100      400     4 0.03966169 0.0043536309 0.03993927 0.0009735015\n## 81   50    100      500     5 0.03028191 0.0038363450 0.02969747 0.0008578328\n## 101  60    100      600     6 0.02581532 0.0024130983 0.02594087 0.0005395852\n## 121  70    100      700     7 0.02203176 0.0019850490 0.02179919 0.0004438705\n## 141  80    100      800     8 0.01903728 0.0013822504 0.01864631 0.0003090806\n## 161  90    100      900     9 0.01631016 0.0005450121 0.01627551 0.0001218684\n## 181 100    100     1000    10 0.01453930 0.0000000000 0.01453930 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.06536710 0.19336279 0.8598281 0.04676196  0.8501328 0.010456291 0.7861258\n## 21  0.05791970 0.10233899 0.7270197 0.04065439  0.7303578 0.009090597 0.6439753\n## 41  0.03669016 0.07242601 0.6050233 0.05144961  0.5948197 0.011504482 0.5386225\n## 61  0.03200825 0.05010319 0.5606233 0.02683686  0.5606224 0.006000904 0.5089432\n## 81  0.02357618 0.03609465 0.5058454 0.02833751  0.4987880 0.006336460 0.4682137\n## 101 0.02033462 0.02954242 0.4593096 0.02170712  0.4544798 0.004853860 0.4236357\n## 121 0.01906877 0.02678785 0.4255822 0.02072414  0.4292869 0.004634059 0.3811220\n## 141 0.01719583 0.02208829 0.3928887 0.01439171  0.3941605 0.003218084 0.3591961\n## 161 0.01526377 0.01767972 0.3577524 0.01054775  0.3544386 0.002358548 0.3458710\n## 181 0.01453930 0.01453930 0.3306953 0.00000000  0.3306953 0.000000000 0.3306953\n##       max.max\n## 1   0.9361766\n## 21  0.8123514\n## 41  0.7304035\n## 61  0.6127437\n## 81  0.5618276\n## 101 0.5129718\n## 121 0.4548082\n## 141 0.4260502\n## 161 0.3798020\n## 181 0.3306953\ntest.df2 <- getSimulatedData(ncontrol = 2000)\npsranges2 <- psrange(test.df2, test.df2$treat, treat ~ ., samples = seq(100, \n    2000, by = 100), nboot = 20)\nplot(psranges2)\nsummary(psranges2)##       p ntreat ncontrol ratio    min.mean       min.sd  min.median       min.se\n## 1     5    100      100     1 0.108479153 3.478953e-02 0.113600581 7.779176e-03\n## 21   10    100      200     2 0.046371411 1.434092e-02 0.044963287 3.206727e-03\n## 41   15    100      300     3 0.031643058 8.312285e-03 0.030554662 1.858683e-03\n## 61   20    100      400     4 0.017812919 3.780810e-03 0.017660947 8.454147e-04\n## 81   25    100      500     5 0.014711230 2.496677e-03 0.015022455 5.582738e-04\n## 101  30    100      600     6 0.010719571 2.578118e-03 0.009863200 5.764847e-04\n## 121  35    100      700     7 0.009514228 1.588785e-03 0.009334077 3.552632e-04\n## 141  40    100      800     8 0.007691382 1.171925e-03 0.007477029 2.620504e-04\n## 161  45    100      900     9 0.006889727 1.222477e-03 0.006726188 2.733541e-04\n## 181  50    100     1000    10 0.006276889 5.128093e-04 0.006246931 1.146676e-04\n## 201  55    100     1100    11 0.005318209 5.985626e-04 0.005257637 1.338427e-04\n## 221  60    100     1200    12 0.004854049 3.641814e-04 0.004806155 8.143343e-05\n## 241  65    100     1300    13 0.004545931 3.242687e-04 0.004520731 7.250869e-05\n## 261  70    100     1400    14 0.004114851 2.213926e-04 0.004099411 4.950490e-05\n## 281  75    100     1500    15 0.003827730 3.163611e-04 0.003794601 7.074050e-05\n## 301  80    100     1600    16 0.003565950 1.937171e-04 0.003520204 4.331645e-05\n## 321  85    100     1700    17 0.003321481 1.553915e-04 0.003348171 3.474661e-05\n## 341  90    100     1800    18 0.003153977 7.759093e-05 0.003159270 1.734986e-05\n## 361  95    100     1900    19 0.002936686 5.616000e-05 0.002935480 1.255776e-05\n## 381 100    100     2000    20 0.002768276 0.000000e+00 0.002768276 0.000000e+00\n##         min.min     min.max  max.mean      max.sd max.median      max.se\n## 1   0.058393439 0.170711051 0.8819158 0.032569683  0.8876059 0.007282803\n## 21  0.028012247 0.080533570 0.8039199 0.038136538  0.8047723 0.008527589\n## 41  0.019685674 0.053076019 0.7255648 0.038281827  0.7324588 0.008560077\n## 61  0.012532975 0.025846915 0.6973759 0.039330036  0.6887384 0.008794463\n## 81  0.009046485 0.020133140 0.6381940 0.032865186  0.6328090 0.007348879\n## 101 0.008257497 0.019303295 0.6197714 0.029943565  0.6216856 0.006695585\n## 121 0.006840465 0.013738017 0.5788401 0.029811196  0.5822845 0.006665986\n## 141 0.005879615 0.011053336 0.5589409 0.043400845  0.5554842 0.009704724\n## 161 0.005364998 0.011462546 0.5217990 0.027818443  0.5244945 0.006220393\n## 181 0.005345154 0.007100721 0.4896128 0.022943577  0.4909629 0.005130340\n## 201 0.004317742 0.006380071 0.4837967 0.023518184  0.4848799 0.005258826\n## 221 0.004328371 0.005446223 0.4630265 0.022398047  0.4599904 0.005008355\n## 241 0.004046610 0.005226542 0.4414052 0.023000235  0.4404226 0.005143009\n## 261 0.003657161 0.004503838 0.4322617 0.021211389  0.4340543 0.004743011\n## 281 0.003223369 0.004460980 0.4089992 0.015626515  0.4098423 0.003494195\n## 301 0.003307291 0.003902424 0.3932721 0.015087435  0.3914948 0.003373653\n## 321 0.002975258 0.003588740 0.3851770 0.011768615  0.3860391 0.002631542\n## 341 0.003005001 0.003287277 0.3760762 0.009748364  0.3764867 0.002179801\n## 361 0.002838330 0.003063846 0.3659962 0.003577472  0.3653889 0.000799947\n## 381 0.002768276 0.002768276 0.3550118 0.000000000  0.3550118 0.000000000\n##       max.min   max.max\n## 1   0.8071012 0.9270047\n## 21  0.7323388 0.9063155\n## 41  0.6191583 0.7766993\n## 61  0.5878796 0.7694392\n## 81  0.5915755 0.6922287\n## 101 0.5586192 0.6855857\n## 121 0.5269503 0.6250264\n## 141 0.4901740 0.6524009\n## 161 0.4480063 0.5559810\n## 181 0.4391822 0.5360344\n## 201 0.4462260 0.5317380\n## 221 0.4234709 0.5015449\n## 241 0.3962534 0.4878237\n## 261 0.3966437 0.4709061\n## 281 0.3779369 0.4337704\n## 301 0.3746633 0.4216166\n## 321 0.3634942 0.4050216\n## 341 0.3531399 0.3906495\n## 361 0.3578767 0.3711270\n## 381 0.3550118 0.3550118\ntest.df3 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.5, control.mean = 0.5)\npsranges3 <- psrange(test.df3, test.df3$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges3)\nsummary(psranges3)##       p ntreat ncontrol ratio   min.mean      min.sd min.median       min.se\n## 1    10    100      100     1 0.28350601 0.064087988 0.28885238 0.0143305097\n## 21   20    100      200     2 0.16705507 0.031529985 0.16046048 0.0070503191\n## 41   30    100      300     3 0.12344740 0.019802371 0.12273928 0.0044279448\n## 61   40    100      400     4 0.09522531 0.011914719 0.09279009 0.0026642122\n## 81   50    100      500     5 0.07563025 0.008418365 0.07552338 0.0018824037\n## 101  60    100      600     6 0.06076604 0.008521491 0.06124068 0.0019054633\n## 121  70    100      700     7 0.05269255 0.005924987 0.05247310 0.0013248673\n## 141  80    100      800     8 0.04586865 0.002395134 0.04624844 0.0005355682\n## 161  90    100      900     9 0.04048884 0.002059943 0.04014488 0.0004606172\n## 181 100    100     1000    10 0.03634508 0.000000000 0.03634508 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.16192908 0.37939385 0.7132598 0.04361440  0.7129799 0.009752476 0.6394674\n## 21  0.10563283 0.22609654 0.5202766 0.04031786  0.5161472 0.009015347 0.4561985\n## 41  0.09125843 0.16864202 0.4143754 0.02875963  0.4181810 0.006430849 0.3529161\n## 61  0.07550229 0.12048280 0.3532632 0.02511123  0.3510016 0.005615041 0.3137257\n## 81  0.06256572 0.09642339 0.2941406 0.02138895  0.2973524 0.004782715 0.2463536\n## 101 0.05119536 0.08325665 0.2668781 0.01733506  0.2681488 0.003876236 0.2279083\n## 121 0.04216244 0.06796214 0.2363525 0.01411165  0.2348300 0.003155462 0.2141914\n## 141 0.04222852 0.05135097 0.2190724 0.01032624  0.2199282 0.002309018 0.2030472\n## 161 0.03701920 0.04386187 0.1978531 0.00701747  0.1986408 0.001569154 0.1863798\n## 181 0.03634508 0.03634508 0.1835126 0.00000000  0.1835126 0.000000000 0.1835126\n##       max.max\n## 1   0.7816167\n## 21  0.6045684\n## 41  0.4675561\n## 61  0.3965734\n## 81  0.3318606\n## 101 0.2927092\n## 121 0.2619002\n## 141 0.2443409\n## 161 0.2090405\n## 181 0.1835126\ntest.df4 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.25, treat.sd = 0.3, \n    control.mean = 0.75, control.sd = 0.3)\npsranges4 <- psrange(test.df4, test.df4$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges4)\nsummary(psranges4)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 1.525384e-05 2.837735e-05 2.299369e-06\n## 21   20    100      200     2 2.386395e-06 4.263708e-06 6.796030e-07\n## 41   30    100      300     3 7.854093e-07 1.369501e-06 3.322104e-07\n## 61   40    100      400     4 1.139238e-07 9.950192e-08 9.888239e-08\n## 81   50    100      500     5 4.970589e-08 7.085309e-08 1.910629e-08\n## 101  60    100      600     6 9.456041e-08 8.805110e-08 6.876480e-08\n## 121  70    100      700     7 4.562272e-08 3.277532e-08 4.271103e-08\n## 141  80    100      800     8 4.834941e-08 1.732541e-08 4.683829e-08\n## 161  90    100      900     9 3.734849e-08 1.141374e-08 3.473544e-08\n## 181 100    100     1000    10 3.302817e-08 0.000000e+00 3.302817e-08\n##           min.se      min.min      min.max  max.mean       max.sd max.median\n## 1   6.345368e-06 8.242210e-10 1.189959e-04 0.9999919 1.177109e-05  0.9999975\n## 21  9.533941e-07 4.512240e-09 1.459366e-05 0.9999698 4.357035e-05  0.9999873\n## 41  3.062298e-07 9.339904e-09 5.821350e-06 0.9999734 2.437621e-05  0.9999837\n## 61  2.224931e-08 5.006341e-09 3.452425e-07 0.9999718 2.790507e-05  0.9999803\n## 81  1.584323e-08 1.753035e-09 3.004791e-07 0.9999730 3.183032e-05  0.9999857\n## 101 1.968882e-08 1.862385e-08 4.026231e-07 0.9999570 1.988995e-05  0.9999546\n## 121 7.328785e-09 1.690112e-09 1.093181e-07 0.9999481 3.208612e-05  0.9999523\n## 141 3.874079e-09 1.740209e-08 9.088383e-08 0.9999326 1.783491e-05  0.9999357\n## 161 2.552189e-09 1.706567e-08 5.651540e-08 0.9999274 1.839133e-05  0.9999295\n## 181 0.000000e+00 3.302817e-08 3.302817e-08 0.9999211 0.000000e+00  0.9999211\n##           max.se   max.min   max.max\n## 1   2.632095e-06 0.9999539 1.0000000\n## 21  9.742627e-06 0.9998202 0.9999998\n## 41  5.450686e-06 0.9999165 0.9999973\n## 61  6.239764e-06 0.9998793 0.9999976\n## 81  7.117475e-06 0.9998587 0.9999988\n## 101 4.447529e-06 0.9999103 0.9999906\n## 121 7.174675e-06 0.9998882 0.9999962\n## 141 3.988007e-06 0.9998969 0.9999700\n## 161 4.112427e-06 0.9998986 0.9999602\n## 181 0.000000e+00 0.9999211 0.9999211\ntest.df5 <- getSimulatedData(nvars = 10, ntreat = 100, ncontrol = 1000)\npsranges5 <- psrange(test.df5, test.df5$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges5)\nsummary(psranges5)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 0.0101680007 5.571726e-03 0.0087507018\n## 21   20    100      200     2 0.0032786858 1.986855e-03 0.0031712876\n## 41   30    100      300     3 0.0016998723 8.777415e-04 0.0014178615\n## 61   40    100      400     4 0.0012710429 7.476010e-04 0.0010189156\n## 81   50    100      500     5 0.0009891488 3.613101e-04 0.0009233698\n## 101  60    100      600     6 0.0006954162 1.699134e-04 0.0006825701\n## 121  70    100      700     7 0.0005748031 1.098981e-04 0.0005927955\n## 141  80    100      800     8 0.0005241338 7.766948e-05 0.0005085729\n## 161  90    100      900     9 0.0004511794 4.957160e-05 0.0004584537\n## 181 100    100     1000    10 0.0004033341 0.000000e+00 0.0004033341\n##           min.se      min.min      min.max  max.mean      max.sd max.median\n## 1   1.245876e-03 0.0034055594 0.0218076912 0.9797889 0.007606976  0.9810769\n## 21  4.442742e-04 0.0010920758 0.0092490802 0.9667801 0.012485108  0.9700905\n## 41  1.962690e-04 0.0007561575 0.0045260935 0.9539731 0.013264618  0.9562029\n## 61  1.671687e-04 0.0006177711 0.0040260207 0.9394156 0.010526497  0.9424796\n## 81  8.079140e-05 0.0005352006 0.0019389389 0.9221930 0.014294291  0.9217427\n## 101 3.799379e-05 0.0002784764 0.0009768850 0.9113210 0.017183853  0.9104470\n## 121 2.457395e-05 0.0004073415 0.0007375938 0.9032366 0.010021744  0.9045313\n## 141 1.736742e-05 0.0003929492 0.0006895618 0.8848322 0.011660804  0.8843501\n## 161 1.108455e-05 0.0003240626 0.0005327837 0.8735074 0.008826874  0.8708373\n## 181 0.000000e+00 0.0004033341 0.0004033341 0.8639081 0.000000000  0.8639081\n##          max.se   max.min   max.max\n## 1   0.001700972 0.9612616 0.9912510\n## 21  0.002791755 0.9357162 0.9860199\n## 41  0.002966059 0.9106519 0.9720211\n## 61  0.002353796 0.9129030 0.9566536\n## 81  0.003196301 0.8929029 0.9531369\n## 101 0.003842426 0.8850449 0.9491578\n## 121 0.002240930 0.8856435 0.9235878\n## 141 0.002607435 0.8583591 0.9084171\n## 161 0.001973749 0.8641812 0.8983841\n## 181 0.000000000 0.8639081 0.8639081"},{"path":"appendix-psmodels.html","id":"appendix-psmodels","chapter":"C Methods for Estimating Propensity Scores","heading":"C Methods for Estimating Propensity Scores","text":"appendix provide R code multiple statistical models estimating propensity scores. examples use lalonde dataset following formula:","code":"\nlalonde.formu <- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2) +\n    u74 + u75"},{"path":"appendix-psmodels.html","id":"logistic-regression","chapter":"C Methods for Estimating Propensity Scores","heading":"C.1 Logistic Regression","text":"","code":"\nlr_out <- glm(lalonde.formu,\n              data = lalonde,\n              family = binomial(link = logit))\nlr_ps <- fitted(lr_out)"},{"path":"appendix-psmodels.html","id":"conditional-inference-trees-with-party-package","chapter":"C Methods for Estimating Propensity Scores","heading":"C.2 Conditional Inference Trees with party package","text":"","code":"\nlibrary(party)\nctree_out <- ctree(lalonde.formu,\n                   data = lalonde)\n# treeresponse(ctree_out)"},{"path":"appendix-psmodels.html","id":"recusrive-partitioning-with-rpart","chapter":"C Methods for Estimating Propensity Scores","heading":"C.3 Recusrive Partitioning with rpart","text":"","code":"\nlibrary(rpart)\nrpart_out <- rpart(lalonde.formu,\n                   data = lalonde,\n                   method = 'class')\n# For classification\nrpart_strata <- rpart_out$where\n# For matching or weighting\nrpart_ps <- predict(rpart_out, type = 'prob')[,1]"},{"path":"appendix-psmodels.html","id":"bayesian-logistic-regression","chapter":"C Methods for Estimating Propensity Scores","heading":"C.4 Bayesian Logistic Regression","text":"","code":"\nlibrary(rstanarm)\nstan_out <- stan_glm(lalonde.formu,\n                     data = lalonde)\nstan_ps <- predict(stan_out, type = 'response')"},{"path":"appendix-psmodels.html","id":"probit-bart-for-dichotomous-outcomes-with-normal-latents","chapter":"C Methods for Estimating Propensity Scores","heading":"C.5 Probit BART for dichotomous outcomes with Normal latents","text":"","code":"\nlibrary(BART)\nbart_out <- pbart(x.train = lalonde[,all.vars(lalonde.formu)[-1]],\n                  y.train = lalonde[,all.vars(lalonde.formu)[1]])\nbart_ps <- bart_out$prob.test.mean"},{"path":"appendix-psmodels.html","id":"random-forests","chapter":"C Methods for Estimating Propensity Scores","heading":"C.6 Random Forests","text":"","code":"\nlibrary(randomForest)\nrf_out <- randomForest(update.formula(lalonde.formu, factor(treat) ~ .),\n                       data = lalonde)\n# For classification\nrf_strata <- predict(rf_out, type = 'response')\n# For matching or weighting\nrf_ps <- predict(rf_out, type = 'prob')[,1,drop=TRUE]"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
