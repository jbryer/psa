[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Last updated: April 04, 2025I first introduced propensity score analysis (PSA) late dissertation advisor Robert Pruzek 2006 entered graduate school. notion get reasonable causal estimates without need randomization foreign first, skeptical. Many years later used PSA many projects, convinced possible, believe instances may preferred randomized control trial. Principal Investigator two Federal grants develop test Diagnostic Assessment Achievement College Skills (DAACS) attempted conduct large scale randomized control trials (RCT) involving thousands students. found experiences conducting large scale RCTs numerous compromises made delivering intervention compromise generalizability results. Moreover, RCTs assume single, homogenous, causal effect everyone. reality rarely true. interventions equally effective everyone. PSA, particularly stratification section, possible tease intervention may vary observed covariates.taught PSA many times years. “book” attempt collect notes experiences conducting PSA. part emphasize applied provide links references reader wishes explore theoretical details. Additionally, book make extensive use visualizations explain concepts well use presenting results. psa R package accompanies book available Github can installed using remotes package command . setting dependencies = 'Enhances' parameter ensure R packages used book installed well. psa package contains number datasets utility functions used throughout book. also contains Shiny application designed conduct PSA using graphical user interface. Details using application provided appendix.","code":"\nremotes::install_github('jbryer/psa',\n                        build_vignettes = TRUE,\n                        dependencies = 'Enhances')"},{"path":"index.html","id":"status","chapter":"Preface","heading":"0.1 Status","text":"","code":""},{"path":"index.html","id":"contributing","chapter":"Preface","heading":"Contributing","text":"books work progress contributions welcome. Please adhere code conduct. page edit link take directly source file Github. can also submit feedback using Github Issues tracker.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"website created using bookdown hosted Github pages.","code":""},{"path":"index.html","id":"colophon","chapter":"Preface","heading":"Colophon","text":"","code":"\ndevtools::session_info()## ─ Session info ───────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.4.3 (2025-02-28)\n##  os       Ubuntu 24.04.2 LTS\n##  system   x86_64, linux-gnu\n##  ui       X11\n##  language (EN)\n##  collate  C.UTF-8\n##  ctype    C.UTF-8\n##  tz       UTC\n##  date     2025-04-04\n##  pandoc   3.1.11 @ /opt/hostedtoolcache/pandoc/3.1.11/x64/ (via rmarkdown)\n##  quarto   NA\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────\n##  package       * version    date (UTC) lib source\n##  abind           1.4-8      2024-09-12 [1] RSPM (R 4.4.0)\n##  backports       1.5.0      2024-05-23 [1] RSPM (R 4.4.0)\n##  bookdown        0.42       2025-01-07 [1] RSPM (R 4.4.0)\n##  boot            1.3-31     2024-08-28 [2] CRAN (R 4.4.3)\n##  bslib           0.9.0      2025-01-30 [1] RSPM (R 4.4.0)\n##  cachem          1.1.0      2024-05-16 [1] RSPM (R 4.4.0)\n##  car           * 3.1-3      2024-09-27 [1] RSPM (R 4.4.0)\n##  carData       * 3.0-5      2022-01-06 [1] RSPM (R 4.4.0)\n##  cli             3.6.4      2025-02-13 [1] RSPM (R 4.4.0)\n##  codetools       0.2-20     2024-03-31 [2] CRAN (R 4.4.3)\n##  coin            1.4-3      2023-09-27 [1] RSPM (R 4.4.0)\n##  colorspace      2.1-1      2024-07-26 [1] RSPM (R 4.4.0)\n##  devtools        2.4.5      2022-10-11 [1] RSPM (R 4.4.0)\n##  digest          0.6.37     2024-08-19 [1] RSPM (R 4.4.0)\n##  downlit         0.4.4      2024-06-10 [1] RSPM (R 4.4.0)\n##  dplyr         * 1.1.4      2023-11-17 [1] RSPM (R 4.4.0)\n##  ellipsis        0.3.2      2021-04-29 [1] RSPM (R 4.4.0)\n##  evaluate        1.0.3      2025-01-10 [1] RSPM (R 4.4.0)\n##  ez            * 4.4-0      2016-11-02 [1] RSPM (R 4.4.0)\n##  fastmap         1.2.0      2024-05-15 [1] RSPM (R 4.4.0)\n##  Formula         1.2-5      2023-02-24 [1] RSPM (R 4.4.0)\n##  fs              1.6.5      2024-10-30 [1] RSPM (R 4.4.0)\n##  generics        0.1.3      2022-07-05 [1] RSPM (R 4.4.0)\n##  ggplot2       * 3.5.1      2024-04-23 [1] RSPM (R 4.4.0)\n##  ggthemes        5.1.0      2024-02-10 [1] RSPM (R 4.4.0)\n##  glue            1.8.0      2024-09-30 [1] RSPM (R 4.4.0)\n##  granova       * 2.2        2023-03-22 [1] RSPM (R 4.4.0)\n##  granovaGG     * 1.4.1.9000 2025-04-04 [1] Github (briandk/granovaGG@7014d74)\n##  gridExtra       2.3        2017-09-09 [1] RSPM (R 4.4.0)\n##  gtable          0.3.6      2024-10-25 [1] RSPM (R 4.4.0)\n##  htmltools       0.5.8.1    2024-04-04 [1] RSPM (R 4.4.0)\n##  htmlwidgets     1.6.4      2023-12-06 [1] RSPM (R 4.4.0)\n##  httpuv          1.6.15     2024-03-26 [1] RSPM (R 4.4.0)\n##  jquerylib       0.1.4      2021-04-26 [1] RSPM (R 4.4.0)\n##  jsonlite        2.0.0      2025-03-27 [1] RSPM (R 4.4.0)\n##  knitr         * 1.50       2025-03-16 [1] RSPM (R 4.4.0)\n##  later           1.4.1      2024-11-27 [1] RSPM (R 4.4.0)\n##  lattice         0.22-6     2024-03-20 [2] CRAN (R 4.4.3)\n##  libcoin         1.0-10     2023-09-27 [1] RSPM (R 4.4.0)\n##  lifecycle       1.0.4      2023-11-07 [1] RSPM (R 4.4.0)\n##  lme4            1.1-37     2025-03-26 [1] RSPM (R 4.4.0)\n##  magrittr        2.0.3      2022-03-30 [1] RSPM (R 4.4.0)\n##  MASS          * 7.3-64     2025-01-04 [2] CRAN (R 4.4.3)\n##  Matching      * 4.10-15    2024-10-14 [1] RSPM (R 4.4.0)\n##  MatchIt       * 4.7.1      2025-03-10 [1] RSPM (R 4.4.0)\n##  Matrix          1.7-2      2025-01-23 [2] CRAN (R 4.4.3)\n##  matrixStats     1.5.0      2025-01-07 [1] RSPM (R 4.4.0)\n##  memoise         2.0.1      2021-11-26 [1] RSPM (R 4.4.0)\n##  mgcv            1.9-1      2023-12-21 [2] CRAN (R 4.4.3)\n##  mime            0.13       2025-03-17 [1] RSPM (R 4.4.0)\n##  miniUI          0.1.1.1    2018-05-18 [1] RSPM (R 4.4.0)\n##  minqa           1.2.8      2024-08-17 [1] RSPM (R 4.4.0)\n##  mnormt          2.1.1      2022-09-26 [1] RSPM (R 4.4.0)\n##  modeltools      0.2-23     2020-03-05 [1] RSPM (R 4.4.0)\n##  multcomp        1.4-28     2025-01-29 [1] RSPM (R 4.4.0)\n##  multilevelPSA * 1.2.5      2018-03-22 [1] RSPM (R 4.4.0)\n##  munsell         0.5.1      2024-04-01 [1] RSPM (R 4.4.0)\n##  mvtnorm         1.3-3      2025-01-10 [1] RSPM (R 4.4.0)\n##  nlme            3.1-167    2025-01-27 [2] CRAN (R 4.4.3)\n##  nloptr          2.2.1      2025-03-17 [1] RSPM (R 4.4.0)\n##  party           1.3-18     2025-01-29 [1] RSPM (R 4.4.0)\n##  pillar          1.10.1     2025-01-07 [1] RSPM (R 4.4.0)\n##  pkgbuild        1.4.7      2025-03-24 [1] RSPM (R 4.4.0)\n##  pkgconfig       2.0.3      2019-09-22 [1] RSPM (R 4.4.0)\n##  pkgload         1.4.0      2024-06-28 [1] RSPM (R 4.4.0)\n##  plyr            1.8.9      2023-10-02 [1] RSPM (R 4.4.0)\n##  profvis         0.4.0      2024-09-20 [1] RSPM (R 4.4.0)\n##  promises        1.3.2      2024-11-28 [1] RSPM (R 4.4.0)\n##  PSAboot       * 1.3.8      2025-04-04 [1] Github (jbryer/PSAboot@f5d73cd)\n##  PSAgraphics   * 2.1.3      2024-03-05 [1] RSPM (R 4.4.0)\n##  psych           2.5.3      2025-03-21 [1] RSPM (R 4.4.0)\n##  purrr           1.0.4      2025-02-05 [1] RSPM (R 4.4.0)\n##  R6              2.6.1      2025-02-15 [1] RSPM (R 4.4.0)\n##  randomForest    4.7-1.2    2024-09-22 [1] RSPM (R 4.4.0)\n##  rbibutils       2.3        2024-10-04 [1] RSPM (R 4.4.0)\n##  RColorBrewer    1.1-3      2022-04-03 [1] RSPM (R 4.4.0)\n##  Rcpp            1.0.14     2025-01-12 [1] RSPM (R 4.4.0)\n##  Rdpack          2.6.3      2025-03-16 [1] RSPM (R 4.4.0)\n##  reformulas      0.4.0      2024-11-03 [1] RSPM (R 4.4.0)\n##  remotes         2.5.0      2024-03-17 [1] RSPM (R 4.4.0)\n##  reshape         0.8.9      2022-04-12 [1] RSPM (R 4.4.0)\n##  reshape2      * 1.4.4      2020-04-09 [1] RSPM (R 4.4.0)\n##  rlang           1.1.5      2025-01-17 [1] RSPM (R 4.4.0)\n##  rmarkdown       2.29       2024-11-04 [1] RSPM (R 4.4.0)\n##  rpart         * 4.1.24     2025-01-07 [2] CRAN (R 4.4.3)\n##  sandwich        3.1-1      2024-09-15 [1] RSPM (R 4.4.0)\n##  sass            0.4.9      2024-03-15 [1] RSPM (R 4.4.0)\n##  scales        * 1.3.0      2023-11-28 [1] RSPM (R 4.4.0)\n##  sessioninfo     1.2.3      2025-02-05 [1] RSPM (R 4.4.0)\n##  shiny           1.10.0     2024-12-14 [1] RSPM (R 4.4.0)\n##  stringi         1.8.7      2025-03-27 [1] RSPM (R 4.4.0)\n##  stringr         1.5.1      2023-11-14 [1] RSPM (R 4.4.0)\n##  strucchange     1.5-4      2024-09-02 [1] RSPM (R 4.4.0)\n##  survival        3.8-3      2024-12-17 [2] CRAN (R 4.4.3)\n##  TH.data         1.1-3      2025-01-17 [1] RSPM (R 4.4.0)\n##  tibble          3.2.1      2023-03-20 [1] RSPM (R 4.4.0)\n##  tidyselect      1.2.1      2024-03-11 [1] RSPM (R 4.4.0)\n##  TriMatch      * 1.0.0      2025-04-03 [1] RSPM (R 4.4.0)\n##  urlchecker      1.0.1      2021-11-30 [1] RSPM (R 4.4.0)\n##  usethis         3.1.0      2024-11-26 [1] RSPM (R 4.4.0)\n##  vctrs           0.6.5      2023-12-01 [1] RSPM (R 4.4.0)\n##  withr           3.0.2      2024-10-28 [1] RSPM (R 4.4.0)\n##  xfun            0.52       2025-04-02 [1] RSPM (R 4.4.0)\n##  xml2            1.3.8      2025-03-14 [1] RSPM (R 4.4.0)\n##  xtable        * 1.8-4      2019-04-21 [1] RSPM (R 4.4.0)\n##  yaml            2.3.10     2024-07-26 [1] RSPM (R 4.4.0)\n##  zoo             1.8-13     2025-02-22 [1] RSPM (R 4.4.0)\n## \n##  [1] /home/runner/work/_temp/Library\n##  [2] /opt/R/4.4.3/lib/R/library\n##  * ── Packages attached to the search path.\n## \n## ──────────────────────────────────────────────────────────────────────────────"},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"work Jason Bryer licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"chapter-introduction.html","id":"chapter-introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"use propensity score methods (Rosenbaum Rubin 1983) estimating causal effects observational studies certain kinds quasi-experiments increasing last couple decades (see Figure 1.1), especially social sciences (Thoemmes Kim 2011) medical research (Austin 2008). Propensity score analysis (PSA) attempts adjust selection bias occurs due lack randomization. Analysis typically conducted three phases phase , probability placement treatment estimated identify matched pairs clusters phase II, comparisons dependent variable can made matched pairs within clusters. Lastly, phase III involves testing robustness estimates unobserved confounders. R (R Core Team 2025) ideal conducting PSA given wide availability current statistical methods vis-à-vis add-packages well superior graphics capabilities.book provide theoretical overview propensity score methods well illustrations discussion implementing PSA methods R. Chapter 1 provides overview three phases PSA minimal R code. Chapters 2, 3, 4 discuss details implementing three major approaches PSA. Chapter 7 provides strategies conducting PSA missing data. Chapters 5 6 provide details phase III PSA using sensitivity analysis bootstrapping, respectively. Lastly, chapter 8 provides methods implementing PSA non-binary treatments chapter 12 discusses methods PSA cluster, Hierarchical, data. appendices contain additional details regarding PSA Shiny application (Appendix ), limitations interpreting fitted values logistic regression (Appendix B), additional methods packages estimating propensity scores (Appendix C).\nFigure 1.1: PSA Citations per year\n","code":""},{"path":"chapter-introduction.html","id":"counterfactual-model-for-causality","chapter":"1 Introduction","heading":"1.1 Counterfactual Model for Causality","text":"order understand propensity score analysis allows us make causal estimates observational data, must first understand basic principals causality, particularly counterfactual model. Figure 1.2 depicts conterfactual model. begin research subject. can student, patient, mouse, asteroid, object wish know whether condition effect . Consider two parallel universes: one subject receives condition another receive condition B. Typically one condition treatment whereas condition absence treatment (also referred control). use treatment control throughout book refer two conditions. individual exposed two conditions, outcome measured. difference outcomes true causal effect. However, unless Dr. Strange living Marvell multiverse, impossible object exist two universes time, therefore can never actually observe true causal effect. Holland (1986) referred Fundamental Problem Causal Inference.\nFigure 1.2: Theoretical Causal Model\n","code":""},{"path":"chapter-introduction.html","id":"randomized-control-trials-the-gold-standard","chapter":"1 Introduction","heading":"1.2 Randomized Control Trials: “The Gold Standard”","text":"randomized control trials (RCT) gold standard estimating causal effects. Effects can estimated using simple means groups, blocks randomized block design. Randomization presumes unbiasedness balance groups. However, randomization often feasible many reasons, especially educational contexts. Although RCT gold standard, important recognize estimates causal effect. look example RCT can wrong average provides good estimates true causal effect can build model closely mimic RCT non-randomized data.Intelligence Quotient (IQ) common measure intelligence. designed mean 100 standard deviation 15. Consider developed intervention known increase anyone’s IQ 4.5 points (standardized effect size 0.3). Figure 1.3 represents scenario 30 individuals. left panel individual’s outcome assigned control condition (blue) treatment condition (red). distance red blue points individual 4.5, stipulated counterfactual difference. RCTs ever get observe one outcome individual. right pane represents one possible set outcomes RCT. , randomly selected one outcome individual left pane.\nFigure 1.3: Example conterfactuals (left panel) one possible randomized control trial.\nFigure 1.4 includes mean differences treatment control vertical lines blue red, respectively. left observe true counterfactuals difference treatment (red) control (blue) vertical lines 4.5. However, right difference treatment control -5.3!\nFigure 1.4: Estimated differences full counterfactual model one RCT.\nexample RCT estimate true effect, estimated wrong direction. However, Figure 1.5 represents distribution effects conducting 1,000 RCTs 30 individuals . point RCT already compromise estimating true counterfactual (.e. causal effect). consider gold standard many trials nearly approximate true counterfactual.\nFigure 1.5: Distribution differences across many RCTs\nRCT works probability anyone treatment 50%. Statistically, call strong ignorability assumption. strong ignorability assumption states outcome independent observed unobserved covariates1 randomization. represented mathematically :\\[\\begin{equation}\n\\begin{aligned}\n\\left( { Y }_{ }\\left( 1 \\right) ,{ Y }_{ }\\left( 0 \\right)  \\right) \\bot { T }_{ }\n\\end{aligned}\n\\tag{1.1}\n\\end{equation}\\]\\({X}_{}\\) , \\(Y\\) outcome interest individual response \\(Y_i(1)\\) outcome subject assigned treatment group \\(Y_i(0)\\) outcome subject assigned control group. \\(\\bot\\) means independent \\(T_i\\) assignment indicator subject . Therefore, follows causal effect treatment difference individual’s outcome situation given treatment (referred counterfactual).\\[\\begin{equation}\n\\begin{aligned}\n{\\delta}_{} = { Y }_{ i1 }-{ Y }_{ i0 }\n\\end{aligned}\n\\tag{1.2}\n\\end{equation}\\]However, impossible directly observe \\({}_{}\\) (referred Fundamental Problem Causal Inference, Holland 1986). Rubin framed problem missing data problem details discussed next section.","code":""},{"path":"chapter-introduction.html","id":"rubins-causal-model","chapter":"1 Introduction","heading":"1.2.1 Rubin’s Causal Model","text":"Returning Figure 1.2, problem getting true causal effect observe outcome outcome B, never . result, missing data estimate causal effect. Neyman (1923) first coined term potential outcomes referring randomized trials. However, Donald Rubin extended Neyman’s idea include observational experimental data. Rubin’s student Holland (1986) later coined Rubin Causal Model.Rubin (1974) discussed example effect aspirin headache:“Intuitively, causal effect one treatment, E, another, C, particular unit interval time \\(t_{1}\\)\n\\(t_{2}\\) difference happened time \\(t_{2}\\) unit exposed E initiated \\(t_{1}\\) happened \\(t_{2}\\) unit exposed C initiated \\(t_{1}\\): ‘hour ago taken two aspirins instead just glass water, headache now gone,’ ‘hour ago took two aspirins instead just glass water, headache now gone.’ definition causal effect E versus C treatment reflect intuitive meaning.”Rubin Causal Model, whether headache cause whether took aspirin one hour ago, can observe one. key estimating causal effect understanding mechanism selecting whether take aspirin. Imagine get chronic headaches need decide many times whether take aspirin. Let’s also stimulate aspirin likely effective take morning afternoon. decide flip coin decide whether take aspirin balance observed headaches morning afternoon. , even though difference morning afternoon, influence observed outcomes. However, decide take aspirin 50 degrees outside. Since likely warmer afternoon morning, comparing outcomes provide bias estimate, part deciding whether take aspirin longer 50%. observed weather can potentially determine probability taking aspirin . enough observations, compare situations probability taking aspirin low, observations without aspirin way across spectrum high probability fo taking aspirin.","code":""},{"path":"chapter-introduction.html","id":"propensity-scores","chapter":"1 Introduction","heading":"1.2.2 Propensity Scores","text":"Propensity scores first introduced Rosenbaum Rubin (1983). defined propensity scores “conditional probability assignment particular treatment given vector observed covariates.” Rosenbaum Rubin showed seminal 1983 paper, Central Role Propensity Score Observational Studies Causal Effects “scalar propensity score sufficient remove bias due observed covariates.” Propensity scores can used variety ways including matching, stratification, weighting.Mathematically can define probability treatment group :\\[\\begin{equation}\n\\begin{aligned}\n\\pi(X_i) = Pr(T_i = 1 \\; | \\; X_i)\n\\end{aligned}\n\\tag{1.3}\n\\end{equation}\\]\\(X\\) matrix observed covariates \\(\\pi(X_i)\\) propensity score. balancing property exogeneity states ,\\[\\begin{equation}\n\\begin{aligned}\n\nT_i \\; \\mathrel{\\unicode{x2AEB}} \\; X_i \\; | \\; \\pi (X_i)\n\\end{aligned}\n\\tag{1.4}\n\\end{equation}\\]Ti treatment indicator subject . case randomized experiments, strong ignorability assumption states,\\[\\begin{equation}\n\\begin{aligned}\n\nY_i(1), \\; Y_i(0)) \\; \\mathrel{\\unicode{x2AEB}} \\; T_i \\; | \\; X_i\n\\end{aligned}\n\\tag{1.5}\n\\end{equation}\\]\\(X_i\\). , treatment independent covariates, observed otherwise. However, strong ignorability assumption can restated propensity score ,\\[\\begin{equation}\n\\begin{aligned}\n\n({ Y }_{ }(1),{ Y }_{ }(0)) \\; \\mathrel{\\unicode{x2AEB}} \\; { T }_{ } \\; | \\; \\pi({ X }_{ })\n\\end{aligned}\n\\tag{1.6}\n\\end{equation}\\]treatment placement ignorable given propensity score presuming sufficient balance2 achieved.average treatment effect (ATE) defined \\(E(r_1) - E(r_0)\\) \\(E(.)\\) expected value population. Given set covariates, \\(X\\), outcomes \\(Y\\), 0 denotes control group 1 denotes treatment group, ATE defined :\\[\\begin{equation}\n\\begin{aligned}\nATE \\; = \\; E(Y_1 - Y_0 \\; | \\; X) \\; = \\; E(Y_1 \\; | \\; X) - E(Y_0 \\; | \\; X)\n\\end{aligned}\n\\tag{1.7}\n\\end{equation}\\]difference treatment control groups given set observed covariates. section 1.3.2 discuss ATE addition causal estimators detail.Simply put, Rosenbaum Rubin (1983) proved observations similar propensity scores roughly equivalent (balanced) across observed covariates. see rest chapter, scalar summarizes many variables convenient finding matches, stratifying, applying regression weights. Although verify balance achieved methods estimating propensity scores better others.","code":""},{"path":"chapter-introduction.html","id":"phases-of-propensity-score-analysis","chapter":"1 Introduction","heading":"1.3 Phases of Propensity Score Analysis","text":"Propensity score analysis typically conducted three phases (see 1.6), namely:Model selection bias\n. Estimate propensity scores\nB. Check balance\nC. Repeat B sufficient balance optimizedModel selection biasA. Estimate propensity scores\nB. Check balance\nC. Repeat B sufficient balance optimizedEstimate causal effects.Estimate causal effects.Check sensitivity unobserved confounders.Check sensitivity unobserved confounders.\nFigure 1.6: Process conducting propensity score analysis\nfollowing sections provide overview phases details implementing phase using one three main methods conducting PSA, stratification (chapter 2), matching (chapter 3), weighting (chapter 4).","code":""},{"path":"chapter-introduction.html","id":"phase-i-estimate-propensity-scores","chapter":"1 Introduction","heading":"1.3.1 Phase I: Estimate Propensity Scores","text":"Phase one propensity score analysis cyclical process propensity scores estimated using statistical model, balance observed covariates checked, modifications model modified sufficient balance achieved. simplicity use logistic regression estimate propensity scores throughout book. However, introduce classification trees chapter 2 given uniquely applicable stratification methods appendix C outlines additional statistical methods, R code, estimating propensity scores.Propensity scores conditional probability treatment given set observed covaraites. practice use statistical models dependent variable dichotomous (treatment control). often logistic regression used, advances predictive models ever increasing number model choices including classification trees, Bayesian models, ensemble random forests, many . demonstrate main features propensity score analysis use simulated dataset three pre-treatment covariates, x1 x2 continuous x3 categorical, treatment indicator, outcome variable treatment effect 1.5. Figure 1.7 scatter plot simulated data.3\nFigure 1.7: Scatterplot simulated datatset\nFigure 1.8 pairs plot (Schloerke et al. 2024) showing relationship covariates (.e. x1 x2) outcome grouped treatment. statistically significant correlation covariates outcome suggesting selection bias bias causal estimate.\nFigure 1.8: Pairs plot showing relationships covariates, treatment, outcome\ngoal adjust selection bias using propensity scores. example used logistic regression estimate propensity scores. Figure 1.9 histogram showing distribution propensity scores treatment group green control group orange . Note distributions skewed; treatment group negatively skewed control group positively skewed. hopefully make intuitive sense. probability treatment increases, see number treatment observations increase number control observations decrease.\nFigure 1.9: Distribution propensity scores\n","code":""},{"path":"chapter-introduction.html","id":"intro-balance","chapter":"1 Introduction","heading":"1.3.1.1 Evaluate Balance","text":"propensity scores estimated important verify balance observed covariates achieved. number ways . matching methods treatment control units paired, dependent sample tests can used (e.g. t-tests continuous variables \\(\\chi^2\\) tests categorical variables). However, significance testing alone generally problematic. Given number covariates, hence number null hypothesis tests conducted, likelihood committing type type II errors high. Moreover, many observational studies wish use PSA large sample sizes , else equal, shrink standard error estimate often resulting small p-values. Instead utilizing standardized effect sizes graphical representations provide better evidence whether balance achieved. PSAgraphics package (J. Helmreich Pruzek 2024) provides number functions assist evaluating balance. Figure 1.10 multiple covariate balance plot summarizes covariates together. x-axis absolute standardized effect size y-axis covariate. red line effect propensity score adjustment blue effect propensity score adjustment. Unfortunately literature doesn’t provide good guidance adjusted effect size threshold indicates sufficient balance achieved. Cohen (1988) frequently cited indicated effect size 0.2 0.3 small. general, recommend trying achieve adjusted effect sizes less 0.1.\nFigure 1.10: Multiple covariate balance assessment plot\nplot left Figure 1.11 balance assessment plot continuous variable. exact procedures stratification discussed chapter 2, short, divide propensity scores five strata using quintiles stratum number observations. yellow bars control group orange bars treatment group. looking center spread roughly equivalent within stratum. example can see stratum 5 higher values stratum 1. plot right plot categorical data using bar plot.plot right balance assessment plot qualitative variable. , stacked bars treatment control strata show distribution categories. Like continuous counterpart, looking similar distributions within stratum.\nFigure 1.11: Continuous (left) categorical (right) covariate balance assessment plots\nsee many choices estimating propensity scores remainder book. practice find phase PSA occupy time. robustness causal estimates rely achieving good balance observed covariates.propensity score method use?Whichever one gives best balance!","code":""},{"path":"chapter-introduction.html","id":"introduction-effects","chapter":"1 Introduction","heading":"1.3.2 Phase II: Estimate Causal Effects","text":"Now sufficient balance achieved observed covariates, time estimate causal effect. section provide overview three used approaches conducting propensity score analysis: stratification, matching, weighting. covered details chapters 2, 3, 4, respectively.using one three approaches conducting PSA, often helpful plot propensity scores outcome. Figure 1.12 scatter plot propensity scores (x-axis) outcome (y-axis), grouped/colored treatment, along Loess regression line (Cleveland 1979). number features observe . First, see propensity score increases outcome increases. direct representation selection bias. Second, Loess regression lines approximate 95% confidence intervals (grey) overlap across entire range propensity scores. Additionally distance two Loess regression lines roughly equal. indication treatment effect homogeneous (.e. units). see later chapters often case. become important feature PSA detecting heterogeneous, uneven, treatments based upon different “profiles.”\nFigure 1.12: Scatter plot propensity scores outcome Loess regression lines\n","code":""},{"path":"chapter-introduction.html","id":"stratification","chapter":"1 Introduction","heading":"1.3.2.1 Stratification","text":"Stratification involves dividing observations strata (subclasses) based upon propensity scores treated comparison units similar within strata. Cochran (1968) observed creating five subclassifications (stratum) removes least 90% bias estimated treatment effect. larger sample sizes may appropriate use 10 strata, however typically provide much additional benefit. Figure 1.13 provides density distribution propensity scores treatment control observations. example, strata defined using quintiles stratum number observations. vertical lines separate strata. can see stratum many control observations treatment observations. Conversely, stratum E many treatment observations control observations. see section 1.3.2.4 implications treatment effects calculated. However, important, verified section 1.3.1.1, observations within stratum similar across observed covariates.\nFigure 1.13: Density distribution propensity scores treatment\nFigure 1.14 plots propensity score outcome. horizontal lines correspond mean group within stratum. calculate overall effect size, independent sample tests (e.g. t-tests) conducted within stratum pooled provide overall estimate.\nFigure 1.14: Scatter plot propensity scores versus outcome\nFigure 1.15 provides alternative way depicting results (J. Helmreich Pruzek 2024). plots average treatment (x-axis) versus control (y-axis) strata. means projected line perpendicular unit line (.e. line \\(y = x\\)) tick marks represent distribution differences. green bar corresponds 95% confidence interval. size circles proportional sample size within stratum. example can different using methods estimation propensity scores classification trees (discussed chapter 2 appendix C). Since \\(y = x\\) points fall line indicate difference zero (.e. \\(y - x = 0\\)). extension, confidence interval represented green line spans unit line one fail reject null hypothesis. example however, statistically significant treatment effect.\nFigure 1.15: Propensity score assessment plot five strata\n","code":""},{"path":"chapter-introduction.html","id":"matching","chapter":"1 Introduction","heading":"1.3.2.2 Matching","text":"matching methods wish pair treatment observations control observations. discussed chapter 3 numerous algorithms finding matches. example simple one--one match found using nearest neighbor based upon propensity score. Additionally, caliper 0.1 used meaning observation matched distance another observation 0.1 standard deviations away. lines figure correspond observations matched. Since observations matched, dependent sample tests (e.g. t-tests) used estimate treatment effects.\nFigure 1.16: Scatterplot propensity score versus outcome matched pairs connected\nSimilar Figure 1.15 stratification, Figure 1.17 dependent sample assessment plot (Danielak Pruzek 2023) point represents matched pair. treatment observations plotted x-axis control observations y-axis. points line perpendicular unit line represent distribution difference scores. confidence interval purple clearly span unit line indicating statistically significant treatment effect.\nFigure 1.17: Dependent sample assessment plot\n","code":""},{"path":"chapter-introduction.html","id":"weighting","chapter":"1 Introduction","heading":"1.3.2.3 Weighting","text":"Propensity score weighting useful wish use propensity scores within regression models. Specifically, observation weighted inverse probability group. Figure 1.18 plots propensity scores outcome, however size point proportional propensity score weight. example weights calculated estimate average treatment effect. Details different treatment effects discussed section 1.3.2.4. Loess regression line (blue) approximate 95% confidence interval (grey) provided along line \\(y - 0\\). Since Loess regression lines overlap zero, conclude statistically significant treatment effect across entire range propensity scores. later examples book find treatment effects homogeneous meaning treatment effect across entire range propensity scores. plot, along Loess regression plot (Figure 1.12) effective tools determining whether treatment effects may differ depending different covariate profiles.\nFigure 1.18: Scatter plot propensity scores versus outcome point sizes corresponding propensity score weights\n","code":""},{"path":"chapter-introduction.html","id":"intro-treatment-effects","chapter":"1 Introduction","heading":"1.3.2.4 Treatment Effects","text":"randomized control trials typically conduct null hypothesis test differences means treatment control groups (defined equation (1.7) ). PSA often done, important recognize observations counted equal causal estimation. moreover, average treatment effect causal estimate measure can calculate. section defines four different causal estimates. presented context propensity score weighting (see 4) conceptually apply stratification matching.","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-ate","chapter":"1 Introduction","heading":"1.3.2.4.1 Average Treatment Effect (ATE)","text":"average treatment effect (ATE) understood estimate given direct analog RCTs. estimate ATE RCT using approach simply assuming everyone propensity score 0.5 since 50% treatment. , assume every treatment unit interchangeable control unit. PSA though, unit different propensity score. goal compare units similar propensity scores. saw Figure 1.9 distributions treatment control . Figure 1.19 depicts ATE works practice, particular different units weighted less towards ATE estimate move across propensity score range. darker color represents propensity score distribution estimated , light bars represent distribution used ATE calculation. treatment units lower propensity scores (fewer ) weighted ATE calculation. move right across propensity score range control units large propensity scores weighted range.\\[\\begin{equation}\n\\begin{aligned}\nATE = E(Y_1 - Y_0 | X) = E(Y_1|X) - E(Y_0|X)\n\\end{aligned}\n\\tag{1.8}\n\\end{equation}\\]\nFigure 1.19: Histogram average treatement effect\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-treated-att","chapter":"1 Introduction","heading":"1.3.2.4.2 Average Treatment Effect Among the Treated (ATT)","text":"average treatment effect among treated (ATT) uses treated units primary focus. Figure 1.20 see entire treatment group used weighting . However, control group weight (grey bars) values lower end propensity score range match distribution treatment group. Conversely, control group observations weighted right side propensity score range, , closely match distribution treatment group. context matching wish pair treatment control units, goal use treatment observations, therefore possible use control observations smaller propensity scores whereas control observations larger propensity scores may reused order find match every treatment observation.Mathematically, ATT defined equation (1.9). important difference ATE calculating expected value given \\(X = 1\\), indicates placement treatment.\\[\\begin{equation}\n\\begin{aligned}\nATT = E(Y_1 - Y_0 | X = 1) = E(Y_1 | X = 1) - E(Y_0 | X = 1)\n\\end{aligned}\n\\tag{1.9}\n\\end{equation}\\]\nFigure 1.20: Histogram average treatement among treated\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-control-atc","chapter":"1 Introduction","heading":"1.3.2.4.3 Average Treatment Effect Among the Control (ATC)","text":"average treatment effect among control (ATC) exactly opposite ATT. , wish use every control observation means treatment observations larger propensity scores used (case matching) weighted (case weighting stratification) represented grey. Conversely, treatment observations smaller propensity scores may match multiple control observations (case matching) weighted (case weighting stratification).Mathematically, ATC defined equation (1.10). important difference ATE calculating expected value given \\(X = 1\\), indicates placement control\\[\\begin{equation}\n\\begin{aligned}\nATC = E(Y_1 - Y_0 | X = 0) = E(Y_1 | X = 0) - E(Y_0 | X = 0)\n\\end{aligned}\n\\tag{1.10}\n\\end{equation}\\]\nFigure 1.21: Histogram average treatement among control\n","code":""},{"path":"chapter-introduction.html","id":"average-treatment-effect-among-the-evenly-matched-atm","chapter":"1 Introduction","heading":"1.3.2.4.4 Average Treatment Effect Among the Evenly Matched (ATM)","text":"average treatment effect among evenly matched (ATM) relatively new estimate developed specifically propensity score weighting closely related estimated conducting one--one matching. Unlike ATT ATC observations weighted equally, calculation ATM observations included estimation equal weight. depicted Figure 1.22 control observations small propensity scores used treatment observations large propensity scores used (represented grey bars). closely mimics occurs one--one matching. one--one matching observation can used can matched one observation group. Hence, tends work observations near mean propensity score range included. See Li Greene (2013), McGowan (2018), Samuels (2017) details.\\[\\begin{equation}\n\\begin{aligned}\nATM_d = E(Y_1 - Y_0 | M_d = 1)\n\\end{aligned}\n\\tag{1.11}\n\\end{equation}\\]\nFigure 1.22: Histogram average treatment effect among evenly matched\n","code":""},{"path":"chapter-introduction.html","id":"phase-iii-sensitivity-analysis","chapter":"1 Introduction","heading":"1.3.3 Phase III: Sensitivity Analysis","text":"final phase propensity score analysis evaluate robustness causal estimates. discuss two approaches test robustness: sensitivity analysis (covered detail chapter 5) bootstrapping (covered detail chapter 6). Sensitivity analysis procedure results tested increasing factors unmeasured confounder changing randomization process. , tests much another variable change prediction treatment result non rejecting null hypothesis.Sensitivity analysis well defined matching methods. Rosenbaum (2012) proposed testing null hypothesis , part, also test sensitivity chosen method. spirit testing null hypothesis , PSAboot R package (Bryer 2023) developed conducting bootstrapping propensity score analysis. framework addresses issues sensitivity method choice, also provides framework addressing issues imbalance treatment placement. Bootstrapping (Efron 1979) become effective approach estimating parameters. approach discussed chapter 6 avoids issues multiple hypothesis testing increased type error rates using bootstrap samples estimate standard errors confidence intervals.","code":""},{"path":"chapter-introduction.html","id":"r-packages","chapter":"1 Introduction","heading":"1.4 R Packages","text":"R statistical software language designed extended vis-à-vis packages. April 04, 2025, currently 22,256 packages available CRAN. Given ease R can extended, become tool choice conducting propensity score analysis. new R highly recommend R Data Science (Wickham Grolemund 2016) excellent introduction R. book make use number RMatchIt (Ho et al. 2025) Nonparametric Preprocessing Parametric Causal InferenceMatching (Singh Sekhon Saarinen 2024) Multivariate Propensity Score Matching Software Causal InferencemultilevelPSA (Bryer 2018) Multilevel Propensity Score Analysisparty (Hothorn et al. 2025) Laboratory Recursive PartytioningPSAboot (Bryer 2023) Bootstrapping Propensity Score AnalysisPSAgraphics (J. Helmreich Pruzek 2024) R Package Support Propensity Score Analysisrbounds (Keele 2022) Overview rebounds: R Package Rosenbaum bounds sensitivity analysis matched data.rpart (Therneau Atkinson 2025) Recursive PartitioningTriMatch (Bryer 2025) Propensity Score Matching Non-Binary TreatmentsThe psa R package specifically designed accompany book including utility functions assist conducting propensity score analysis. following command install psa R package along R packages use book.","code":"\nremotes::install_github('jbryer/psa', dependencies = 'Enhances')"},{"path":"chapter-introduction.html","id":"intro-datasets","chapter":"1 Introduction","heading":"1.5 Datasets","text":"section provides description datasets used throughout book.","code":""},{"path":"chapter-introduction.html","id":"lalonde","chapter":"1 Introduction","heading":"1.5.1 National Supported Work Demonstration (lalonde)","text":"lalonde dataset perhaps one used datasets introducing evaluating propensity score methods. data collected Lalonde (1986) became widely used PSA literature Dehejia Wahba (1999) used paper evaluate propensity score matching. dataset originated National Supported Work Demonstration study conducted 1970s. program provided 12 18 months employment people longstanding employment problems. dataset contains 445 observations 12 variables. primary outcome re78 real earnings 1978. Observed covariates used adjust selection bias include age (age years), edu (number years education), black (black ), hisp (Hispanic ), married (married ), nodegr (whether worker degree , note 1 = degree), re74 (real earnings 1974), re75 (real earnings 1975).age: Integer mean = 25 SD = 7.1educ: Integer mean = 10 SD = 1.8black: Integer mean = 0.83 SD = 0.37hisp: Integer mean = 0.088 SD = 0.28married: Integer mean = 0.17 SD = 0.37nodegr: Integer mean = 0.78 SD = 0.41re74: Numeric mean = 2,102 SD = 5,364re75: Numeric mean = 1,377 SD = 3,151re78: Numeric mean = 5,301 SD = 6,631u74: Integer mean = 0.73 SD = 0.44u75: Integer mean = 0.65 SD = 0.48treat: Integer mean = 0.42 SD = 0.49","code":"\ndata(lalonde, package='Matching')"},{"path":"chapter-introduction.html","id":"lindner","chapter":"1 Introduction","heading":"1.5.2 Lindner Center (lindner)","text":"Data observational study 996 patients receiving PCI Ohio Heart Health 1997 followed least 6 months staff Lindner Center. landmark dataset literature propensity score adjustment treatment selection bias due practice evidence based medicine; patients receiving abciximab tended severely diseased receive IIb/IIIa cascade blocker.lifepres: Numeric mean = 11 SD = 1.9cardbill: Integer mean = 15,674 SD = 11,182abcix: Integer mean = 0.7 SD = 0.46stent: Integer mean = 0.67 SD = 0.47height: Integer mean = 171 SD = 11female: Integer mean = 0.35 SD = 0.48diabetic: Integer mean = 0.22 SD = 0.42acutemi: Integer mean = 0.14 SD = 0.35ejecfrac: Integer mean = 51 SD = 10ves1proc: Integer mean = 1.4 SD = 0.66","code":"\ndata(lindner, package='PSAgraphics')"},{"path":"chapter-introduction.html","id":"tutoring","chapter":"1 Introduction","heading":"1.5.3 Tutoring (tutoring)","text":"tutoring dataset originates study conducted online adult serving institution examining effects tutoring services students English 101, English 201, History 310. Tutoring services available students Treatment (treat) operationalized students used tutoring services least course. 19.6% students used tutoring services approximately half using . use dataset dichotomous treatment (used tutoring ) two level treatment (used tutoring services , used tutoring services two times).treat: Factor 3 levels: Control; Treat1; Treat2Course: Character 3 unique valuesGrade: Integer mean = 2.9 SD = 1.5Gender: Factor 2 levels: FEMALE; MALEEthnicity: Factor 3 levels: ; White; BlackMilitary: Logical 31% TRUE 69% FALSEESL: Logical 8.1% TRUE 92% FALSEEdMother: Integer mean = 3.8 SD = 1.5EdFather: Integer mean = 3.7 SD = 1.7Age: Numeric mean = 37 SD = 9Employment: Integer mean = 2.7 SD = 0.68Income: Numeric mean = 5.1 SD = 2.3Transfer: Numeric mean = 52 SD = 25GPA: Numeric mean = 3.2 SD = 0.57GradeCode: Character 6 unique valuesLevel: Factor 2 levels: Lower; UpperID: Integer mean = 572 SD = 330treat2: Logical 20% TRUE 80% FALSE","code":"\ndata(tutoring, package='TriMatch')\ntutoring$treat2 <- tutoring$treat != 'Control'\ntable(tutoring$Course, tutoring$treat)##          \n##           Control Treat1 Treat2\n##   ENG*101     349     22     31\n##   ENG*201     518     36     32\n##   HSC*310      51     76     27"},{"path":"chapter-introduction.html","id":"pisa","chapter":"1 Introduction","heading":"1.5.4 Programme of International Student Assessment (pisana)","text":"Programme International Student Assessment (PISA) study conducted OECD every three years measure 15-year-olds’ academic abilities reading, mathematics, science along rich set demographic background information. pisana dataset included multilevelPSA package contains results 2009 study North America (.e. Canada, Mexico, United States).Country: Character 3 unique valuesCNT: Character 3 unique valuesSCHOOLID: Factor 1,534 levelsST01Q01: Factor 0 levels: NA (66,548 missing values)ST04Q01: Factor 2 levels: Female; MaleST05Q01: Factor 3 levels: Yes, one year; Yes, one year less; NoST06Q01: Numeric mean = 5.7 SD = 0.81ST07Q01: Factor 3 levels: , never; Yes, ; Yes, twice moreST08Q01: Factor 2 levels: Yes; NoST08Q02: Factor 2 levels: Yes; NoST08Q03: Factor 2 levels: ; YesST08Q04: Factor 2 levels: Yes; NoST08Q05: Factor 2 levels: ; YesST08Q06: Factor 2 levels: ; YesST10Q01: Factor 5 levels: <ISCED level 3A>; <ISCED level 2>; <ISCED level 3B, 3C>; complete <ISCED level 1>; <ISCED level 1>ST12Q01: Factor 4 levels: Working Full-time; Working Part-Time; ; Looking workST14Q01: Factor 5 levels: <ISCED level 3A>; <ISCED level 2>; <ISCED level 1>; Complete <ISCED level 1>; <ISCED level 3B, 3C>ST16Q01: Factor 4 levels: Working Full-time; Working Part-Time; Looking work; OtherST19Q01: Factor 2 levels: Language test; Another languageST20Q01: Factor 2 levels: Yes; NoST20Q02: Factor 2 levels: Yes; NoST20Q03: Factor 2 levels: Yes; NoST20Q04: Factor 2 levels: Yes; NoST20Q05: Factor 2 levels: Yes; NoST20Q06: Factor 2 levels: Yes; NoST20Q07: Factor 2 levels: Yes; NoST20Q08: Factor 2 levels: Yes; NoST20Q09: Factor 2 levels: Yes; NoST20Q10: Factor 2 levels: Yes; NoST20Q12: Factor 2 levels: Yes; NoST20Q13: Factor 2 levels: Yes; NoST20Q14: Factor 2 levels: Yes; NoST21Q01: Factor 4 levels: Three ; Two; One; NoneST21Q02: Factor 4 levels: Three ; Two; One; NoneST21Q03: Factor 4 levels: Three ; One; Two; NoneST21Q04: Factor 4 levels: Two; One; Three ; NoneST21Q05: Factor 4 levels: One; Two; Three ; NoneST22Q01: Factor 6 levels: 26-100 books; 0-10 books; 201-500 books; 11-25 books; 101-200 books; 500 booksST23Q01: Factor 5 levels: 2 hours day; 30 minutes less day; don’t read enjoyment; 30 60 minutes; 1 2 hours dayST31Q01: Factor 2 levels: ; YesST31Q02: Factor 2 levels: ; YesST31Q03: Factor 2 levels: ; YesST31Q05: Factor 2 levels: ; YesST31Q06: Factor 2 levels: ; YesST31Q07: Factor 2 levels: Yes; NoST32Q01: Factor 5 levels: attend; Less 2 hours week; 2 4 Hours week; 6 hours week; 4 6 hours per weekST32Q02: Factor 5 levels: Less 2 hours week; attend; 6 hours week; 2 4 Hours week; 4 6 hours per weekST32Q03: Factor 5 levels: attend; Less 2 hours week; 4 6 hours per week; 2 4 Hours week; 6 hours weekPV1MATH: Numeric mean = 461 SD = 92PV2MATH: Numeric mean = 461 SD = 92PV3MATH: Numeric mean = 461 SD = 92PV4MATH: Numeric mean = 461 SD = 92PV5MATH: Numeric mean = 461 SD = 92PV1READ: Numeric mean = 465 SD = 94PV2READ: Numeric mean = 465 SD = 94PV3READ: Numeric mean = 465 SD = 94PV4READ: Numeric mean = 465 SD = 94PV5READ: Numeric mean = 465 SD = 94PV1SCIE: Numeric mean = 460 SD = 94PV2SCIE: Numeric mean = 460 SD = 94PV3SCIE: Numeric mean = 460 SD = 94PV4SCIE: Numeric mean = 460 SD = 94PV5SCIE: Numeric mean = 460 SD = 94PUBPRIV: Factor 2 levels: Public; PrivateSTRATIO: Numeric mean = 26 SD = 31 (8,576 missing values)","code":"\ndata(pisana, package='multilevelPSA')"},{"path":"chapter-introduction.html","id":"nmes","chapter":"1 Introduction","heading":"1.5.5 National Medical Expenditure Study (nmes)","text":"National Medical Expenditure Study dataset used Imai Dyk (2004a) evaluating method non-binary treatments. study examined relationship smoking status medical expenditures.PIDX: Integer mean = 2.9e+07 SD = 5,107,973LASTAGE: Integer mean = 46 SD = 19MALE: Integer mean = 0.44 SD = 0.5RACE3: Factor 3 levels: 3; 1; 2eversmk: Integer mean = 0.52 SD = 0.5current: Integer mean = 0.55 SD = 0.5 (9,872 missing values)former: Integer mean = 0.23 SD = 0.42smoke: Factor 3 levels: 0; 1; 2AGESMOKE: Integer mean = 18 SD = 5.4 (10,382 missing values)CIGSSMOK: Integer mean = 18 SD = 12 (11,362 missing values)SMOKENOW: Integer mean = 1.4 SD = 0.5 (9,872 missing values)SMOKED: Integer mean = 1.5 SD = 0.5CIGSADAY: Integer mean = 19 SD = 12 (14,990 missing values)AGESTOP: Integer mean = 39 SD = 16 (16,242 missing values)packyears: Numeric mean = 12 SD = 21 (1,119 missing values)yearsince: Integer mean = 3 SD = 8.2 (416 missing values)INCALPER: Numeric mean = 7,171 SD = 3,560HSQACCWT: Numeric mean = 7,850 SD = 3,796TOTALEXP: Numeric mean = 1,947 SD = 6,207TOTALSP3: Numeric mean = 494 SD = 3,418lc5: Integer mean = 0.011 SD = 0.1chd5: Integer mean = 0.053 SD = 0.23beltuse: Factor 3 levels: 3; 2; 1educate: Factor 4 levels: 1; 2; 3; 4marital: Factor 5 levels: 2; 1; 5; 3; 4; NA (76 missing values)SREGION: Factor 4 levels: 1; 2; 3; 4POVSTALB: Factor 5 levels: 1; 3; 4; 5; 2; NA (85 missing values)flag: Integer mean = 0.15 SD = 0.53age: Integer mean = 0.56 SD = 0.5","code":"\ndata(nmes, package='TriMatch')"},{"path":"chapter-stratification.html","id":"chapter-stratification","chapter":"2 Stratification","heading":"2 Stratification","text":"stratifyverb: stratify; 3rd person present: stratifies; past tense: stratified; past participle: stratified; gerund present participle: stratifying\n1. arrange classify.\n2. form arrange strata.Propensity score stratification leverages propensity scores can define strata (groups) roughly equivalent observed covariates. Although reasonable start chapter 3 matching, stratification important method even prefer use matching method, stratification often used order evaluate balance.","code":""},{"path":"chapter-stratification.html","id":"phase-i-estimate-propensity-scores-logistic-regression","chapter":"2 Stratification","heading":"2.1 Phase I: Estimate Propensity Scores (Logistic regression)","text":"begin let’s estimate propensity scores using logistic regression National Supported Work Demonostration (lalonde) dataset (Lalonde 1986). , using final model specification used Dehejia Wahba (1999).Check distributions propensity scores ensure good overlap","code":"\ndata(lalonde, package = 'Matching')\nlalonde_formu <- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\nlr_out <- glm(formula = lalonde_formu,\n              data = lalonde,\n              family = binomial(link = 'logit'))\nsummary(lr_out)## \n## Call:\n## glm(formula = lalonde_formu, family = binomial(link = \"logit\"), \n##     data = lalonde)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)  4.008e+00  2.155e+00   1.860   0.0629 .\n## age          1.372e-02  8.929e-02   0.154   0.8779  \n## I(age^2)    -2.535e-04  1.472e-03  -0.172   0.8632  \n## educ        -8.612e-01  4.154e-01  -2.073   0.0382 *\n## I(educ^2)    4.482e-02  2.334e-02   1.920   0.0549 .\n## black       -2.933e-01  3.679e-01  -0.797   0.4253  \n## hisp        -9.472e-01  5.127e-01  -1.847   0.0647 .\n## married      1.730e-01  2.826e-01   0.612   0.5404  \n## nodegr      -4.280e-01  3.917e-01  -1.093   0.2745  \n## re74         4.815e-06  5.689e-05   0.085   0.9326  \n## I(re74^2)   -1.692e-09  1.999e-09  -0.846   0.3974  \n## re75         1.273e-04  8.310e-05   1.532   0.1256  \n## I(re75^2)   -4.623e-09  4.384e-09  -1.055   0.2916  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 581.07  on 432  degrees of freedom\n## AIC: 607.07\n## \n## Number of Fisher Scoring iterations: 4\nlalonde$lr_ps <- fitted(lr_out)\nggplot(lalonde, aes(x = lr_ps, color = as.logical(treat))) + \n    geom_density() +\n    scale_color_manual('Treatment', values = palette2) +\n    xlab('Propensity Score')"},{"path":"chapter-stratification.html","id":"stratifying","chapter":"2 Stratification","heading":"2.1.1 Stratifying","text":"Stratification using quintiles.\nFigure 2.1: Distribution propensity scores strata breaks\n\nFigure 2.2: Scatter plot propensity scores log real earnings 1978 treatment strata breaks\n","code":"\nbreaks5 <- psa::get_strata_breaks(lalonde$lr_ps)\nbreaks5## $breaks\n##         0%        20%        40%        60%        80%       100% \n## 0.08491513 0.34032233 0.35943734 0.40797660 0.51119006 0.83047510 \n## \n## $labels\n##     strata       xmin      xmax      xmid\n## 0%       A 0.08491513 0.3403223 0.2126187\n## 20%      B 0.34032233 0.3594373 0.3498798\n## 40%      C 0.35943734 0.4079766 0.3837070\n## 60%      D 0.40797660 0.5111901 0.4595833\n## 80%      E 0.51119006 0.8304751 0.6708326\nlalonde$lr_strata5 <- cut(x = lalonde$lr_ps, \n                          breaks = breaks5$breaks, \n                          include.lowest = TRUE, \n                          labels = breaks5$labels$strata)\ntable(lalonde$treat, lalonde$lr_strata5)##    \n##      A  B  C  D  E\n##   0 66 61 51 42 40\n##   1 23 28 38 47 49"},{"path":"chapter-stratification.html","id":"stratification-balance","chapter":"2 Stratification","heading":"2.1.2 Checking Balance","text":"\nFigure 2.3: Covariate balance plots categorical variables\n\nFigure 2.4: Covariate balance plots numeric variables\n","code":"\ncovars <- all.vars(lalonde.formu)\ncovars <- lalonde[,covars[-1]]\nPSAgraphics::cv.bal.psa(covariates = covars, \n                        treatment = lalonde$treat,\n                        propensity = lalonde$lr_ps,\n                        strata = lalonde$lr_strata)\nPSAgraphics::box.psa(continuous = lalonde$age, \n                     treatment = lalonde$treat, \n                     strata = lalonde$lr_strata,\n                     xlab = \"Strata\", \n                     balance = FALSE)\nPSAgraphics::cat.psa(categorical = lalonde$nodegr, \n                     treatment = lalonde$treat, \n                     strata = lalonde$lr_strata, \n                     xlab = 'Strata',\n                     balance = FALSE)"},{"path":"chapter-stratification.html","id":"phase-ii-estimate-effects","chapter":"2 Stratification","heading":"2.2 Phase II: Estimate Effects","text":"","code":"\nPSAgraphics::loess.psa(response = log(lalonde$re78 + 1),\n                       treatment = lalonde$treat,\n                       propensity = lalonde$lr_ps)## $ATE\n## [1] 0.9008386\n## \n## $se.wtd\n## [1] 0.3913399\n## \n## $CI95\n## [1] 0.1181588 1.6835185\n## \n## $summary.strata\n##    counts.0 counts.1  means.0  means.1 diff.means\n## 1        34       11 6.268705 6.474912  0.2062076\n## 2        32       12 5.491717 5.659280  0.1675631\n## 3        31       14 5.467712 5.703584  0.2358722\n## 4        30       14 5.425593 5.747613  0.3220194\n## 5        27       18 5.397146 5.831117  0.4339703\n## 6        24       20 5.302660 6.339721  1.0370619\n## 7        21       23 5.125331 6.607936  1.4826043\n## 8        21       24 5.036908 6.594808  1.5578999\n## 9        22       22 5.182703 6.981383  1.7986801\n## 10       18       27 6.047529 7.820786  1.7732573\npsa::loess_plot(ps = lalonde$lr_ps,\n                outcome = log(lalonde$re78 + 1),\n                treatment = lalonde$treat == 1,\n                responseTitle = 'log(re78)',\n                plot.strata = 5,\n                points.treat.alpha = 0.5,\n                points.control.alpha = 0.5,\n                percentPoints.treat = 1,\n                percentPoints.control = 1,\n                se = FALSE, \n                method = 'loess')\nPSAgraphics::circ.psa(response = log(lalonde$re78 + 1), \n                      treatment = lalonde$treat == 1, \n                      strata = lalonde$lr_strata5)## $summary.strata\n##   n.FALSE n.TRUE means.FALSE means.TRUE\n## A      66     23    6.280406   6.600537\n## B      61     28    4.409935   5.129193\n## C      51     38    6.212981   6.455034\n## D      42     47    4.705981   6.208840\n## E      40     49    5.783529   7.576461\n## \n## $wtd.Mn.FALSE\n## [1] 5.478567\n## \n## $wtd.Mn.TRUE\n## [1] 6.394013\n## \n## $ATE\n## [1] 0.9154463\n## \n## $se.wtd\n## [1] 0.394155\n## \n## $approx.t\n## [1] 2.322554\n## \n## $df\n## [1] 435\n## \n## $CI.95\n## [1] 0.1407612 1.6901314"},{"path":"chapter-stratification.html","id":"phase-iii-sensitivity-analysis-1","chapter":"2 Stratification","heading":"2.3 Phase III: Sensitivity Analysis","text":"Now established statistically significant effect intervention adjusting selection bias using propensity scores want evaluate robustness effect. Sensitivity analysis one approach well defined matching methods. chapter 6 introduce bootstrapping method can help test robustness. Rosenbaum (2012) suggest another approach test sensitivity test null hypothesis twice. using classification tree approach estimating propensity scores strata.","code":""},{"path":"chapter-stratification.html","id":"estimate-propensity-scores-classification-tree","chapter":"2 Stratification","heading":"2.3.1 Estimate Propensity Scores (classification tree)","text":"\n(#fig:tree_plot)Classification tree\n","code":"\nlibrary(tree)\ntree_out <- tree::tree(lalonde_formu,\n                       data = lalonde)\nplot(tree_out); text(tree_out)\nlalonde$tree_ps <- predict(tree_out)\ntable(lalonde$tree_ps, lalonde$treat, useNA = 'ifany')##                    \n##                       0   1\n##   0.332             167  83\n##   0.344827586206897  19  10\n##   0.351851851851852  35  19\n##   0.612903225806452  24  38\n##   0.659090909090909  15  29\n##   1                   0   6\nlalonde$tree_strata <- predict(tree_out, type = 'where')\ntable(lalonde$tree_strata, lalonde$treat, useNA = 'ifany')##     \n##        0   1\n##   3  167  83\n##   5   15  29\n##   6   35  19\n##   9   24  38\n##   10  19  10\n##   11   0   6"},{"path":"chapter-matching.html","id":"chapter-matching","chapter":"3 Matching","heading":"3 Matching","text":"matchverb\n1. correspond cause correspond essential respect; make harmonious.\n2. equal (something) quality strength.name suggests, propensity score matching concerned matching treatment control observations…Matching Methods","code":""},{"path":"chapter-matching.html","id":"nearest-neighbor-matching","chapter":"3 Matching","heading":"3.0.1 Nearest Neighbor Matching","text":"MatchIt::matchit(method = “nearest”)","code":""},{"path":"chapter-matching.html","id":"optimal-pair-matching","chapter":"3 Matching","heading":"3.0.2 Optimal Pair Matching","text":"MatchIt::matchit(method = “optimal”)","code":""},{"path":"chapter-matching.html","id":"optimal-full-matching","chapter":"3 Matching","heading":"3.0.3 Optimal Full Matching","text":"MatchIt::matchit(method = “full”)","code":""},{"path":"chapter-matching.html","id":"generalized-full-matching","chapter":"3 Matching","heading":"3.0.4 Generalized Full Matching","text":"MatchIt::matchit(method = “quick”)","code":""},{"path":"chapter-matching.html","id":"genetic-matching","chapter":"3 Matching","heading":"3.0.5 Genetic Matching","text":"MatchIt::matchit(method = “genetic”)X <- cbind(age, educ, black, hisp, married, nodegr, re74, re75, u74, u75)\nBalanceMatrix <- cbind(age, (age^2), educ, (educ^2), black, hisp,\nmarried, nodegr, re74, (re74^2), re75, (re75^2), u74, u75,\n(re74 * re75), (age * nodegr), (educ * re74), (educ * re75))\ngen1 <- GenMatch(Tr = Tr, X = X, BalanceMatrix = BalanceMatrix,\npop.size = 1000)","code":""},{"path":"chapter-matching.html","id":"exact-matching","chapter":"3 Matching","heading":"3.0.6 Exact Matching","text":"MatchIt::matchit(method = “exact”)","code":""},{"path":"chapter-matching.html","id":"coarsened-exact-matching","chapter":"3 Matching","heading":"3.0.7 Coarsened Exact Matching","text":"MatchIt::matchit(method = “cem”)","code":""},{"path":"chapter-matching.html","id":"subclassification","chapter":"3 Matching","heading":"3.0.8 Subclassification","text":"MatchIt::matchit(method = “subclass”)","code":""},{"path":"chapter-matching.html","id":"cardinality-and-profile-matching","chapter":"3 Matching","heading":"3.0.9 Cardinality and Profile Matching","text":"MatchIt::matchit(method = “cardinality”)\nFigure 3.1: Propensity Scores Logistic Regression Sample Matched Pairs\n","code":"\nlr_out <- glm(lalonde.formu, \n              data = lalonde,\n              family = binomial(link = logit))\nlalonde$lr_ps <- fitted(lr_out)  # Propensity scores"},{"path":"chapter-matching.html","id":"one-to-one-matching-ate","chapter":"3 Matching","heading":"3.1 One-to-One Matching ATE","text":"One--one matching replacement (M = 1 option). Estimating treatment effect treated (default ATT).","code":"\nrr_att <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                estimand='ATT')\nsummary(rr_att) # The default estimate is ATT here## \n## Estimate...  2153.3 \n## AI SE......  825.4 \n## T-stat.....  2.6088 \n## p.val......  0.0090858 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  346"},{"path":"chapter-matching.html","id":"checking-balance","chapter":"3 Matching","heading":"3.1.1 Checking Balance","text":"\nFigure 3.2: Covariate Balance Plot Matching\n","code":"\nrr_att_mb <- psa::MatchBalance(\n    df = lalonde,\n    formu = lalonde.formu,\n    formu.Y = update.formula(lalonde.formu, re78 ~ .),\n    index.treated = rr_att$index.treated,\n    index.control = rr_att$index.control,\n    tolerance = 0.25,\n    M = 1,\n    estimand = 'ATT')\nplot(rr_att_mb)\n# ls(rr_att_mb)\nsummary(rr_att_mb)## Sample sizes and number of matches:\n##    Group   n n.matched n.percent.matched\n##  Treated 185       185         1.0000000\n##  Control 260       173         0.6653846\n##    Total 445       358         0.8044944\n## \n## Covariate importance and t-tests for matched pairs:\n##           Import.Treat Import.Y Import.Total std.estimate       t p.value\n## I(educ^2)        1.931   1.5228        3.453     -0.04903 -0.8916  0.3732\n## educ             2.099   1.2121        3.311     -0.05483 -0.9577  0.3389\n## black            0.705   1.8424        2.547     -0.02326 -0.5383  0.5907\n## I(re74^2)        0.353   1.6415        1.994      0.07581  2.0955  0.0369\n## u75              0.852   0.9435        1.796     -0.06655 -1.8144  0.0705\n## hisp             1.731   0.0404        1.771      0.02042  0.8161  0.4150\n## nodegr           1.090   0.5011        1.591      0.03496  1.0914  0.2759\n## re74             0.280   1.1019        1.382      0.07979  1.7483  0.0813\n## re75             0.642   0.5903        1.232      0.06147  1.3171  0.1887\n## age              0.237   0.6729        0.910      0.00896  0.1374  0.8908\n## married          0.646   0.1406        0.787      0.04627  1.0000  0.3180\n## I(re75^2)        0.390   0.3817        0.772      0.05125  1.0364  0.3007\n## I(age^2)         0.232   0.5096        0.742      0.00297  0.0438  0.9651\n## u74              0.184   0.0702        0.254      0.03913  0.7495  0.4541\n##             ci.min  ci.max PercentMatched\n## I(educ^2) -0.15719 0.05913           60.4\n## educ      -0.16744 0.05778           60.1\n## black     -0.10826 0.06173           91.0\n## I(re74^2)  0.00465 0.14696           86.7\n## u75       -0.13870 0.00559           89.3\n## hisp      -0.02879 0.06963           98.3\n## nodegr    -0.02804 0.09797           93.9\n## re74      -0.00998 0.16956           77.2\n## re75      -0.03032 0.15326           78.0\n## age       -0.11922 0.13713           44.8\n## married   -0.04474 0.13728           89.6\n## I(re75^2) -0.04602 0.14852           88.7\n## I(age^2)  -0.13062 0.13656           49.7\n## u74       -0.06356 0.14183           81.5"},{"path":"chapter-matching.html","id":"one-to-one-matching-att","chapter":"3 Matching","heading":"3.2 One-to-One matching (ATT)","text":"","code":"\nrr.ate <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                estimand = 'ATE')\nsummary(rr.ate)## \n## Estimate...  2013.3 \n## AI SE......  817.76 \n## T-stat.....  2.4619 \n## p.val......  0.013819 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  445 \n## Matched number of observations  (unweighted).  756"},{"path":"chapter-matching.html","id":"one-to-many-matching-att","chapter":"3 Matching","heading":"3.3 One-to-Many Matching (ATT)","text":"","code":"\nrr2 <- Match(Y = lalonde$re78,      \n             Tr = lalonde$treat, \n             X = lalonde$lr_ps,\n             M = 1, \n             ties = TRUE, \n             replace = TRUE,\n             estimand = 'ATT')\nsummary(rr2) # The default estimate is ATT here## \n## Estimate...  2153.3 \n## AI SE......  825.4 \n## T-stat.....  2.6088 \n## p.val......  0.0090858 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  346"},{"path":"chapter-matching.html","id":"the-matchit-package","chapter":"3 Matching","heading":"3.4 The MatchIt Package","text":"","code":"\nmatchit.out <- MatchIt::matchit(lalonde.formu, data = lalonde)\nsummary(matchit.out)## \n## Call:\n## MatchIt::matchit(formula = lalonde.formu, data = lalonde)\n## \n## Summary of Balance for All Data:\n##           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance         0.4468        0.3936          0.4533     1.2101    0.1340\n## age             25.8162       25.0538          0.1066     1.0278    0.0254\n## I(age^2)       717.3946      677.3154          0.0929     1.0115    0.0254\n## educ            10.3459       10.0885          0.1281     1.5513    0.0287\n## I(educ^2)      111.0595      104.3731          0.1701     1.6625    0.0287\n## black            0.8432        0.8269          0.0449          .    0.0163\n## hisp             0.0595        0.1077         -0.2040          .    0.0482\n## married          0.1892        0.1538          0.0902          .    0.0353\n## nodegr           0.7081        0.8346         -0.2783          .    0.1265\n## re74          2095.5740     2107.0268         -0.0023     0.7381    0.0192\n## I(re74^2) 28141433.9907 36667413.1577         -0.0747     0.5038    0.0192\n## re75          1532.0556     1266.9092          0.0824     1.0763    0.0508\n## I(re75^2) 12654752.6909 11196530.0057          0.0260     1.4609    0.0508\n## u74              0.7081        0.7500         -0.0921          .    0.0419\n## u75              0.6000        0.6846         -0.1727          .    0.0846\n##           eCDF Max\n## distance    0.2244\n## age         0.0652\n## I(age^2)    0.0652\n## educ        0.1265\n## I(educ^2)   0.1265\n## black       0.0163\n## hisp        0.0482\n## married     0.0353\n## nodegr      0.1265\n## re74        0.0471\n## I(re74^2)   0.0471\n## re75        0.1075\n## I(re75^2)   0.1075\n## u74         0.0419\n## u75         0.0846\n## \n## Summary of Balance for Matched Data:\n##           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance         0.4468        0.4284          0.1571     1.3077    0.0387\n## age             25.8162       25.1351          0.0952     1.1734    0.0243\n## I(age^2)       717.3946      675.1676          0.0979     1.1512    0.0243\n## educ            10.3459       10.2649          0.0403     1.2869    0.0174\n## I(educ^2)      111.0595      108.4919          0.0653     1.3938    0.0174\n## black            0.8432        0.8486         -0.0149          .    0.0054\n## hisp             0.0595        0.0703         -0.0457          .    0.0108\n## married          0.1892        0.1892          0.0000          .    0.0000\n## nodegr           0.7081        0.7676         -0.1308          .    0.0595\n## re74          2095.5740     1741.2109          0.0725     1.5797    0.0146\n## I(re74^2) 28141433.9907 18066538.6428          0.0883     3.5436    0.0146\n## re75          1532.0556     1314.8073          0.0675     1.3933    0.0264\n## I(re75^2) 12654752.6909  9126579.7979          0.0630     3.4873    0.0264\n## u74              0.7081        0.7243         -0.0357          .    0.0162\n## u75              0.6000        0.6108         -0.0221          .    0.0108\n##           eCDF Max Std. Pair Dist.\n## distance    0.1189          0.1585\n## age         0.0541          0.8159\n## I(age^2)    0.0541          0.7701\n## educ        0.0595          0.7662\n## I(educ^2)   0.0595          0.7604\n## black       0.0054          0.5798\n## hisp        0.0108          0.2286\n## married     0.0000          0.2378\n## nodegr      0.0595          0.5588\n## re74        0.0432          0.6080\n## I(re74^2)   0.0432          0.3620\n## re75        0.0649          0.7292\n## I(re75^2)   0.0649          0.3690\n## u74         0.0162          0.7728\n## u75         0.0108          0.7282\n## \n## Sample Sizes:\n##           Control Treated\n## All           260     185\n## Matched       185     185\n## Unmatched      75       0\n## Discarded       0       0\n# Same as above but calculate average treatment effect\nrr.ate <- Match(Y = lalonde$re78, \n                Tr = lalonde$treat, \n                X = lalonde$lr_ps,\n                M = 1,\n                ties = FALSE, \n                replace = FALSE, \n                estimand='ATE')\nsummary(rr.ate) # Here the estimate is ATE## \n## Estimate...  2130.3 \n## SE.........  496.24 \n## T-stat.....  4.2929 \n## p.val......  1.7638e-05 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  370 \n## Matched number of observations  (unweighted).  370\n## Genetic Matching\nrr.gen <- GenMatch(Tr = lalonde$treat, \n                   X = lalonde$lr_ps, \n                   BalanceMatrix = lalonde[,all.vars(lalonde.formu)[-1]],\n                   estimand = 'ATE', \n                   M = 1, \n                   pop.size = 16,\n                   print.level = 0)\nrr.gen.mout <- Match(Y = lalonde$re78, \n                     Tr = lalonde$treat, \n                     X = lalonde$lr_ps,\n                     estimand = 'ATE',\n                     Weight.matrix = rr.gen)\nsummary(rr.gen.mout)## \n## Estimate...  2159 \n## AI SE......  814.91 \n## T-stat.....  2.6494 \n## p.val......  0.0080631 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  445 \n## Matched number of observations  (unweighted).  650\n## Partial exact matching\nrr2 <- Matchby(Y = lalonde$re78, \n               Tr = lalonde$treat, \n               X = lalonde$lr_ps, \n               by = factor(lalonde$nodegr),\n               print.level = 0)\nsummary(rr2)## \n## Estimate...  2333.2 \n## SE.........  682.81 \n## T-stat.....  3.417 \n## p.val......  0.00063307 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185\n## Partial exact matching on two covariates\nrr3 <- Matchby(Y = lalonde$re78, \n               Tr = lalonde$treat, \n               X = lalonde$lr_ps, \n               by = lalonde[,c('nodegr','married')],\n               print.level = 0)\nsummary(rr3)## \n## Estimate...  1961.7 \n## SE.........  701.93 \n## T-stat.....  2.7947 \n## p.val......  0.0051952 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185"},{"path":"chapter-weighting.html","id":"chapter-weighting","chapter":"4 Weighting","heading":"4 Weighting","text":"weightverb\n1. hold (something) placing heavy object top .\n2. attach importance value .Propensity score weighting approach using propensity scores weights statistical models regression ANOVA. Like stratification (see Chapter 2), propensity score weighting advantage observations. section 1.3.2 introduced four different treatment estimators. histograms used conceptually explain observations included, included, calculation used propensity score weights. chapter discuss mathematical details weights calculated applied, include R code generate estimates.present formula treatment effects wish estimate. formulas define weights. weights can use statistical model using following formula estimate treatment effect.\\[\\begin{equation}\n\\begin{aligned}\nTreatment\\ Effect = \\frac{\\sum Y_{}Z_{}w_{}}{\\sum Z_{} w_{}} - \\frac{\\sum Y_{}(1 - Z_{}) w_{}}{\\sum (1 - Z_{}) w_{} }\n\\end{aligned}\n\\tag{4.1}\n\\end{equation}\\]equation (4.1), \\(w\\) weight (defined following sections), \\(Z_i\\) treatment assignment \\(Z = 1\\) treatment \\(Z = 0\\) control, \\(Y_i\\) outcome.","code":""},{"path":"chapter-weighting.html","id":"estimate-propensity-scores","chapter":"4 Weighting","heading":"4.1 Estimate Propensity Scores","text":"begin, estimate propensity scores, using logistic regression.","code":"\ndata(\"lalonde\", package = 'Matching')\nlr_out <- glm(formula = lalonde.formu,\n              data = lalonde,\n              family = binomial(link = 'logit'))\nlalonde$lr_ps <- fitted(lr_out)"},{"path":"chapter-weighting.html","id":"checking-balance-1","chapter":"4 Weighting","heading":"4.2 Checking Balance","text":"Checking balance propensity score weighting stratification. Figure 4.1 multiple covariate balance assessment plot. See section 2.1.2 stratification chapter details can check balance individual covariates.\nFigure 4.1: Multiple covariate balance assessment plot Lalonde data estimating propensity scores logistic regression\none additional balance check can done propensity score weights. can run propensity score estimation model estimated propensity score weights. result covariates non-statistically significant effect treatment. explore details four treatment effects discussed next section.","code":"\nPSAgraphics::cv.bal.psa(covariates = lalonde[,all.vars(lalonde.formu)[-1]],\n                        treatment = lalonde$treat,\n                        propensity = lalonde$lr_ps,\n                        strata = 5)"},{"path":"chapter-weighting.html","id":"average-treatment-effect-ate-1","chapter":"4 Weighting","heading":"4.3 Average Treatment Effect (ATE)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATE} = \\frac{Z_i}{\\pi_i} + \\frac{1 - Z_i}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.2}\n\\end{equation}\\]","code":"\nate_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps,                          \n                                         estimand = 'ATE')"},{"path":"chapter-weighting.html","id":"check-balance-with-ate-weights","chapter":"4 Weighting","heading":"4.3.1 Check Balance with ATE Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = ate_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = ate_weights)\n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -2.000e-01  1.977e+00  -0.101    0.919\n## age          2.686e-02  8.513e-02   0.316    0.753\n## I(age^2)    -4.695e-04  1.397e-03  -0.336    0.737\n## educ        -6.326e-02  4.024e-01  -0.157    0.875\n## I(educ^2)    3.510e-03  2.259e-02   0.155    0.877\n## black       -3.695e-03  3.714e-01  -0.010    0.992\n## hisp         2.232e-02  4.904e-01   0.046    0.964\n## married     -8.664e-03  2.784e-01  -0.031    0.975\n## nodegr       4.154e-02  3.889e-01   0.107    0.915\n## re74         2.291e-05  7.493e-05   0.306    0.760\n## I(re74^2)   -9.734e-10  2.337e-09  -0.416    0.677\n## re75         5.001e-06  1.015e-04   0.049    0.961\n## I(re75^2)   -3.543e-10  5.032e-09  -0.070    0.944\n## u74          8.163e-02  4.449e-01   0.183    0.854\n## u75         -4.153e-04  3.566e-01  -0.001    0.999\n## \n## (Dispersion parameter for quasibinomial family taken to be 2.067727)\n## \n##     Null deviance: 1232.6  on 444  degrees of freedom\n## Residual deviance: 1231.9  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-ate","chapter":"4 Weighting","heading":"4.3.2 Estimate ATE","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = ate_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = ate_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     4556        450  10.125   <2e-16 ***\n## treat           1558        637   2.446   0.0148 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9497 on 443 degrees of freedom\n## Multiple R-squared:  0.01333,    Adjusted R-squared:  0.0111 \n## F-statistic: 5.983 on 1 and 443 DF,  p-value: 0.01483\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = ate_weights)## 1558.09"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-treated-att-1","chapter":"4 Weighting","heading":"4.4 Average Treatment Effect Among the Treated (ATT)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATT} = \\frac{\\pi_i Z_i}{\\pi_i} + \\frac{\\pi_i (1 - Z_i)}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.3}\n\\end{equation}\\]","code":"\natt_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATT')"},{"path":"chapter-weighting.html","id":"check-balance-with-att-weights","chapter":"4 Weighting","heading":"4.4.1 Check Balance with ATT Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = att_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = att_weights)\n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  1.122e-01  1.754e+00   0.064    0.949\n## age          2.350e-02  8.362e-02   0.281    0.779\n## I(age^2)    -4.382e-04  1.352e-03  -0.324    0.746\n## educ        -1.279e-01  3.424e-01  -0.374    0.709\n## I(educ^2)    7.725e-03  1.931e-02   0.400    0.689\n## black       -5.090e-02  3.388e-01  -0.150    0.881\n## hisp        -7.925e-02  5.202e-01  -0.152    0.879\n## married     -2.667e-02  2.691e-01  -0.099    0.921\n## nodegr       1.449e-01  3.623e-01   0.400    0.689\n## re74         9.327e-06  7.444e-05   0.125    0.900\n## I(re74^2)   -9.597e-11  2.521e-09  -0.038    0.970\n## re75        -1.340e-05  9.575e-05  -0.140    0.889\n## I(re75^2)    7.444e-10  4.817e-09   0.155    0.877\n## u74         -6.362e-02  4.320e-01  -0.147    0.883\n## u75          8.469e-02  3.390e-01   0.250    0.803\n## \n## (Dispersion parameter for quasibinomial family taken to be 0.8614842)\n## \n##     Null deviance: 513.54  on 444  degrees of freedom\n## Residual deviance: 513.06  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-att","chapter":"4 Weighting","heading":"4.4.2 Estimate ATT","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = att_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = att_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4557.4      454.2  10.033  < 2e-16 ***\n## treat         1791.7      642.8   2.787  0.00554 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6186 on 443 degrees of freedom\n## Multiple R-squared:  0.01724,    Adjusted R-squared:  0.01502 \n## F-statistic:  7.77 on 1 and 443 DF,  p-value: 0.00554\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = att_weights)## 1791.72"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-control-atc-1","chapter":"4 Weighting","heading":"4.5 Average Treatment Effect Among the Control (ATC)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATC} = \\frac{(1 - \\pi_i) Z_i}{\\pi_i} + \\frac{(1 - e_i)(1 - Z_i)}{1 - \\pi_i}\n\\end{aligned}\n\\tag{4.4}\n\\end{equation}\\]","code":"\natc_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATC')"},{"path":"chapter-weighting.html","id":"check-balance-with-atc-weights","chapter":"4 Weighting","heading":"4.5.1 Check Balance with ATC Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = atc_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = atc_weights)\n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept) -6.598e-01  2.390e+00  -0.276    0.783\n## age          3.097e-02  8.728e-02   0.355    0.723\n## I(age^2)    -5.201e-04  1.449e-03  -0.359    0.720\n## educ         4.722e-02  4.975e-01   0.095    0.924\n## I(educ^2)   -3.225e-03  2.766e-02  -0.117    0.907\n## black        3.598e-02  4.033e-01   0.089    0.929\n## hisp         7.912e-02  4.941e-01   0.160    0.873\n## married      7.290e-03  2.868e-01   0.025    0.980\n## nodegr      -7.488e-02  4.205e-01  -0.178    0.859\n## re74         2.763e-05  7.658e-05   0.361    0.718\n## I(re74^2)   -1.296e-09  2.319e-09  -0.559    0.577\n## re75         2.037e-05  1.073e-04   0.190    0.849\n## I(re75^2)   -1.341e-09  5.282e-09  -0.254    0.800\n## u74          1.831e-01  4.577e-01   0.400    0.689\n## u75         -7.234e-02  3.730e-01  -0.194    0.846\n## \n## (Dispersion parameter for quasibinomial family taken to be 1.206136)\n## \n##     Null deviance: 719.09  on 444  degrees of freedom\n## Residual deviance: 717.84  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"chapter-weighting.html","id":"estimate-atc","chapter":"4 Weighting","heading":"4.5.2 Estimate ATC","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = atc_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = atc_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4554.8      446.8  10.195   <2e-16 ***\n## treat         1391.0      632.6   2.199   0.0284 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7204 on 443 degrees of freedom\n## Multiple R-squared:  0.0108, Adjusted R-squared:  0.008564 \n## F-statistic: 4.835 on 1 and 443 DF,  p-value: 0.0284\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = atc_weights)## 1391.02"},{"path":"chapter-weighting.html","id":"average-treatment-effect-among-the-evenly-matched-atm-1","chapter":"4 Weighting","heading":"4.6 Average Treatment Effect Among the Evenly Matched (ATM)","text":"\\[\\begin{equation}\n\\begin{aligned}\nw_{ATM} = \\frac{min\\{\\pi_i, 1 - \\pi_i\\}}{Z_i \\pi_i (1 - Z_i)(1 - \\pi_i)}\n\\end{aligned}\n\\tag{4.5}\n\\end{equation}\\]","code":"\natm_weights <- psa::calculate_ps_weights(treatment = lalonde$treat,\n                                         ps = lalonde$lr_ps, \n                                         estimand = 'ATM')"},{"path":"chapter-weighting.html","id":"check-balance-with-atm-weights","chapter":"4 Weighting","heading":"4.6.1 Check Balance with ATM Weights","text":"","code":"\nglm(formula = lalonde.formu,\n    data = lalonde,\n    family = quasibinomial(link = 'logit'),\n    weights = atm_weights\n) |> summary()## \n## Call:\n## glm(formula = lalonde.formu, family = quasibinomial(link = \"logit\"), \n##     data = lalonde, weights = atm_weights)\n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)\n## (Intercept)  1.657e-01  2.129e+00   0.078    0.938\n## age         -2.418e-02  8.893e-02  -0.272    0.786\n## I(age^2)     4.192e-04  1.465e-03   0.286    0.775\n## educ         6.308e-02  4.304e-01   0.147    0.884\n## I(educ^2)   -3.347e-03  2.420e-02  -0.138    0.890\n## black        1.480e-02  3.577e-01   0.041    0.967\n## hisp        -3.495e-02  5.203e-01  -0.067    0.946\n## married     -3.486e-03  2.736e-01  -0.013    0.990\n## nodegr      -4.659e-02  3.825e-01  -0.122    0.903\n## re74        -2.333e-05  7.526e-05  -0.310    0.757\n## I(re74^2)    8.298e-10  2.510e-09   0.331    0.741\n## re75        -7.419e-06  9.758e-05  -0.076    0.939\n## I(re75^2)    7.957e-10  4.775e-09   0.167    0.868\n## u74         -6.630e-02  4.360e-01  -0.152    0.879\n## u75         -3.051e-02  3.436e-01  -0.089    0.929\n## \n## (Dispersion parameter for quasibinomial family taken to be 0.7850194)\n## \n##     Null deviance: 467.94  on 444  degrees of freedom\n## Residual deviance: 467.73  on 430  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 3"},{"path":"chapter-weighting.html","id":"estimate-atm","chapter":"4 Weighting","heading":"4.6.2 Estimate ATM","text":"","code":"\nlm(formula = re78 ~ treat, \n   data = lalonde,\n   weights = atm_weights) |> summary()## \n## Call:\n## lm(formula = re78 ~ treat, data = lalonde, weights = atm_weights)\n## \n## Weighted Residuals:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4504.6      459.8   9.797  < 2e-16 ***\n## treat         1707.7      648.8   2.632  0.00878 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5960 on 443 degrees of freedom\n## Multiple R-squared:  0.0154, Adjusted R-squared:  0.01318 \n## F-statistic: 6.928 on 1 and 443 DF,  p-value: 0.008783\npsa::treatment_effect(treatment = lalonde$treat,\n                      outcome = lalonde$re78,\n                      weights = atm_weights)## 1707.69"},{"path":"chapter-sensitivity.html","id":"chapter-sensitivity","chapter":"5 Sensitivity Analysis","heading":"5 Sensitivity Analysis","text":"","code":"\nrequire(rbounds)\ndata(lalonde, package='Matching')\n\nY  <- lalonde$re78   #the outcome of interest\nTr <- lalonde$treat #the treatment of interest\nattach(lalonde)\n# The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n# The covariates we want to obtain balance on\nBalanceMat <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74,\n                    I(re74*re75))\ndetach(lalonde)\n\ngen1 <- GenMatch(Tr=Tr, X=X, BalanceMat=BalanceMat, pop.size=50,\n                  data.type.int=FALSE, print=0, replace=FALSE)\nmgen1 <- Match(Y=Y, Tr=Tr, X=X, Weight.matrix=gen1, replace=FALSE)\nsummary(mgen1)## \n## Estimate...  1613.6 \n## SE.........  721.22 \n## T-stat.....  2.2373 \n## p.val......  0.025266 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  185 \n## Matched number of observations  (unweighted).  185\nrbounds::psens(x = Y[mgen1$index.treated],\n      y =Y[mgen1$index.contro],\n      Gamma = 1.5, \n      GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n##  \n## Unconfounded estimate ....  0.0228 \n## \n##  Gamma Lower bound Upper bound\n##    1.0      0.0228      0.0228\n##    1.1      0.0056      0.0716\n##    1.2      0.0012      0.1640\n##    1.3      0.0002      0.2970\n##    1.4      0.0000      0.4516\n##    1.5      0.0000      0.6030\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## \nrbounds::hlsens(x = Y[mgen1$index.treated],\n       y = Y[mgen1$index.contro],\n       Gamma = 1.5, \n       GammaInc = .1)## \n##  Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate \n##  \n## Unconfounded estimate ....  1431.4 \n## \n##  Gamma Lower bound Upper bound\n##    1.0  1.4314e+03      1431.4\n##    1.1  7.9320e+02      1547.1\n##    1.2  4.9780e+02      1901.1\n##    1.3  2.0850e+02      2162.1\n##    1.4 -3.4140e-05      2441.0\n##    1.5 -2.1990e+02      2694.4\n## \n##  Note: Gamma is Odds of Differential Assignment To\n##  Treatment Due to Unobserved Factors \n## "},{"path":"chapter-bootstrapping.html","id":"chapter-bootstrapping","chapter":"6 Bootstrapping","heading":"6 Bootstrapping","text":"\nFigure 6.1: Mean difference across bootstrap samples method\nDetails available within returned object","code":"\nlibrary(PSAboot)\n\nboot.matching.1to3 <- function(Tr, Y, X, X.trans, formu, ...) {\n    return(boot.matching(Tr=Tr, Y=Y, X=X, X.trans=X.trans, formu=formu, M=3, ...))\n}\n\n\nboot_out <- PSAboot(Tr = lalonde$treat == 1, \n                    Y = lalonde$re78, \n                    X = lalonde[,all.vars(lalonde.formu)[-1]], \n                    seed = 2112,\n                    methods=c('Stratification' = boot.strata,\n                              'ctree' = boot.ctree,\n                              'rpart' = boot.rpart,\n                              'Matching' = boot.matching,\n                              'Matching-1-to-3' = boot.matching.1to3,\n                              'MatchIt' = boot.matchit) )\n\nsummary(boot_out)\nplot(boot_out)\nboxplot(boot_out)\nmatrixplot(boot_out)\nboot_balance <- balance(boot_out)\nboot_balance## Unadjusted balance: 1.48309081605193## 0.04\n## 0.08\n## 0.05\n## 0.06\n## 0.01\n## 0.06\n## 0.04\n##  NA\n## 0.07\n## 0.07\n## 0.05\n## 0.08\nplot(boot_balance)\nboxplot(boot_balance) + geom_hline(yintercept=.1, color='red')\nboot_balance$unadjusted## 3.51\n## 5.46\n## 1.12\n## 1.16\n## 0.66\n## 0.89\n## 0.39\n## 0.44\n## 0.72\n## 0.49\nboot_balance$complete## 0.04\n## 0.00\n## 0.00\n## 0.01\n## 0.00\n## 0.16\n## 0.03\n## 0.16\n## 0.05\n## 0.04\n## 0.02\n## 0.06\n## 0.09\n## 0.15\n## 0.14\n## 0.04\n## 0.01\n## 0.02\n## 0.04\n## 0.05\n## 0.08\n## 0.04\n## 0.00\n## 0.09\n## 0.02\n## 0.08\n## 0.10\n## 0.09\n## 0.01\n## 0.02\n## 0.03\n## 0.08\n## 0.02\n## 0.09\n## 0.02\n## 0.07\n## 0.00\n## 0.09\n## 0.05\n## 0.06\n## 0.00\n## 0.00\n## 0.03\n## 0.09\n## 0.01\n## 0.07\n## 0.01\n## 0.04\n## 0.06\n## 0.06\n## 0.06\n## 0.02\n## 0.05\n## 0.08\n## 0.03\n## 0.03\n## 0.01\n## 0.10\n## 0.00\n## 0.05\nboot_balance$pooled |> head()## 0.02\n## 0.03\n## 0.04\n## 0.03\n## 0.04\n## 0.05\n## 0.07\n## 0.09\n## 0.12\n##  NA\n## 0.06\n## 0.12\n## 0.05\n## 0.09\n## 0.08\n## 0.11\n## 0.08\n## 0.07\n## 0.06\n## 0.07\n## 0.03\n## 0.07\n## 0.05\n## 0.06\n## 0.04\n## 0.03\n## 0.03\n## 0.08\n## 0.03\n## 0.05\n## 0.08\n## 0.10\n## 0.09\n## 0.07\n## 0.10\n## 0.10"},{"path":"chapter-missing.html","id":"chapter-missing","chapter":"7 Missing Data","heading":"7 Missing Data","text":"Create copy covariates simulate missing random (mar) missing random (nmar).Add missingness existing data. missing random data treatment units twice many missing values control group.proportion missing values first covariateCreate shadow matrix. logical vector cell TRUE value missing original data frame.Change column names include “_miss” name.Impute missing values using mice packageGet imputed data set.Estimate propensity scores using logistic regression.see two indicator columns shadow matrix statistically significant predictors suggesting data missing random.","code":"\nrequire(Matching)\nrequire(mice)\ndata(lalonde, package='Matching')\nTr <- lalonde$treat\nY <- lalonde$re78\nX <- lalonde[,c('age','educ','black','hisp','married','nodegr','re74','re75')]\nlalonde.glm <- glm(treat ~ ., family=binomial, data=cbind(treat=Tr, X))\nsummary(lalonde.glm)## \n## Call:\n## glm(formula = treat ~ ., family = binomial, data = cbind(treat = Tr, \n##     X))\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  1.178e+00  1.056e+00   1.115  0.26474   \n## age          4.698e-03  1.433e-02   0.328  0.74297   \n## educ        -7.124e-02  7.173e-02  -0.993  0.32061   \n## black       -2.247e-01  3.655e-01  -0.615  0.53874   \n## hisp        -8.528e-01  5.066e-01  -1.683  0.09228 . \n## married      1.636e-01  2.769e-01   0.591  0.55463   \n## nodegr      -9.035e-01  3.135e-01  -2.882  0.00395 **\n## re74        -3.161e-05  2.584e-05  -1.223  0.22122   \n## re75         6.161e-05  4.358e-05   1.414  0.15744   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 604.20  on 444  degrees of freedom\n## Residual deviance: 587.22  on 436  degrees of freedom\n## AIC: 605.22\n## \n## Number of Fisher Scoring iterations: 4\nlalonde.mar <- X\nlalonde.nmar <- X\n\nmissing.rate <- .2 # What percent of rows will have missing data\nmissing.cols <- c('nodegr', 're75') # The columns we will add missing values to\n\n# Vectors indiciating which rows are treatment and control.\ntreat.rows <- which(lalonde$treat == 1)\ncontrol.rows <- which(lalonde$treat == 0)\nset.seed(2112)\nfor(i in missing.cols) {\n    lalonde.mar[sample(nrow(lalonde), nrow(lalonde) * missing.rate), i] <- NA\n    lalonde.nmar[sample(treat.rows, length(treat.rows) * missing.rate * 2), i] <- NA\n    lalonde.nmar[sample(control.rows, length(control.rows) * missing.rate), i] <- NA\n}\nprop.table(table(is.na(lalonde.mar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                  0          1\n##   FALSE 0.46292135 0.33707865\n##   TRUE  0.12134831 0.07865169\nprop.table(table(is.na(lalonde.nmar[,missing.cols[1]]), lalonde$treat, useNA='ifany'))##        \n##                 0         1\n##   FALSE 0.4674157 0.2494382\n##   TRUE  0.1168539 0.1662921\nshadow.matrix.mar <- as.data.frame(is.na(lalonde.mar))\nshadow.matrix.nmar <- as.data.frame(is.na(lalonde.nmar))\nnames(shadow.matrix.mar) <- names(shadow.matrix.nmar) <- paste0(names(shadow.matrix.mar), '_miss')\nset.seed(2112)\nmice.mar <- mice(lalonde.mar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\nmice.nmar <- mice(lalonde.nmar, m=1)## \n##  iter imp variable\n##   1   1  nodegr  re75\n##   2   1  nodegr  re75\n##   3   1  nodegr  re75\n##   4   1  nodegr  re75\n##   5   1  nodegr  re75\ncomplete.mar <- complete(mice.mar)\ncomplete.nmar <- complete(mice.nmar)\nlalonde.mar.glm <- glm(treat~., data=cbind(treat=Tr, complete.mar, shadow.matrix.mar))\nlalonde.nmar.glm <- glm(treat~., data=cbind(treat=Tr, complete.nmar, shadow.matrix.nmar))\nsummary(lalonde.mar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.mar, \n##     shadow.matrix.mar))\n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       8.996e-01  2.504e-01   3.592 0.000366 ***\n## age               9.447e-04  3.395e-03   0.278 0.780957    \n## educ             -2.514e-02  1.706e-02  -1.474 0.141191    \n## black            -3.895e-02  8.746e-02  -0.445 0.656285    \n## hisp             -1.726e-01  1.156e-01  -1.493 0.136068    \n## married           3.008e-02  6.671e-02   0.451 0.652326    \n## nodegr           -2.672e-01  7.475e-02  -3.574 0.000390 ***\n## re74             -1.059e-05  5.681e-06  -1.863 0.063076 .  \n## re75              2.378e-05  1.059e-05   2.246 0.025227 *  \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE  -1.852e-02  5.853e-02  -0.316 0.751797    \n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE    -3.304e-02  5.870e-02  -0.563 0.573823    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2361624)\n## \n##     Null deviance: 108.09  on 444  degrees of freedom\n## Residual deviance: 102.49  on 434  degrees of freedom\n## AIC: 633.48\n## \n## Number of Fisher Scoring iterations: 2\nsummary(lalonde.nmar.glm)## \n## Call:\n## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.nmar, \n##     shadow.matrix.nmar))\n## \n## Coefficients: (6 not defined because of singularities)\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       7.427e-01  2.319e-01   3.203 0.001459 ** \n## age               7.641e-04  3.254e-03   0.235 0.814441    \n## educ             -2.451e-02  1.584e-02  -1.547 0.122656    \n## black            -1.964e-02  8.493e-02  -0.231 0.817243    \n## hisp             -1.366e-01  1.113e-01  -1.228 0.220246    \n## married           4.426e-02  6.440e-02   0.687 0.492303    \n## nodegr           -2.572e-01  7.143e-02  -3.601 0.000354 ***\n## re74             -3.326e-06  5.324e-06  -0.625 0.532421    \n## re75              4.742e-06  9.893e-06   0.479 0.631935    \n## age_missTRUE             NA         NA      NA       NA    \n## educ_missTRUE            NA         NA      NA       NA    \n## black_missTRUE           NA         NA      NA       NA    \n## hisp_missTRUE            NA         NA      NA       NA    \n## married_missTRUE         NA         NA      NA       NA    \n## nodegr_missTRUE   2.300e-01  4.957e-02   4.639 4.64e-06 ***\n## re74_missTRUE            NA         NA      NA       NA    \n## re75_missTRUE     2.246e-01  4.922e-02   4.563 6.57e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 0.2164914)\n## \n##     Null deviance: 108.090  on 444  degrees of freedom\n## Residual deviance:  93.957  on 434  degrees of freedom\n## AIC: 594.78\n## \n## Number of Fisher Scoring iterations: 2"},{"path":"chapter-non-binary.html","id":"chapter-non-binary","chapter":"8 Non-Binary Treatments","heading":"8 Non-Binary Treatments","text":"","code":""},{"path":"introduction.html","id":"introduction","chapter":"9 Introduction","heading":"9 Introduction","text":"Consider two treatments, \\(Tr_1\\) \\(Tr_2\\), control, \\(C\\). estimate propensity scores three separate logistic regression models model one predicts \\(Tr_1\\) \\(C\\), model two predicts \\(Tr_2\\) \\(C\\), model three predicts \\(Tr_1\\) \\(Tr_2\\). triangle plot Figure 10.1 depicts fitted values (.e. propensity scores) three models edge triangle. Since unit propensity score two three models, scores connected. can calculate three distances propensity scores possible matched triplet using three models. Given distances, matched triplets smallest standardized distance (.e. \\(D_{x,y} = \\frac{| PS_{x} - PS_{y} |}{sd(PS)}\\)) retained. Several methods determining matched triplets retain provided possibility researcher implement . black lines Figure 10.1 represent one matched triplet (.e. one row returned data frame).Propensity score analysis two groups typically use dependent sample t-tests (Austin 2010). analogue matched triplets include repeated measures ANOVA Freidman Rank Sum Test. TriMatch package provides utility functions conducting visualizing statistical tests. Moreover, set functions extending PSAgraphics (J. E. Helmreich Pruzek 2009) matched triplets check covariate balance provided.","code":""},{"path":"introduction.html","id":"the-trimatch-algorithm","chapter":"9 Introduction","heading":"9.1 The TriMatch Algorithm","text":"trips trimatch functions used estimate propensity scores find best matched triplets, respectively.. Propensity scores estimated three models using logistic regression.\n\\[{ PS }_{ 1 }=e({ x }_{ { T }_{ 1 }C })=Pr(z=1|{ X }_{ { T }_{ 1 }C })\\]\n\\[{ PS }_{ 2 }=e({ x }_{ { T }_{ 2 }C })=Pr(z=1|{ X}_{ { T }_{ 2 }C })\\]\n\\[{ PS }_{ 3 }=e({ x }_{ { T }_{ 2 }{ T }_{ 1 } })=Pr(z=1|{ X }_{ { T }_{ 2 }{ T }_{ 1 } })\\]B. Match order determined. default start larger two treatments, followed second treatment, lastly control group. However, match order configurable vis--vis match.order parameter.C. Three distance matrices calculated, \\(D_1\\), \\(D_2\\), \\(D_3\\) corresponding propensity scores estimated step ??. , \\(D_1\\) \\(n_{Tr_1}\\) x \\(n_{Tr_2}\\) matrix \\(D_1[x,y]\\) standardized distance \\(PS_1[x]\\) \\(PS_1[y]\\).D. Distances greater caliper, 0.25 default recommended (Rosenbaum Rubin 1985), eliminated. caliper specified standard units 0.25 corresponds one-quarter one standard deviation.E. partial exact matching desired, three logical matrices created dimensions distance matrices calculated step ??. , position \\(x,y\\) matrix true covariate(s) match exactly unit \\(x\\) \\(y\\) match exactly. Distances exact exact matches eliminated.F. remaining units, possible combinations matched triplets formed total standardized distance calculated.result procedure equivalent caliper matching two group case. , possible matches within specified caliper retained. can achieved specifying method = NULL parameter trimatch function. Two additional methods provided reduce number matched triplets. maximumTreat method attempts reduce number duplicate treatment units. analogous matching without replacement two group case. However, treatment 1 units may matched two different treatment 2 units treatment 2 unit otherwise matched. OneToN method allow user specify exactly many times treatment 1 treatment 2 may reused.","code":""},{"path":"effects-of-tutoring-on-course-grades.html","id":"effects-of-tutoring-on-course-grades","chapter":"10 Effects of Tutoring on Course Grades","heading":"10 Effects of Tutoring on Course Grades","text":"first example utilize observational data obtained evaluate effectiveness tutoring services course grades. Treatment students consisted students used tutoring services enrolled online writing course 2008 2011. comparison group identified students enrolled course section student used tutoring services. treatment group divided two based upon number times utilized tutoring services. “Novice” users used services “regular” users used services two times. Covariates available estimating propensity scores gender, ethnicity, military status, English second language learner, educational level mother father, age beginning course, employment level college enrollment, income level college enrollment, number transfer credits, GPA start course.courses represented structured variation section--section minimal. However, differences courses substantial therefore utilize partial exact matching matched students taken course.first step analysis estimate propensity scores. trips function estimate three propensity score models, \\(PS_1\\), \\(PS_2\\), \\(PS_3\\) described . Note specifying formula dependent variable, treatment indicator, included. trips function replace dependent variable estimates three logistic regression models.Figure 10.1 triangle plot depicts propensity scores three models. Since student two propensity scores, scores connected line. black line Figure 10.1 represents one matched triplet estimated .default trimatch use maximumTreat method retaining treatment unit treatment one units matched corresponding treatment two unit matched otherwise.Setting method parameter NULL result caliper matching. matched triplets within specified caliper retained. result largest number matched triplets.Lastly, use OneToN method retain 2--1--n 3--2-n matches.\nFigure 10.1: Traingle Plot\n","code":"\nnames(tutoring)##  [1] \"treat\"      \"Course\"     \"Grade\"      \"Gender\"     \"Ethnicity\" \n##  [6] \"Military\"   \"ESL\"        \"EdMother\"   \"EdFather\"   \"Age\"       \n## [11] \"Employment\" \"Income\"     \"Transfer\"   \"GPA\"        \"GradeCode\" \n## [16] \"Level\"      \"ID\"         \"treat2\"\ntable(tutoring$treat, tutoring$Course, useNA=\"ifany\")##          \n##           ENG*101 ENG*201 HSC*310\n##   Control     349     518      51\n##   Treat1       22      36      76\n##   Treat2       31      32      27\nformu <- ~ Gender + Ethnicity + Military + ESL + EdMother + EdFather + \nAge + Employment + Income + Transfer + GPA\ntutoring.tpsa <- trips(tutoring, tutoring$treat, formu)\nplot(tutoring.tpsa)\ntutoring.matched <- trimatch(tutoring.tpsa, exact=tutoring[,c(\"Course\")])\ntutoring.matched.caliper <- trimatch(tutoring.tpsa, \nexact=tutoring[,c(\"Course\")], method=NULL)\ntutoring.matched.2to1 <- trimatch(tutoring.tpsa, \nexact=tutoring[,c(\"Course\")], method=OneToN, M1=2, M2=1)\ntutoring.matched.3to2 <- trimatch(tutoring.tpsa, \nexact=tutoring[,c(\"Course\")], \nmethod=OneToN, M1=3, M2=2)\nprint(plot(tutoring.matched, rows=c(50), line.alpha=1, draw.segments=TRUE))"},{"path":"effects-of-tutoring-on-course-grades.html","id":"examining-unmatched-students","chapter":"10 Effects of Tutoring on Course Grades","heading":"10.1 Examining Unmatched Students","text":"different methods retaining matched triplets address issue overrepresentation treatment units. example four times many control units treatment units (ratio larger considering treatments separately). methods fall spectrum treatment unit used minimally (maximumTreat method) units used (caliper matching). (Rosenbaum 2012) suggests testing hypothesis general recommendation utilize multiple methods. Functions help present compare results multiple methods provided discussed .unmatched function return rows students matched. summary function provide information many students within group matched. shown , caliper matching match students. particular example, fact, substantial difference unmatched students control group. methods fail match 37 treatment one students. due fact another student within specified caliper match exactly course.","code":"\nsummary(unmatched(tutoring.matched))## 892 (78.1%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 834 (90.8%)  39 (29.1%)  19 (21.1%)\nsummary(unmatched(tutoring.matched.caliper))## 638 (55.9%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 580 (63.2%)  39 (29.1%)  19 (21.1%)\nsummary(unmatched(tutoring.matched.2to1))## 1000 (87.6%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 870 (94.8%)  97 (72.4%)  33 (36.7%)\nsummary(unmatched(tutoring.matched.3to2))## 928 (81.3%) of 1142 total data points were not matched.\n## Unmatched by treatment:\n##     Control      Treat1      Treat2 \n## 831 (90.5%)    75 (56%)  22 (24.4%)"},{"path":"effects-of-tutoring-on-course-grades.html","id":"checking-balance-2","chapter":"10 Effects of Tutoring on Course Grades","heading":"10.2 Checking Balance","text":"eventual strength propensity score methods dependent well balance achieved. (J. E. Helmreich Pruzek 2009) introduced graphical approaches evaluating balance. provide functions extend framework matching three groups. Figure 10.2 multiple covariate balance plot plots absolute effect size covariate adjustment. example, figure suggests reasonable balance achieved across covariates across three models since effect sizes smaller unadjusted cases relatively small.\nFigure 10.2: Multiple Covariate Balance Plot Absolute Standardized Effect Sizes Propensity Score Adjustment\nFigure 10.3 results balance.plot function. function provide bar chart categorical covariates box plots quantitative covariates, individually grid.\nFigure 10.3: Covariate Balance Plots\n","code":"\nprint(multibalance.plot(tutoring.tpsa) + ggtitle(\"Covariate Balance Plot\"))\nbplots <- balance.plot(tutoring.matched, tutoring[,all.vars(formu)], \n        legend.position=\"none\", x.axis.labels=c(\"C\",\"T1\",\"T1\"), x.axis.angle=0)\nprint(plot(bplots, cols=3, byrow=FALSE))## NULL"},{"path":"effects-of-tutoring-on-course-grades.html","id":"phase-ii-estimating-effects-of-tutoring-on-course-grades","chapter":"10 Effects of Tutoring on Course Grades","heading":"10.3 Phase II: Estimating Effects of Tutoring on Course Grades","text":"phase two propensity score analysis wish compare outcome interest, course grade example, across matches. custom merge function provided merge outcome original data frame results trimatch. merge function add three columns outcome three groups.Although merge function convenient conducting analysis, summary function perform common analyses including Friedman Rank Sum test repeated measures ANOVA. either tests produce p value less specified threshold (0.05 default), summary function also perform return Wilcoxon signed rank test three separate dependent sample t-tests see (Austin 2010) discussion dependent versus independent t-tests.print method accept multiple object returned summary combine single table output. Note parameter must named name used identify row containing results.\nFigure 10.4: Boxplot Differences\nAnother useful visualization presenting results Loess plot. Figure 10.5 plot propensity scores x-axis outcome (grade example) y-axis. Loess regression line overlaid.4 Since three propensity score scales, plot.loess3 function use propensity scores model predicting treatment one treatment two. Propensity scores control group imputed taking mean propensity scores two treatment units control matched . noted control unit matched two different sets treatment units, control unit two propensity scores. propensity score scale utilized can explicitly specified using model parameter.\nFigure 10.5: Loess Plot Caliper Matching\n","code":"\nmatched.out <- merge(tutoring.matched, tutoring$Grade)\nnames(matched.out)##  [1] \"Treat1\"      \"Treat2\"      \"Control\"     \"D.m3\"        \"D.m2\"       \n##  [6] \"D.m1\"        \"Dtotal\"      \"Treat1.out\"  \"Treat2.out\"  \"Control.out\"\nhead(matched.out)##   Treat1 Treat2 Control        D.m3        D.m2         D.m1     Dtotal\n## 1    368     39     331 0.007053754 0.001788577 1.039322e-02 0.01923555\n## 2    158    279     365 0.003373585 0.009530680 1.071188e-02 0.02361614\n## 3    899    209     100 0.001929173 0.013633300 9.183572e-03 0.02474604\n## 4    692    596    1055 0.023785086 0.010292177 1.862058e-03 0.03593932\n## 5    616    209     208 0.020203398 0.016562521 3.168865e-05 0.03679761\n## 6     28    852     154 0.007501996 0.014214100 1.776640e-02 0.03948249\n##   Treat1.out Treat2.out Control.out\n## 1          4          4           0\n## 2          4          4           4\n## 3          4          3           4\n## 4          4          3           4\n## 5          4          3           0\n## 6          4          4           2\ns1 <- summary(tutoring.matched, tutoring$Grade)\nnames(s1)## [1] \"PercentMatched\"       \"friedman.test\"        \"rmanova\"             \n## [4] \"pairwise.wilcox.test\" \"t.tests\"\ns1$friedman.test## \n##  Friedman rank sum test\n## \n## data:  Outcome and Treatment and ID\n## Friedman chi-squared = 27.904, df = 2, p-value = 8.726e-07\ns1$t.tests##               Treatments         t  df      p.value sig  mean.diff     ci.min\n## 1  Treat1.out-Treat2.out -2.791302 117 6.132601e-03  ** -0.3220339 -0.5505191\n## 2 Treat1.out-Control.out  3.817862 117 2.168534e-04 ***  0.7033898  0.3385190\n## 3 Treat2.out-Control.out  6.922623 117 2.539540e-10 ***  1.0254237  0.7320670\n##        ci.max\n## 1 -0.09354868\n## 2  1.06826071\n## 3  1.31878047\ns2 <- summary(tutoring.matched.caliper, tutoring$Grade)\ns3 <- summary(tutoring.matched.2to1, tutoring$Grade)\ns4 <- summary(tutoring.matched.3to2, tutoring$Grade)\n\nprint(\"Max Treat\"=s1, \"Caliper\"=s2, \"2-to-1\"=s3, \"3-to-2\"=s4)##      Method Friedman.chi2   Friedman.p     rmANOVA.F    rmANOVA.p    \n## 1 Max Treat      27.90354 8.726175e-07 ***  23.84506 3.758176e-10 ***\n## 2   Caliper     112.99646 2.904890e-25 *** 108.21057 2.374092e-45 ***\n## 3    2-to-1      18.95541 7.653924e-05 ***  15.54945 1.097932e-06 ***\n## 4    3-to-2      32.27083 9.828281e-08 ***  25.03567 1.538341e-10 ***\nboxdiff.plot(tutoring.matched, tutoring$Grade, \n             ordering=c(\"Treat2\",\"Treat1\",\"Control\")) + \n    ggtitle(\"Maximum Treatment Matching\")\nboxdiff.plot(tutoring.matched.caliper, tutoring$Grade, \n             ordering=c(\"Treat2\",\"Treat1\",\"Control\")) +\n    ggtitle(\"Caliper Matching\")\nboxdiff.plot(tutoring.matched.2to1, tutoring$Grade, \n             ordering=c(\"Treat2\",\"Treat1\",\"Control\")) +\n    ggtitle(\"2-to-1-to-n Matching\")\nloess3.plot(tutoring.matched.caliper, tutoring$Grade, ylab=\"Grade\", \n            points.alpha=.1, method=\"loess\")"},{"path":"effects-of-smoking-on-medical-expenditures.html","id":"effects-of-smoking-on-medical-expenditures","chapter":"11 Effects of Smoking on Medical Expenditures","heading":"11 Effects of Smoking on Medical Expenditures","text":"example5 utilize National Medical Expenditure Study (National Center Health Services Research 1987) estimate effects smoking medical expenditures. dataset first used (Johnson et al. 2003) estimate effects smoking diseases, effect diseases medical expenditures. (Imai Dyk 2004b) developed method generalize propensity score, called p-score, directly estimate effects smoking medical expenditures. specifically, defined quantitative treatment variable, pack year, defined :\\[packyear = \\frac{\\text{number cigarettes per day}}{20} \\times \\text{number years smoked}\\]approach designed match three separate groups continuous treatment. address two research questions: (1) effects smoking status (.e. never smoked, former smoker, current smoker) medical expenditures? (2) effects lifetime smoking medical expenditures? Figure 11.1 represent relationship two different treatments6. figure reveals several, perhaps counterintuitive, facts. First, unadjusted total medical expenditures former smokers higher current smokers. Secondly, distribution \\(log(packyear)\\) overlap substantial former current smokers. dichotomize pack year smoking variable, split median pack year, labeled moderate smokers (.e. \\(packyear \\le median(packyear)\\)) heavy smokers (.e. \\(packyear > median(packyear)\\)).(Johnson et al. 2003) (Imai Dyk 2004b) conducted complete-case analysis Johnson et al. reported multiple imputation substantially affect results.Since many participants zero medical expenditures, add one total expenditures log transforming variable. calculate median pack year create new treatment variable, smoke2, moderate heavy smokers non-smokers.\nFigure 11.1: Relationship Pack Year Total Expenditures Current Smoking Status\nImai van Dyk observed appeared relationship age medical expenditures. create new categorical age variable using quintiles use partial exact matching. serves two purposes, first ensures balance critical covariate (note also exactly match gender ethnicity) two, decrease search space matched triplets therefore increasing efficiency matching algorithm. possible disadvantage exact matching many treated units matched. examine unmatched treatment units .Define model estimate propensity scores.Estimate propensity scores two different treatments. Figure 11.2 provides triangle plots models.\nFigure 11.2: Triangle Plots NMES\nCreate two sets matched triplets two treatments.following summary unmatched rows show 96% treatment units matched models.Figure 11.3 multiple covariate balance plot two treatments. shows absolute effect sizes adjustment better covariates. demo included TriMatch package provides functions create individual balance plots covaraite.\nFigure 11.3: Multiple Covariate Balance Plots NMES\n","code":"\ndata(nmes)\nnmes <- subset(nmes, select = c(packyears, smoke, LASTAGE, MALE, RACE3, beltuse, educate, marital, SREGION, POVSTALB, HSQACCWT, TOTALEXP))\nnmes <- na.omit(nmes)\nnmes$smoke <- factor(nmes$smoke, levels=c(0,1,2), labels=c(\"Never\",\"Smoker\",\"Former\"))\nnmes$LogTotalExp <- log(nmes$TOTALEXP + 1)\n(medPY <- median(nmes[nmes$smoke != \"Never\",]$packyears))## 17.00\ntable(nmes$smoke, nmes$packyears > medPY)##         \n##          FALSE TRUE\n##   Never   9802    0\n##   Smoker  2571 2901\n##   Former  2209 1869\nnmes$smoke2 <- ifelse(nmes$smoke == \"Never\", \"Never\", \nifelse(nmes$packyears > 17, \"Heavy\", \"Moderate\"))\ntable(nmes$smoke, nmes$smoke2, useNA=\"ifany\")##         \n##          Heavy Moderate Never\n##   Never      0        0  9802\n##   Smoker  2901     2571     0\n##   Former  1869     2209     0\nggplot(nmes[nmes$smoke != \"Never\",], aes(x=log(packyears+1), color=smoke, fill=smoke)) +\n        geom_density(alpha=.1) + \n        theme(legend.position=\"none\", plot.margin=rep(unit(0, \"cm\"), 4)) +\n        xlab(\"\") + ylab(\"Density\")\nggplot(nmes[nmes$smoke != \"Never\",], aes(x=log(packyears+1), y=LogTotalExp, color=smoke, fill=smoke)) + \n        geom_point(alpha=.2) + \n        geom_smooth(method=\"loess\", formula = y ~ x) +\n        scale_color_hue(\"\") + scale_fill_hue(\"\") +\n        theme(legend.position=c(.9,1), plot.margin=rep(unit(0, \"cm\"), 4)) + \n        xlab(\"log(Pack Year)\") + ylab(\"log(Total Expenditures)\")\nnmes$LastAge5 <- cut(nmes$LASTAGE, \nbreaks=quantile(nmes$LASTAGE, probs=seq(0,1,1/5)),\ninclude.lowest=TRUE, orderd_result=TRUE)\nformu <- ~ LASTAGE + MALE + RACE3 + beltuse + educate + marital + \nSREGION + POVSTALB\ntpsa.smoke <- trips(nmes, nmes$smoke, formu)\ntpsa.packyears <- trips(nmes, nmes$smoke2, formu)\np.smoke <- plot(tpsa.smoke, sample=c(.05), edge.alpha=.1) + ggtitle(\"Treatment Variable: Current Smoking Status\")\np.packyears <- plot(tpsa.packyears, sample=c(.05), edge.alpha=.1) + ggtitle(\"Treatment Variable: Lifetime Smoking Frequency\")\np.smoke\np.packyears\ntmatch.smoke <- trimatch(tpsa.smoke, \nexact=nmes[,c(\"LastAge5\",\"MALE\",\"RACE3\")])\ntmatch.packyears <- trimatch(tpsa.packyears, \nexact=nmes[,c(\"LastAge5\",\"MALE\",\"RACE3\")])\nsummary(unmatched(tmatch.smoke))## 7048 (36.4%) of 19352 total data points were not matched.\n## Unmatched by treatment:\n##        Never       Smoker       Former \n## 6804 (69.4%)   142 (2.6%)   102 (2.5%)\nsummary(unmatched(tmatch.packyears))## 7532 (38.9%) of 19352 total data points were not matched.\n## Unmatched by treatment:\n##        Heavy     Moderate        Never \n##  181 (3.79%)  323 (6.76%) 7028 (71.7%)\np.smoke <- multibalance.plot(tpsa.smoke) + ggtitle(\"Treatment Variable: Current Smoking Status\")\np.packyears <- multibalance.plot(tpsa.packyears) + ggtitle(\"Treatment Variable: Lifetime Smoking Frequency\")\np.smoke\np.packyears"},{"path":"effects-of-smoking-on-medical-expenditures.html","id":"phase-ii-estimating-effects-of-smoking-on-medical-expenditures","chapter":"11 Effects of Smoking on Medical Expenditures","heading":"11.1 Phase II: Estimating Effects of Smoking on Medical Expenditures","text":"treatment regimes used maximumTreat method finding matched triplets retain treatment unit possibility using treatment units twice cases treatment unit otherwise matched. Friedman Rank Sum Test repeated measures ANOVA indicate statistically significant difference treatment regimes. Figure 11.4 provides box plots differences two treatment regimes. current smoking status treatment, results indicate smoker’s actually spend less former non-smokers. However, (Imai Dyk 2004b) explain, sample smokers includes survivors considered interpreting results.Imai van Dyk’s analysis used pack year treatment indicator. dichotomizing pack year moderate heavy smokers closely adheres approach. results treatment regime indicate smokers, moderate heavy, higher medical expenditures non-smokers. However, statistically significant difference heavy moderate smokers medical expenditures.\nFigure 11.4: Boxplot Differences NMES\n","code":"\nboxdiff.plot(tmatch.smoke, nmes$LogTotalExp, ordering=c(\"Smoker\",\"Former\",\"Never\")) + \n    ggtitle(\"Treatment Variable: Current Smoking Status\")\nboxdiff.plot(tmatch.packyears, nmes$LogTotalExp, ordering=c(\"Heavy\",\"Moderate\",\"Never\")) +\n    ggtitle(\"Treatment Variable: Lifetime Smoking Frequency\")\nsum.smoke <- summary(tmatch.smoke, nmes$LogTotalExp, \nordering=c(\"Smoker\",\"Former\",\"Never\"))\nsum.packyears <- summary(tmatch.packyears, nmes$LogTotalExp, \nordering=c(\"Heavy\",\"Moderate\",\"Never\"))\nprint(\"Current Smoking Status\" = sum.smoke, \"Smoking Frequency\" = sum.packyears)##                   Method Friedman.chi2   Friedman.p     rmANOVA.F    rmANOVA.p\n## 1 Current Smoking Status      88.92973 4.888274e-20 ***  74.75344 4.985672e-33\n## 2      Smoking Frequency      33.73246 4.732484e-08 ***  13.72351 1.110186e-06\n##      \n## 1 ***\n## 2 ***\nsum.smoke$t.tests##              Treatments          t   df      p.value sig  mean.diff     ci.min\n## 1 Smoker.out-Former.out -11.606558 7407 7.053664e-31 *** -0.4777939 -0.5584907\n## 2  Smoker.out-Never.out  -2.644559 7407 8.196991e-03  ** -0.1114564 -0.1940737\n## 3  Former.out-Never.out   9.321687 7407 1.483073e-20 ***  0.3663375  0.2892994\n##        ci.max\n## 1 -0.39709714\n## 2 -0.02883914\n## 3  0.44337570\nsum.packyears$t.test##               Treatments          t   df      p.value sig   mean.diff\n## 1 Heavy.out-Moderate.out -0.3063659 7507 7.593345e-01     -0.01251173\n## 2    Heavy.out-Never.out  4.3264245 7507 1.535124e-05 ***  0.17925216\n## 3 Moderate.out-Never.out  4.7239231 7507 2.355422e-06 ***  0.19176389\n##        ci.min     ci.max\n## 1 -0.09256793 0.06754447\n## 2  0.09803395 0.26047036\n## 3  0.11218788 0.27133989"},{"path":"chapter-multilevelpsa.html","id":"chapter-multilevelpsa","chapter":"12 Multilevel PSA","heading":"12 Multilevel PSA","text":"","code":""},{"path":"chapter-multilevelpsa.html","id":"introduction-1","chapter":"12 Multilevel PSA","heading":"12.1 Introduction","text":"Given large amount data summarized, use graphics integral component representing results. Pruzek Helmreich (2009) introduced class graphics visualizing dependent sample tests (see also Pruzek & Helmreich, 2010; Danielak, Pruzek, Doane, Helmreich, & Bryer, 2011). framework extended propensity score methods using stratification (Helmreich & Pruzek, 2009). particular, representation confidence intervals relative unit line (.e. line y=x) provided new way determining whether statistically significant diﬀerence two groups. multilevelPSA package provides number graphing functions extend frameworks multilevel PSA. figure represents multilevel PSA assessment plot annotations. graphic represents results comparing private public schools North America using Programme International Student Assessment (PISA; Organisation Economic Co-Operation Development, 2009). PISA data create graphic included multilevelPSA package detailed description create graphic discussed next section. Additionally, use PISA makes visible certain features graphics used. discussed chapters four five, diﬀerences charter traditional public schools minimal therefore features figures less apparent. following section focuses features graphic.figure , x-axis corresponds math scores private schools y-axis corresponds public school maths cores. colored circle () country size corresponding number students sampled within country. country projected lower left, parallel unit line, tick mark placed line slope -1 (b). tick marks represent distribution diﬀerences private public schools across countries. Diﬀerences aggregated (weighted size) across countries. math, overall adjusted mean private schools 487, overall adjusted mean public schools 459 represented horizontal (c) vertical (d) blue lines, respectively. dashed blue line parallel unit line (e) corresponds overall adjusted mean diﬀerence likewise, dashed green lines (f) correspond confidence interval. Lastly, rug plots along right top edges graphic (g) correspond distribution country’s overall mean private public school math scores, respectively.figure represents large amount data provides insight data results. figure provides overall results present traditional table, instance fact green dashed lines span unit line (.e. y = x) indicates statistically significant diﬀerence two groups. However additional information diﬃcult convey tabular format. example, rug plots indicate spread performance private public schools across countries large. Also observe Canada, largest PISA scores groups, also largest diﬀerence (favor private schools) represented larger distance unit line.\nFigure 12.1: Annotated multilevel PSA assessment plot. plot compares private schools (x- axis) public schools (y-axis) North America Programme International Student Assessment.\n","code":""},{"path":"chapter-multilevelpsa.html","id":"working-example","chapter":"12 Multilevel PSA","heading":"12.2 Working Example","text":"multilevelPSA package includes North American data Programme International Student Assessment (PISA; Organisation Economic Co-Operation Development, 2009). data made freely available research utilized R code reproducible9. example compares performance private public schools clustered country. Note PISA provide five plausible values academic scores since students complete subset total assessment. simplicity, math score used analysis average five plausible scores.mlpsa.ctree function performs phase propensity score analysis using classification trees, specifically using ctree function party package. getStrata function returns data frame number rows equivalent original data frame indicating stratum student.Similarly, mlpsa.logistic estimates propensity scores using logistic regression. getPropensityScores function returns data frame number rows equivalent original data frame.covariate.balance function calculates balance statistics covariate estimating eﬀect covariate adjustment. results can converted data frame view numeric results plot function provides balance plot. figure depicts eﬀect size covariate (blue triangle) (red circle) propensity score adjustment. shown , eﬀect size nearly covariates smaller unadjusted eﬀect size. exceptions covariates unadjusted eﬀect size already small. established threshold considered suﬃciently small eﬀect size. general, recommend adjusted eﬀect sizes less 0.1 reflect less 1% variance explained.\nFigure 12.2: Multilevel PSA balance plot PISA. eﬀect sizes (standardized mean diﬀerences) covariate provided PSA adjustment (blue triangles) PSA adjustment (red circles).\nmlpsa function performs phase II propensity score analysis requires four parameters: response variable, treatment indicator, stratum, clustering indicator. minN parameter (defaults five) indicates minimum stratum size included analysis. example, 463, less one percent students removed stratum (leaf node classification trees) contain least five students treatment control groups.summary function provides overall treatment estimates well level one two summaries.plot function creates multilevel assessment plot. depicted side panels showing distribution math scores strata public school students left private school students . panels can plotted separately using mlpsa.circ.plot mlpsa.distribution.plot functions.\nFigure 12.3: Multilevel PSA assessment plot PISA. main panel provides adjusted mean private (x-axis) public (y-axis) country. left lower panels provide mean stratum public private students, respectively. overall adjusted mean diﬀerence represented dashed blue line 95% confidence interval dashed green lines. statistically significant diﬀerence private public school student performance evidenced confidence interval spanning zero (.e. crossing unit line y=x.\nLastly, mlpsa.difference.plot function plots overall diﬀerences. sd parameter optional, specified, x-axis can interpreted standardized eﬀect sizes.\nFigure 12.4: Multilevel PSA diﬀerence plot PISA. blue dot corresponds eﬀect size (standardized mean diﬀerence) country. vertical blue line corresponds overall eﬀect size countries. green lines correspond 95% confidence intervals. dashed green lines Bonferroni-Sidak (c.f. Abdi, 2007) adjusted confidence intervals. size dot proportional sample size within country.\n","code":"\nlibrary(multilevelPSA)\nlibrary(party)\ndata(pisana)\ndata(pisa.psa.cols)\npisana$MathScore <- apply(pisana[,paste0('PV', 1:5, 'MATH')], 1, sum) / 5\nmlpsa <- mlpsa.ctree(pisana[,c('CNT', 'PUBPRIV', pisa.psa.cols)], \n                     formula = PUBPRIV ~ ., \n                     level2 = 'CNT')\nmlpsa.df <- getStrata(mlpsa, pisana, level2 = 'CNT')\nmlpsa.lr <- mlpsa.logistic(pisana[,c('CNT', 'PUBPRIV', pisa.psa.cols)], \n                           formula = PUBPRIV ~ ., \n                           level2 = 'CNT')\nmlpsa.lr.df <- getPropensityScores(mlpsa.lr, nStrata = 5)\nhead(mlpsa.lr.df)##   level2        ps strata\n## 1    CAN 0.9171885      2\n## 2    CAN 0.9410543      3\n## 3    CAN 0.9694831      4\n## 4    CAN 0.9300448      2\n## 5    CAN 0.8362229      1\n## 6    CAN 0.9734376      4\ncv.bal <- covariate.balance(covariates = pisana[,pisa.psa.cols],\n                            treatment = pisana$PUBPRIV,\n                            level2 = pisana$CNT,\n                            strata = mlpsa.df$strata)\nhead(as.data.frame(cv.bal))##                        covariate     es.adj    es.adj.wtd   es.unadj\n## 1                    (Intercept) 0.00000000  0.000000e+00        NaN\n## 2 ST05Q01Yes, more than one year 0.09720705  2.335610e-04 0.28695986\n## 3   ST05Q01Yes, one year or less 0.05131164  6.622276e-05 0.22032056\n## 4               ST07Q01Yes, once 0.10349619 -1.123270e-03 0.23453956\n## 5      ST07Q01Yes, twice or more 0.04447145 -3.720711e-04 0.08655983\n## 6         ST10Q01<ISCED level 2> 0.02969316 -1.733254e-04 0.17085288\nplot(cv.bal)\nresults.psa.math <- mlpsa(response = mlpsa.df$MathScore,\n                          treatment = mlpsa.df$PUBPRIV,\n                          strata = mlpsa.df$strata,\n                          level2 = mlpsa.df$CNT)\nsummary(results.psa.math)##    level2  strata    Treat Treat.n  Control Control.n    ci.min     ci.max\n## 1     CAN Overall 578.6262    1625 512.7997     21093 -72.08031 -59.572751\n## 2    <NA>       1 580.0069      28 491.6351      1128        NA         NA\n## 3    <NA>       2 599.9024       9 476.2717      1326        NA         NA\n## 4    <NA>       3 585.2667      11 512.7478       630        NA         NA\n## 5    <NA>       4 570.6395     140 508.2071      2240        NA         NA\n## 6    <NA>       5 578.2330       8 470.1353       179        NA         NA\n## 7    <NA>       6 499.9746      19 447.4332       310        NA         NA\n## 8    <NA>       7 584.4352      83 503.0039      3276        NA         NA\n## 9    <NA>       8 471.0636       5 464.1163       120        NA         NA\n## 10   <NA>       9 559.6253      41 525.9859       190        NA         NA\n## 11   <NA>      10 501.7639      20 463.2017        91        NA         NA\n## 12   <NA>      11 556.9058      44 516.6588       750        NA         NA\n## 13   <NA>      12 559.2598      34 520.6171       292        NA         NA\n## 14   <NA>      13 561.9538       8 489.0613       475        NA         NA\n## 15   <NA>      14 533.6619      21 463.1110       151        NA         NA\n## 16   <NA>      15 584.6585     126 519.8063      2134        NA         NA\n## 17   <NA>      16 565.9650      25 532.7037       245        NA         NA\n## 18   <NA>      17 613.4358      49 576.0543       137        NA         NA\n## 19   <NA>      18 563.2071      57 526.8732       659        NA         NA\n## 20   <NA>      19 598.0457     113 541.9625       318        NA         NA\n## 21   <NA>      20 629.1768      15 561.7597       143        NA         NA\n## 22   <NA>      21 588.7047      46 522.5698       398        NA         NA\n## 23   <NA>      22 594.4176      99 548.0534       194        NA         NA\n## 24   <NA>      23 579.9481      40 542.2590       183        NA         NA\n## 25   <NA>      24 581.4634      52 539.2326       342        NA         NA\n## 26   <NA>      25 582.1617     103 525.7403      1219        NA         NA\n## 27   <NA>      26 625.7713      11 525.2688       113        NA         NA\n## 28   <NA>      27 589.4497      35 516.2204       804        NA         NA\n## 29   <NA>      28 524.1864       5 482.3364        15        NA         NA\n## 30   <NA>      29 551.1838      12 526.8786       348        NA         NA\n## 31   <NA>      30 588.0390     145 526.4254      1195        NA         NA\n## 32   <NA>      31 602.9849     147 545.2503       822        NA         NA\n## 33   <NA>      32 528.0377      27 528.4283         7        NA         NA\n## 34   <NA>      33 552.0727      47 510.3964       659        NA         NA\n## 35    MEX Overall 429.5247    4044 422.9746     34090 -10.04346  -3.056743\n## 36   <NA>       1 516.4931      83 485.4115        13        NA         NA\n## 37   <NA>       2 491.4609     145 447.6702        89        NA         NA\n## 38   <NA>       3 494.1497     151 475.4165       178        NA         NA\n## 39   <NA>       4 417.9966      14 415.7178       154        NA         NA\n## 40   <NA>       5 453.8781     127 438.1276       484        NA         NA\n## 41   <NA>       6 452.5168      58 431.0051       635        NA         NA\n## 42   <NA>       7 496.4715     247 486.7400       293        NA         NA\n## 43   <NA>       8 483.2088     431 461.4194       871        NA         NA\n## 44   <NA>       9 472.3820       6 466.6849       110        NA         NA\n## 45   <NA>      10 460.6080      16 449.7257       121        NA         NA\n## 46   <NA>      11 481.4797     285 470.4227       696        NA         NA\n## 47   <NA>      12 474.0785      16 442.7985       112        NA         NA\n## 48   <NA>      13 405.3955      33 413.0509       943        NA         NA\n## 49   <NA>      14 431.6230     138 429.0045      1484        NA         NA\n## 50   <NA>      15 472.3767      99 459.7159       619        NA         NA\n## 51   <NA>      16 445.3934      78 437.8734       898        NA         NA\n## 52   <NA>      17 460.4928      34 461.1886       262        NA         NA\n## 53   <NA>      18 460.1316     113 454.8656       367        NA         NA\n## 54   <NA>      19 433.8334      53 444.9685       454        NA         NA\n## 55   <NA>      20 457.9868      69 445.7740       367        NA         NA\n## 56   <NA>      21 461.7578      76 457.0506       217        NA         NA\n## 57   <NA>      22 477.1849      93 452.3768       150        NA         NA\n## 58   <NA>      23 475.5367     186 459.6453       547        NA         NA\n## 59   <NA>      24 476.8898      10 437.3241       130        NA         NA\n## 60   <NA>      25 475.5922      80 441.7604       159        NA         NA\n## 61   <NA>      26 436.7231     167 427.7595      1040        NA         NA\n## 62   <NA>      27 436.3677     146 434.9281      1175        NA         NA\n## 63   <NA>      28 441.9648      45 428.9776       406        NA         NA\n## 64   <NA>      29 424.4490      80 428.0604      1963        NA         NA\n## 65   <NA>      30 436.0249      61 426.6096       787        NA         NA\n## 66   <NA>      31 426.2893      48 427.6944       645        NA         NA\n## 67   <NA>      32 400.6292     231 409.2472      4314        NA         NA\n## 68   <NA>      33 442.1996      28 437.6917       279        NA         NA\n## 69   <NA>      34 423.5202      34 426.4274      1013        NA         NA\n## 70   <NA>      35 419.9079      63 427.3862      2364        NA         NA\n## 71   <NA>      36 408.3097      18 400.4286       234        NA         NA\n## 72   <NA>      37 406.4552     107 397.2436      3632        NA         NA\n## 73   <NA>      38 424.5754      25 411.6970      2434        NA         NA\n## 74   <NA>      39 448.6755      15 431.5050       342        NA         NA\n## 75   <NA>      40 362.4143       7 348.4556      1959        NA         NA\n## 76   <NA>      41 381.2044      15 346.2349      1004        NA         NA\n## 77   <NA>      42 510.2599     313 487.5493       146        NA         NA\n## 78    USA Overall 505.2189     345 484.8212      4888 -32.03916  -8.756334\n## 79   <NA>       1 479.2228      50 475.0463      1219        NA         NA\n## 80   <NA>       2 490.3678      16 447.5507      1323        NA         NA\n## 81   <NA>       3 501.7149      34 487.6484       462        NA         NA\n## 82   <NA>       4 528.1903      42 506.7805       263        NA         NA\n## 83   <NA>       5 511.8919      42 502.8831       335        NA         NA\n## 84   <NA>       6 519.6260      21 476.6840       526        NA         NA\n## 85   <NA>       7 565.4985      80 558.6464       278        NA         NA\n## 86   <NA>       8 560.4173      41 541.0285       207        NA         NA\n## 87   <NA>       9 511.0133      14 528.7377       259        NA         NA\n## 88   <NA>      10 522.3160       5 460.6910        16        NA         NA\nplot(results.psa.math)\nmlpsa.difference.plot(results.psa.math, \n                      sd = mean(mlpsa.df$MathScore, na.rm=TRUE))"},{"path":"appendix-shiny.html","id":"appendix-shiny","chapter":"A Shiny Applications","heading":"A Shiny Applications","text":"psa R package includes Shiny applications designed conduct explore propensity score analysis.","code":""},{"path":"appendix-shiny.html","id":"propensity-score-analysis-shiny-application","chapter":"A Shiny Applications","heading":"A.1 Propensity Score Analysis Shiny Application","text":"main PSA Shiny application allows conduct phases PSA. application contains several datasets discussed section 1.5 can also upload dataset.\nFigure .1: PSA Shiny Application\n","code":"\npsa::psa_shiny()"},{"path":"appendix-shiny.html","id":"propensity-score-analysis-simulation-shiny-application","chapter":"A Shiny Applications","heading":"A.2 Propensity Score Analysis Simulation Shiny Application","text":"PSA Simulation Shiny application designed explore figures introduced chapter 1 stratification, matching, weighting. Using data simulation procedure used chapter, can control number parameters including treatment effect explore visualizations change can use make decisions treatment effects.\nFigure .2: PSA Simulation Shiny Application\n","code":"\npsa::psa_simulation_shiny()"},{"path":"appendix-psranges.html","id":"appendix-psranges","chapter":"B Propensity Score Ranges","heading":"B Propensity Score Ranges","text":"regard propensity score ranges, range tends shrink ratio treatment--control increases. Figure 20 depicts range distribution propensity scores (using logistic regression) varying treatment--control ratios. data used create figure simulated available Appendix K. psrange plot.psrange functions included multilevelPSA R package. Propensity scores estimated single covariate mean treatment control 0.6 0.4, respectively. standard deviation 0.4. 100 treatment units 1,000 control units simulated. goal choosing means standard deviations separation treatment control. row figure represents percentage control units sampled estimating propensity scores, starting 100% (.e. 1,000 control units) 10% (100 control units). figure shows, ratio decreases equal treatment control units, range propensity scores becomes normal. calculate ranges, sampling step bootstrapped green bar black points represent 20 bootstrap samples taken. bars represent mean minimum mean maximum step.“shrinking” propensity score ranges ratio treatment--control increases implications interpretation propensity scores. Typically, propensity scores interpreted probability treatment. studies number treatment control units roughly equal, interpretation valid. However, cases ratio treatment--control large, best simply interpret propensity scores adjustment scores probabilities. Since matching stratification procedures utilize standard scores (.e. propensity score divided standard deviation propensity scores), impact interpretation propensity scores impact estimated treatment e↵ects. appears issue explored either PSA logistic regression literature additional exploration topic appears warranted.1:10 (100 treatments, 1000 control units)1:20 (100 treatments, 2000 control units)100 treatments, 1000 control units, equal means standard deviations100 treatments, 1000 control units, little overlap100 treat, 1000 control, 10 covariates","code":"\nlibrary(multilevelPSA)\ngetSimulatedData <- function(nvars = 3, ntreat = 100, treat.mean = 0.6, treat.sd = 0.5, \n    ncontrol = 1000, control.mean = 0.4, control.sd = 0.5) {\n    if (length(treat.mean) == 1) {\n        treat.mean = rep(treat.mean, nvars)\n    }\n    if (length(treat.sd) == 1) {\n        treat.sd = rep(treat.sd, nvars)\n    }\n    if (length(control.mean) == 1) {\n        control.mean = rep(control.mean, nvars)\n    }\n    if (length(control.sd) == 1) {\n        control.sd = rep(control.sd, nvars)\n    }\n    \n    df <- c(rep(0, ncontrol), rep(1, ntreat))\n    for (i in 1:nvars) {\n        df <- cbind(df, c(rnorm(ncontrol, mean = control.mean[1], sd = control.sd[1]), \n            rnorm(ntreat, mean = treat.mean[1], sd = treat.sd[1])))\n    }\n    df <- as.data.frame(df)\n    names(df) <- c(\"treat\", letters[1:nvars])\n    return(df)\n}\ntest.df1 <- getSimulatedData(ntreat = 100, ncontrol = 1000)\npsranges1 <- psrange(test.df1, test.df1$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges1)\nsummary(psranges1)##       p ntreat ncontrol ratio   min.mean       min.sd min.median       min.se\n## 1    10    100      100     1 0.15158085 0.0399247330 0.16675914 0.0089274417\n## 21   20    100      200     2 0.08065862 0.0110939596 0.07987970 0.0024806848\n## 41   30    100      300     3 0.05825915 0.0089598303 0.06117074 0.0020034790\n## 61   40    100      400     4 0.03966169 0.0043536309 0.03993927 0.0009735015\n## 81   50    100      500     5 0.03028191 0.0038363450 0.02969747 0.0008578328\n## 101  60    100      600     6 0.02581532 0.0024130983 0.02594087 0.0005395852\n## 121  70    100      700     7 0.02203176 0.0019850490 0.02179919 0.0004438705\n## 141  80    100      800     8 0.01903728 0.0013822504 0.01864631 0.0003090806\n## 161  90    100      900     9 0.01631016 0.0005450121 0.01627551 0.0001218684\n## 181 100    100     1000    10 0.01453930 0.0000000000 0.01453930 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.06536710 0.19336279 0.8598281 0.04676196  0.8501328 0.010456291 0.7861258\n## 21  0.05791970 0.10233899 0.7270197 0.04065439  0.7303578 0.009090597 0.6439753\n## 41  0.03669016 0.07242601 0.6050233 0.05144961  0.5948197 0.011504482 0.5386225\n## 61  0.03200825 0.05010319 0.5606233 0.02683686  0.5606224 0.006000904 0.5089432\n## 81  0.02357618 0.03609465 0.5058454 0.02833751  0.4987880 0.006336460 0.4682137\n## 101 0.02033462 0.02954242 0.4593096 0.02170712  0.4544798 0.004853860 0.4236357\n## 121 0.01906877 0.02678785 0.4255822 0.02072414  0.4292869 0.004634059 0.3811220\n## 141 0.01719583 0.02208829 0.3928887 0.01439171  0.3941605 0.003218084 0.3591961\n## 161 0.01526377 0.01767972 0.3577524 0.01054775  0.3544386 0.002358548 0.3458710\n## 181 0.01453930 0.01453930 0.3306953 0.00000000  0.3306953 0.000000000 0.3306953\n##       max.max\n## 1   0.9361766\n## 21  0.8123514\n## 41  0.7304035\n## 61  0.6127437\n## 81  0.5618276\n## 101 0.5129718\n## 121 0.4548082\n## 141 0.4260502\n## 161 0.3798020\n## 181 0.3306953\ntest.df2 <- getSimulatedData(ncontrol = 2000)\npsranges2 <- psrange(test.df2, test.df2$treat, treat ~ ., samples = seq(100, \n    2000, by = 100), nboot = 20)\nplot(psranges2)\nsummary(psranges2)##       p ntreat ncontrol ratio    min.mean       min.sd  min.median       min.se\n## 1     5    100      100     1 0.108479153 3.478953e-02 0.113600581 7.779176e-03\n## 21   10    100      200     2 0.046371411 1.434092e-02 0.044963287 3.206727e-03\n## 41   15    100      300     3 0.031643058 8.312285e-03 0.030554662 1.858683e-03\n## 61   20    100      400     4 0.017812919 3.780810e-03 0.017660947 8.454147e-04\n## 81   25    100      500     5 0.014711230 2.496677e-03 0.015022455 5.582738e-04\n## 101  30    100      600     6 0.010719571 2.578118e-03 0.009863200 5.764847e-04\n## 121  35    100      700     7 0.009514228 1.588785e-03 0.009334077 3.552632e-04\n## 141  40    100      800     8 0.007691382 1.171925e-03 0.007477029 2.620504e-04\n## 161  45    100      900     9 0.006889727 1.222477e-03 0.006726188 2.733541e-04\n## 181  50    100     1000    10 0.006276889 5.128093e-04 0.006246931 1.146676e-04\n## 201  55    100     1100    11 0.005318209 5.985626e-04 0.005257637 1.338427e-04\n## 221  60    100     1200    12 0.004854049 3.641814e-04 0.004806155 8.143343e-05\n## 241  65    100     1300    13 0.004545931 3.242687e-04 0.004520731 7.250869e-05\n## 261  70    100     1400    14 0.004114851 2.213926e-04 0.004099411 4.950490e-05\n## 281  75    100     1500    15 0.003827730 3.163611e-04 0.003794601 7.074050e-05\n## 301  80    100     1600    16 0.003565950 1.937171e-04 0.003520204 4.331645e-05\n## 321  85    100     1700    17 0.003321481 1.553915e-04 0.003348171 3.474661e-05\n## 341  90    100     1800    18 0.003153977 7.759093e-05 0.003159270 1.734986e-05\n## 361  95    100     1900    19 0.002936686 5.616000e-05 0.002935480 1.255776e-05\n## 381 100    100     2000    20 0.002768276 0.000000e+00 0.002768276 0.000000e+00\n##         min.min     min.max  max.mean      max.sd max.median      max.se\n## 1   0.058393439 0.170711051 0.8819158 0.032569683  0.8876059 0.007282803\n## 21  0.028012247 0.080533570 0.8039199 0.038136538  0.8047723 0.008527589\n## 41  0.019685674 0.053076019 0.7255648 0.038281827  0.7324588 0.008560077\n## 61  0.012532975 0.025846915 0.6973759 0.039330036  0.6887384 0.008794463\n## 81  0.009046485 0.020133140 0.6381940 0.032865186  0.6328090 0.007348879\n## 101 0.008257497 0.019303295 0.6197714 0.029943565  0.6216856 0.006695585\n## 121 0.006840465 0.013738017 0.5788401 0.029811196  0.5822845 0.006665986\n## 141 0.005879615 0.011053336 0.5589409 0.043400845  0.5554842 0.009704724\n## 161 0.005364998 0.011462546 0.5217990 0.027818443  0.5244945 0.006220393\n## 181 0.005345154 0.007100721 0.4896128 0.022943577  0.4909629 0.005130340\n## 201 0.004317742 0.006380071 0.4837967 0.023518184  0.4848799 0.005258826\n## 221 0.004328371 0.005446223 0.4630265 0.022398047  0.4599904 0.005008355\n## 241 0.004046610 0.005226542 0.4414052 0.023000235  0.4404226 0.005143009\n## 261 0.003657161 0.004503838 0.4322617 0.021211389  0.4340543 0.004743011\n## 281 0.003223369 0.004460980 0.4089992 0.015626515  0.4098423 0.003494195\n## 301 0.003307291 0.003902424 0.3932721 0.015087435  0.3914948 0.003373653\n## 321 0.002975258 0.003588740 0.3851770 0.011768615  0.3860391 0.002631542\n## 341 0.003005001 0.003287277 0.3760762 0.009748364  0.3764867 0.002179801\n## 361 0.002838330 0.003063846 0.3659962 0.003577472  0.3653889 0.000799947\n## 381 0.002768276 0.002768276 0.3550118 0.000000000  0.3550118 0.000000000\n##       max.min   max.max\n## 1   0.8071012 0.9270047\n## 21  0.7323388 0.9063155\n## 41  0.6191583 0.7766993\n## 61  0.5878796 0.7694392\n## 81  0.5915755 0.6922287\n## 101 0.5586192 0.6855857\n## 121 0.5269503 0.6250264\n## 141 0.4901740 0.6524009\n## 161 0.4480063 0.5559810\n## 181 0.4391822 0.5360344\n## 201 0.4462260 0.5317380\n## 221 0.4234709 0.5015449\n## 241 0.3962534 0.4878237\n## 261 0.3966437 0.4709061\n## 281 0.3779369 0.4337704\n## 301 0.3746633 0.4216166\n## 321 0.3634942 0.4050216\n## 341 0.3531399 0.3906495\n## 361 0.3578767 0.3711270\n## 381 0.3550118 0.3550118\ntest.df3 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.5, control.mean = 0.5)\npsranges3 <- psrange(test.df3, test.df3$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges3)\nsummary(psranges3)##       p ntreat ncontrol ratio   min.mean      min.sd min.median       min.se\n## 1    10    100      100     1 0.28350601 0.064087988 0.28885238 0.0143305097\n## 21   20    100      200     2 0.16705507 0.031529985 0.16046048 0.0070503191\n## 41   30    100      300     3 0.12344740 0.019802371 0.12273928 0.0044279448\n## 61   40    100      400     4 0.09522531 0.011914719 0.09279009 0.0026642122\n## 81   50    100      500     5 0.07563025 0.008418365 0.07552338 0.0018824037\n## 101  60    100      600     6 0.06076604 0.008521491 0.06124068 0.0019054633\n## 121  70    100      700     7 0.05269255 0.005924987 0.05247310 0.0013248673\n## 141  80    100      800     8 0.04586865 0.002395134 0.04624844 0.0005355682\n## 161  90    100      900     9 0.04048884 0.002059943 0.04014488 0.0004606172\n## 181 100    100     1000    10 0.03634508 0.000000000 0.03634508 0.0000000000\n##        min.min    min.max  max.mean     max.sd max.median      max.se   max.min\n## 1   0.16192908 0.37939385 0.7132598 0.04361440  0.7129799 0.009752476 0.6394674\n## 21  0.10563283 0.22609654 0.5202766 0.04031786  0.5161472 0.009015347 0.4561985\n## 41  0.09125843 0.16864202 0.4143754 0.02875963  0.4181810 0.006430849 0.3529161\n## 61  0.07550229 0.12048280 0.3532632 0.02511123  0.3510016 0.005615041 0.3137257\n## 81  0.06256572 0.09642339 0.2941406 0.02138895  0.2973524 0.004782715 0.2463536\n## 101 0.05119536 0.08325665 0.2668781 0.01733506  0.2681488 0.003876236 0.2279083\n## 121 0.04216244 0.06796214 0.2363525 0.01411165  0.2348300 0.003155462 0.2141914\n## 141 0.04222852 0.05135097 0.2190724 0.01032624  0.2199282 0.002309018 0.2030472\n## 161 0.03701920 0.04386187 0.1978531 0.00701747  0.1986408 0.001569154 0.1863798\n## 181 0.03634508 0.03634508 0.1835126 0.00000000  0.1835126 0.000000000 0.1835126\n##       max.max\n## 1   0.7816167\n## 21  0.6045684\n## 41  0.4675561\n## 61  0.3965734\n## 81  0.3318606\n## 101 0.2927092\n## 121 0.2619002\n## 141 0.2443409\n## 161 0.2090405\n## 181 0.1835126\ntest.df4 <- getSimulatedData(ncontrol = 1000, treat.mean = 0.25, treat.sd = 0.3, \n    control.mean = 0.75, control.sd = 0.3)\npsranges4 <- psrange(test.df4, test.df4$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges4)\nsummary(psranges4)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 1.525384e-05 2.837735e-05 2.299369e-06\n## 21   20    100      200     2 2.386395e-06 4.263708e-06 6.796030e-07\n## 41   30    100      300     3 7.854093e-07 1.369501e-06 3.322104e-07\n## 61   40    100      400     4 1.139238e-07 9.950192e-08 9.888239e-08\n## 81   50    100      500     5 4.970589e-08 7.085309e-08 1.910629e-08\n## 101  60    100      600     6 9.456041e-08 8.805110e-08 6.876480e-08\n## 121  70    100      700     7 4.562272e-08 3.277532e-08 4.271103e-08\n## 141  80    100      800     8 4.834941e-08 1.732541e-08 4.683829e-08\n## 161  90    100      900     9 3.734849e-08 1.141374e-08 3.473544e-08\n## 181 100    100     1000    10 3.302817e-08 0.000000e+00 3.302817e-08\n##           min.se      min.min      min.max  max.mean       max.sd max.median\n## 1   6.345368e-06 8.242210e-10 1.189959e-04 0.9999919 1.177109e-05  0.9999975\n## 21  9.533941e-07 4.512240e-09 1.459366e-05 0.9999698 4.357035e-05  0.9999873\n## 41  3.062298e-07 9.339904e-09 5.821350e-06 0.9999734 2.437621e-05  0.9999837\n## 61  2.224931e-08 5.006341e-09 3.452425e-07 0.9999718 2.790507e-05  0.9999803\n## 81  1.584323e-08 1.753035e-09 3.004791e-07 0.9999730 3.183032e-05  0.9999857\n## 101 1.968882e-08 1.862385e-08 4.026231e-07 0.9999570 1.988995e-05  0.9999546\n## 121 7.328785e-09 1.690112e-09 1.093181e-07 0.9999481 3.208612e-05  0.9999523\n## 141 3.874079e-09 1.740209e-08 9.088383e-08 0.9999326 1.783491e-05  0.9999357\n## 161 2.552189e-09 1.706567e-08 5.651540e-08 0.9999274 1.839133e-05  0.9999295\n## 181 0.000000e+00 3.302817e-08 3.302817e-08 0.9999211 0.000000e+00  0.9999211\n##           max.se   max.min   max.max\n## 1   2.632095e-06 0.9999539 1.0000000\n## 21  9.742627e-06 0.9998202 0.9999998\n## 41  5.450686e-06 0.9999165 0.9999973\n## 61  6.239764e-06 0.9998793 0.9999976\n## 81  7.117475e-06 0.9998587 0.9999988\n## 101 4.447529e-06 0.9999103 0.9999906\n## 121 7.174675e-06 0.9998882 0.9999962\n## 141 3.988007e-06 0.9998969 0.9999700\n## 161 4.112427e-06 0.9998986 0.9999602\n## 181 0.000000e+00 0.9999211 0.9999211\ntest.df5 <- getSimulatedData(nvars = 10, ntreat = 100, ncontrol = 1000)\npsranges5 <- psrange(test.df5, test.df5$treat, treat ~ ., samples = seq(100, \n    1000, by = 100), nboot = 20)\nplot(psranges5)\nsummary(psranges5)##       p ntreat ncontrol ratio     min.mean       min.sd   min.median\n## 1    10    100      100     1 0.0101680007 5.571726e-03 0.0087507018\n## 21   20    100      200     2 0.0032786858 1.986855e-03 0.0031712876\n## 41   30    100      300     3 0.0016998723 8.777415e-04 0.0014178615\n## 61   40    100      400     4 0.0012710429 7.476010e-04 0.0010189156\n## 81   50    100      500     5 0.0009891488 3.613101e-04 0.0009233698\n## 101  60    100      600     6 0.0006954162 1.699134e-04 0.0006825701\n## 121  70    100      700     7 0.0005748031 1.098981e-04 0.0005927955\n## 141  80    100      800     8 0.0005241338 7.766948e-05 0.0005085729\n## 161  90    100      900     9 0.0004511794 4.957160e-05 0.0004584537\n## 181 100    100     1000    10 0.0004033341 0.000000e+00 0.0004033341\n##           min.se      min.min      min.max  max.mean      max.sd max.median\n## 1   1.245876e-03 0.0034055594 0.0218076912 0.9797889 0.007606976  0.9810769\n## 21  4.442742e-04 0.0010920758 0.0092490802 0.9667801 0.012485108  0.9700905\n## 41  1.962690e-04 0.0007561575 0.0045260935 0.9539731 0.013264618  0.9562029\n## 61  1.671687e-04 0.0006177711 0.0040260207 0.9394156 0.010526497  0.9424796\n## 81  8.079140e-05 0.0005352006 0.0019389389 0.9221930 0.014294291  0.9217427\n## 101 3.799379e-05 0.0002784764 0.0009768850 0.9113210 0.017183853  0.9104470\n## 121 2.457395e-05 0.0004073415 0.0007375938 0.9032366 0.010021744  0.9045313\n## 141 1.736742e-05 0.0003929492 0.0006895618 0.8848322 0.011660804  0.8843501\n## 161 1.108455e-05 0.0003240626 0.0005327837 0.8735074 0.008826874  0.8708373\n## 181 0.000000e+00 0.0004033341 0.0004033341 0.8639081 0.000000000  0.8639081\n##          max.se   max.min   max.max\n## 1   0.001700972 0.9612616 0.9912510\n## 21  0.002791755 0.9357162 0.9860199\n## 41  0.002966059 0.9106519 0.9720211\n## 61  0.002353796 0.9129030 0.9566536\n## 81  0.003196301 0.8929029 0.9531369\n## 101 0.003842426 0.8850449 0.9491578\n## 121 0.002240930 0.8856435 0.9235878\n## 141 0.002607435 0.8583591 0.9084171\n## 161 0.001973749 0.8641812 0.8983841\n## 181 0.000000000 0.8639081 0.8639081"},{"path":"appendix-psmodels.html","id":"appendix-psmodels","chapter":"C Methods for Estimating Propensity Scores","heading":"C Methods for Estimating Propensity Scores","text":"appendix provide R code multiple statistical models estimating propensity scores. examples use lalonde dataset following formula:","code":"\nlalonde.formu <- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2) +\n    u74 + u75"},{"path":"appendix-psmodels.html","id":"logistic-regression","chapter":"C Methods for Estimating Propensity Scores","heading":"C.1 Logistic Regression","text":"","code":"\nlr_out <- glm(lalonde.formu,\n              data = lalonde,\n              family = binomial(link = logit))\nlr_ps <- fitted(lr_out)"},{"path":"appendix-psmodels.html","id":"conditional-inference-trees-with-party-package","chapter":"C Methods for Estimating Propensity Scores","heading":"C.2 Conditional Inference Trees with party package","text":"","code":"\nlibrary(party)\nctree_out <- ctree(lalonde.formu,\n                   data = lalonde)\n# treeresponse(ctree_out)"},{"path":"appendix-psmodels.html","id":"recusrive-partitioning-with-rpart","chapter":"C Methods for Estimating Propensity Scores","heading":"C.3 Recusrive Partitioning with rpart","text":"","code":"\nlibrary(rpart)\nrpart_out <- rpart(lalonde.formu,\n                   data = lalonde,\n                   method = 'class')\n# For classification\nrpart_strata <- rpart_out$where\n# For matching or weighting\nrpart_ps <- predict(rpart_out, type = 'prob')[,1]"},{"path":"appendix-psmodels.html","id":"bayesian-logistic-regression","chapter":"C Methods for Estimating Propensity Scores","heading":"C.4 Bayesian Logistic Regression","text":"","code":"\nlibrary(rstanarm)\nstan_out <- stan_glm(lalonde.formu,\n                     data = lalonde)\nstan_ps <- predict(stan_out, type = 'response')"},{"path":"appendix-psmodels.html","id":"probit-bart-for-dichotomous-outcomes-with-normal-latents","chapter":"C Methods for Estimating Propensity Scores","heading":"C.5 Probit BART for dichotomous outcomes with Normal latents","text":"","code":"\nlibrary(BART)\nbart_out <- pbart(x.train = lalonde[,all.vars(lalonde.formu)[-1]],\n                  y.train = lalonde[,all.vars(lalonde.formu)[1]])\nbart_ps <- bart_out$prob.test.mean"},{"path":"appendix-psmodels.html","id":"random-forests","chapter":"C Methods for Estimating Propensity Scores","heading":"C.6 Random Forests","text":"","code":"\nlibrary(randomForest)\nrf_out <- randomForest(update.formula(lalonde.formu, factor(treat) ~ .),\n                       data = lalonde)\n# For classification\nrf_strata <- predict(rf_out, type = 'response')\n# For matching or weighting\nrf_ps <- predict(rf_out, type = 'prob')[,1,drop=TRUE]"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
