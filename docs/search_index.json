[
["index.html", "Applied Propensity Score Analysis with R Preface 0.1 Colophon", " Applied Propensity Score Analysis with R Jason Bryer Last updated January 19, 2019 Preface 0.1 Colophon This book was created using the bookdown R package (Xie 2016). The book is available online at http://jason.bryer.org/psa/ and the source is available at https://github.com/jbryer/psa. devtools::session_info() ## ─ Session info ────────────────────────────────────────────────────────── ## setting value ## version R version 3.5.2 (2018-12-20) ## os macOS Mojave 10.14.2 ## system x86_64, darwin15.6.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2019-01-19 ## ## ─ Packages ────────────────────────────────────────────────────────────── ## package * version date lib source ## abind 1.4-5 2016-07-21 [1] CRAN (R 3.5.0) ## assertthat 0.2.0 2017-04-11 [1] CRAN (R 3.5.0) ## backports 1.1.2 2017-12-13 [1] CRAN (R 3.5.0) ## bindr 0.1.1 2018-03-13 [1] CRAN (R 3.5.0) ## bindrcpp 0.2.2 2018-03-29 [1] CRAN (R 3.5.0) ## bookdown * 0.9 2018-12-21 [1] CRAN (R 3.5.0) ## broom 0.5.1 2018-12-05 [1] CRAN (R 3.5.0) ## callr 3.1.0 2018-12-10 [1] CRAN (R 3.5.0) ## car * 3.0-2 2018-08-23 [1] CRAN (R 3.5.0) ## carData * 3.0-2 2018-09-30 [1] CRAN (R 3.5.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.5.0) ## cli 1.0.1 2018-09-25 [1] CRAN (R 3.5.0) ## codetools 0.2-15 2016-10-05 [1] CRAN (R 3.5.0) ## coin 1.2-2 2017-11-28 [1] CRAN (R 3.5.0) ## colorspace 1.3-2 2016-12-14 [1] CRAN (R 3.5.0) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 3.5.0) ## curl 3.2 2018-03-28 [1] CRAN (R 3.5.0) ## data.table 1.11.8 2018-09-30 [1] CRAN (R 3.5.0) ## desc 1.2.0 2018-05-01 [1] CRAN (R 3.5.0) ## devtools 2.0.1 2018-10-26 [1] CRAN (R 3.5.2) ## digest 0.6.18 2018-10-10 [1] CRAN (R 3.5.0) ## dplyr 0.7.8 2018-11-10 [1] CRAN (R 3.5.0) ## evaluate 0.12 2018-10-09 [1] CRAN (R 3.5.0) ## ez * 4.4-0 2016-11-02 [1] CRAN (R 3.5.0) ## forcats 0.3.0 2018-02-19 [1] CRAN (R 3.5.0) ## foreign 0.8-71 2018-07-20 [1] CRAN (R 3.5.0) ## fs 1.2.6 2018-08-23 [1] CRAN (R 3.5.0) ## gdata * 2.18.0 2017-06-06 [1] CRAN (R 3.5.0) ## generics 0.0.2 2018-11-29 [1] CRAN (R 3.5.0) ## ggplot2 * 3.1.0 2018-10-25 [1] CRAN (R 3.5.0) ## ggthemes 4.0.1 2018-08-24 [1] CRAN (R 3.5.0) ## glue 1.3.0 2018-07-17 [1] CRAN (R 3.5.0) ## granova * 2.1 2014-08-25 [1] CRAN (R 3.5.0) ## granovaGG * 1.4.0 2015-12-18 [1] CRAN (R 3.5.0) ## gridExtra 2.3 2017-09-09 [1] CRAN (R 3.5.0) ## gtable 0.2.0 2016-02-26 [1] CRAN (R 3.5.0) ## gtools 3.8.1 2018-06-26 [1] CRAN (R 3.5.0) ## haven 2.0.0 2018-11-22 [1] CRAN (R 3.5.0) ## highr 0.7 2018-06-09 [1] CRAN (R 3.5.0) ## hms 0.4.2 2018-03-10 [1] CRAN (R 3.5.0) ## htmltools 0.3.6 2017-04-28 [1] CRAN (R 3.5.0) ## jomo 2.6-5 2018-11-09 [1] CRAN (R 3.5.0) ## knitr * 1.21 2018-12-10 [1] CRAN (R 3.5.1) ## labeling 0.3 2014-08-23 [1] CRAN (R 3.5.0) ## lattice * 0.20-38 2018-11-04 [1] CRAN (R 3.5.0) ## lazyeval 0.2.1 2017-10-29 [1] CRAN (R 3.5.0) ## lme4 1.1-19 2018-11-10 [1] CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 [1] CRAN (R 3.5.0) ## MASS * 7.3-51.1 2018-11-01 [1] CRAN (R 3.5.0) ## Matching * 4.9-3 2018-05-04 [1] CRAN (R 3.5.0) ## MatchIt * 3.0.2 2018-01-09 [1] CRAN (R 3.5.0) ## Matrix 1.2-15 2018-11-01 [1] CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 [1] CRAN (R 3.5.0) ## mgcv 1.8-26 2018-11-21 [1] CRAN (R 3.5.0) ## mice * 3.3.0 2018-07-27 [1] CRAN (R 3.5.0) ## minqa 1.2.4 2014-10-09 [1] CRAN (R 3.5.0) ## mitml 0.3-6 2018-07-10 [1] CRAN (R 3.5.0) ## mnormt 1.5-5 2016-10-15 [1] CRAN (R 3.5.0) ## modeltools 0.2-22 2018-07-16 [1] CRAN (R 3.5.0) ## multcomp 1.4-8 2017-11-08 [1] CRAN (R 3.5.0) ## multilevelPSA * 1.2.5 2018-03-22 [1] CRAN (R 3.5.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 3.5.0) ## mvtnorm 1.0-8 2018-05-31 [1] CRAN (R 3.5.0) ## nlme 3.1-137 2018-04-07 [1] CRAN (R 3.5.0) ## nloptr 1.2.1 2018-10-03 [1] CRAN (R 3.5.0) ## nnet 7.3-12 2016-02-02 [1] CRAN (R 3.5.0) ## openxlsx 4.1.0 2018-05-26 [1] CRAN (R 3.5.0) ## packrat 0.5.0 2018-11-14 [1] CRAN (R 3.5.0) ## pan 1.6 2018-06-29 [1] CRAN (R 3.5.0) ## party 1.3-1 2018-08-08 [1] CRAN (R 3.5.0) ## pillar 1.3.1 2018-12-15 [1] CRAN (R 3.5.0) ## pkgbuild 1.0.2 2018-10-16 [1] CRAN (R 3.5.0) ## pkgconfig 2.0.2 2018-08-16 [1] CRAN (R 3.5.0) ## pkgload 1.0.2 2018-10-29 [1] CRAN (R 3.5.0) ## plyr 1.8.4 2016-06-08 [1] CRAN (R 3.5.0) ## prettyunits 1.0.2 2015-07-13 [1] CRAN (R 3.5.0) ## processx 3.2.1 2018-12-05 [1] CRAN (R 3.5.0) ## ps 1.2.1 2018-11-06 [1] CRAN (R 3.5.0) ## PSAboot * 1.3.4 2018-03-31 [1] local (jbryer/PSAboot@NA) ## PSAgraphics * 2.1.1 2012-03-18 [1] CRAN (R 3.5.0) ## psych 1.8.10 2018-10-31 [1] CRAN (R 3.5.0) ## purrr 0.2.5 2018-05-29 [1] CRAN (R 3.5.0) ## R6 2.3.0 2018-10-04 [1] CRAN (R 3.5.0) ## randomForest 4.6-14 2018-03-25 [1] CRAN (R 3.5.0) ## RColorBrewer 1.1-2 2014-12-07 [1] CRAN (R 3.5.0) ## Rcpp 1.0.0 2018-11-07 [1] CRAN (R 3.5.0) ## readxl 1.1.0 2018-04-20 [1] CRAN (R 3.5.0) ## remotes 2.0.2 2018-10-30 [1] CRAN (R 3.5.0) ## reshape 0.8.8 2018-10-23 [1] CRAN (R 3.5.0) ## reshape2 * 1.4.3 2017-12-11 [1] CRAN (R 3.5.0) ## rgenoud 5.8-2.0 2018-05-04 [1] CRAN (R 3.5.0) ## rio 0.5.16 2018-11-26 [1] CRAN (R 3.5.0) ## rlang 0.3.1 2019-01-08 [1] CRAN (R 3.5.2) ## rmarkdown 1.11 2018-12-08 [1] CRAN (R 3.5.0) ## rpart * 4.1-13 2018-02-23 [1] CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.5.0) ## rstudioapi 0.8 2018-10-02 [1] CRAN (R 3.5.0) ## sandwich 2.5-0 2018-08-17 [1] CRAN (R 3.5.0) ## scales * 1.0.0 2018-08-09 [1] CRAN (R 3.5.0) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.5.0) ## stringi 1.2.4 2018-07-20 [1] CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 [1] CRAN (R 3.5.0) ## strucchange 1.5-1 2015-06-06 [1] CRAN (R 3.5.0) ## survival 2.43-3 2018-11-26 [1] CRAN (R 3.5.0) ## testthat 2.0.1 2018-10-13 [1] CRAN (R 3.5.0) ## TH.data 1.0-9 2018-07-10 [1] CRAN (R 3.5.0) ## tibble 2.0.1 2019-01-12 [1] CRAN (R 3.5.2) ## tidyr 0.8.2 2018-10-28 [1] CRAN (R 3.5.0) ## tidyselect 0.2.5 2018-10-11 [1] CRAN (R 3.5.0) ## TriMatch * 0.9.9 2017-12-06 [1] CRAN (R 3.5.0) ## usethis 1.4.0 2018-08-14 [1] CRAN (R 3.5.0) ## withr 2.1.2 2018-03-15 [1] CRAN (R 3.5.0) ## xfun 0.4 2018-10-23 [1] CRAN (R 3.5.0) ## xtable * 1.8-3 2018-08-29 [1] CRAN (R 3.5.0) ## yaml 2.2.0 2018-07-25 [1] CRAN (R 3.5.0) ## zip 1.0.0 2017-04-25 [1] CRAN (R 3.5.0) ## zoo 1.8-4 2018-09-19 [1] CRAN (R 3.5.0) ## ## [1] /Users/jbryer/R ## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library "],
["introduction.html", "Chapter 1 Introduction 1.1 Counterfactual Model for Causality 1.2 Randomized Control Trials “The Gold Standard” 1.3 Conceptual Steps for Conducting Propensity Score Analysis 1.4 R Primer 1.5 Datasets", " Chapter 1 Introduction The use of propensity score methods (Rosenbaum and Rubin 1983) for estimating causal effects in observational studies or certain kinds of quasi-experiments has been increasing in the social sciences (Thoemmes and Kim 2011) and in medical research (Austin 2008) in the last decade. Propensity score analysis (PSA) attempts to adjust selection bias that occurs due to the lack of randomization. Analysis is typically conducted in two phases where in phase I, the probability of placement in the treatment is estimated to identify matched pairs or clusters so that in phase II, comparisons on the dependent variable can be made between matched pairs or within clusters. R (R Core Team 2016) is ideal for conducting PSA given its wide availability of the most current statistical methods vis-à-vis add-on packages as well as its superior graphics capabilities. This book will provide a theoretical overview of propensity score methods as well as illustrations and discussion of PSA applications. Methods used in phase I of PSA (i.e. models or methods for estimating propensity scores) include logistic regression, classification trees, and matching. Discussions on appropriate comparisons and estimations of effect size and confidence intervals in phase II will also be covered. The use of graphics for diagnosing covariate balance as well as summarizing overall results will be emphasized. Lastly, the extension of PSA methods for multilevel data will also be presented. Figure 1.1: PSA Citations per Year 1.1 Counterfactual Model for Causality In order to understand how propensity score analysis allows us to make causal estimates from observational data, we must first understand the basic principals of causality, particulary the counterfactual model. Figure 1.2 depicts a conterfactual model. We begin with our research subject. This can be a student, patient, rat, asteroid, or any other object we wish to know whether some condition has an effect on. Consider two parallel universes: one where the subject receives condition A and another where they receive condition B. Typically one condition is some treatment whereas the other condition is the absense of that treatment (also referred to as the control). We will use treatment and control throughout this book to refer to these two conditions. Once the individual has been exposed to the two conditions, the outcome is measured. The difference between these outcomes is the true causal effect. However, it is impossible for an object to exist in two universes at the same time, therefore we can never actually observe the true causal effect. Holland (1986) referred to this as the Fundamental Problem of Causal Inference. Figure 1.2: Causal Model 1.2 Randomized Control Trials “The Gold Standard” The randomized experiment has been the goals standard for estimating causal effects. Effects can be estimated using simple means between groups, or blocks in randomized block design. Randomization presumes unbiasedness and balance between groups. However, randomization is often not feasible for many reasons, especially in educational contexts. The strong ignorability assumtion states that an outcome is independent of any observed or unobserved covariates under randomization. This is represented mathematically as: \\[\\left( { Y }_{ i }\\left( 1 \\right) ,{ Y }_{ i }\\left( 0 \\right) \\right) \\bot { T }_{ i }\\] For all \\({X}_{i}\\) Therefore, it follows that the causal effect of a treatment is the difference in an individual’s outcome under the situation they were given the treatment and not (referred to as a counterfactual). \\[{\\delta}_{i} = { Y }_{ i1 }-{ Y }_{ i0 }\\] However, it is impossible to directly observe \\({}_{i}\\) (referred to as The Fundamental Problem of Causal Inference, Holland 1986). Rubin framed this problem as a missing data problem. 1.2.1 Rubin’s Causal Model 1.3 Conceptual Steps for Conducting Propensity Score Analysis 1.3.1 Phase I: Modeling for Selection Bias 1.3.1.1 Estimate Propensity Scores The goal in phase one of PSA is to estimate the probability of being in the treatment. Since in most cases this is a binary outcome, logistic regressionis a common approach to estimating propensity scores. \\[ \\sigma =\\frac { { e }^{ t } }{ { e }^{ t }+1 } =\\frac { 1 }{ 1+{ e }^{ -1 } } \\] \\[ t={ \\beta }+{ \\beta }_{ 1 }x+\\cdots +{ \\beta }_{ k } \\] \\[ F\\left( x \\right) =\\frac { 1 }{ 1+{ e }^{ -\\left( { \\beta }+{ \\beta }_{ 1 }x+\\cdots +{ \\beta }_{ k } \\right) } } \\] Figure 1.3 depicts a fitted logistic regression along with a sample of matchs connected by the purple lines.1 Figure 1.3: Propensity Scores from Logistic Regression with Sample of Matched Pairs 1.3.1.2 Evaluate Balance 1.3.2 Phase II: Estimate Causal Effects 1.4 R Primer R is a statistical software language designed to be extended vis-à-vis packages. As of September 2016, there are currently over 9,000. Given the ease by which R can be extended, it has become the tool of choice for conducting propensity score analysis. This book will make use of a number of packages matching, multiple imputation of missing values, and to visualize results. MatchIt (Ho et al. 2018) Nonparametric Preprocessing for Parametric Causal Inference Matching (Sekhon 2018) Multivariate and Propensity Score Matching Software for Causal Inference multilevelPSA (Bryer 2018a) Multilevel Propensity Score Analysis party (Hothorn et al. 2018) A Laboratory for Recursive Partytioning PSAboot (Bryer 2018b) Bootstrapping for Propensity Score Analysis PSAgraphics [R-PSAgraphics] An R Package to Support Propensity Score Analysis rbounds (Keele 2014) An Overview of rebounds: An R Package for Rosenbaum bounds sensitivity analysis with matched data. rpart (Therneau and Atkinson 2018) Recursive Partitioning TriMatch (Bryer 2017) Propensity Score Matching for Non-Binary Treatments The following command will install the R packages we will use in this book. pkgs &lt;- c(&#39;granova&#39;, &#39;granovaGG&#39;, &#39;Matching&#39;, &#39;MatchIt&#39;, &#39;mice&#39;, &#39;multilevelPSA&#39;, &#39;party&#39;, &#39;PSAboot&#39;, &#39;PSAgraphics&#39;, &#39;rbounds&#39;, &#39;TriMatch&#39;) install.packages(pkgs) 1.5 Datasets 1.5.1 National Supported Work Demonstration (Dehejia and Wahba 1999) (Lalonde 1986) data(lalonde, package=&#39;Matching&#39;) 1.5.2 Lindner Center data(lindner, package=&#39;PSAgraphics&#39;) 1.5.3 Tutoring data(tutoring, package=&#39;TriMatch&#39;) tutoring$treat2 &lt;- tutoring$treat != &#39;Control&#39; 1.5.4 Programme of International Student Assessment (PISA) data(pisana, package=&#39;multilevelPSA&#39;) pisa.usa &lt;- pisana[pisana$Country == &#39;United States&#39;,] 1.5.5 National Medical Expenditure Study data(nmes, package=&#39;TriMatch&#39;) The data in this figure are from the lalonde dataset that will be described at the end of this chapter.↩ "],
["stratification.html", "Chapter 2 Stratification", " Chapter 2 Stratification "],
["matching.html", "Chapter 3 Matching", " Chapter 3 Matching "],
["weighting.html", "Chapter 4 Weighting", " Chapter 4 Weighting "],
["missing-data.html", "Chapter 5 Missing Data", " Chapter 5 Missing Data require(Matching) require(mice) data(lalonde, package=&#39;Matching&#39;) Tr &lt;- lalonde$treat Y &lt;- lalonde$re78 X &lt;- lalonde[,c(&#39;age&#39;,&#39;educ&#39;,&#39;black&#39;,&#39;hisp&#39;,&#39;married&#39;,&#39;nodegr&#39;,&#39;re74&#39;,&#39;re75&#39;)] lalonde.glm &lt;- glm(treat ~ ., family=binomial, data=cbind(treat=Tr, X)) summary(lalonde.glm) ## ## Call: ## glm(formula = treat ~ ., family = binomial, data = cbind(treat = Tr, ## X)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4358 -0.9904 -0.9071 1.2825 1.6946 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.178e+00 1.056e+00 1.115 0.26474 ## age 4.698e-03 1.433e-02 0.328 0.74297 ## educ -7.124e-02 7.173e-02 -0.993 0.32061 ## black -2.247e-01 3.655e-01 -0.615 0.53874 ## hisp -8.528e-01 5.066e-01 -1.683 0.09228 . ## married 1.636e-01 2.769e-01 0.591 0.55463 ## nodegr -9.035e-01 3.135e-01 -2.882 0.00395 ** ## re74 -3.161e-05 2.584e-05 -1.223 0.22122 ## re75 6.161e-05 4.358e-05 1.414 0.15744 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 604.20 on 444 degrees of freedom ## Residual deviance: 587.22 on 436 degrees of freedom ## AIC: 605.22 ## ## Number of Fisher Scoring iterations: 4 Create a copy of the covariates to simulate missing at random (mar) and not missing at random (nmar). lalonde.mar &lt;- X lalonde.nmar &lt;- X missing.rate &lt;- .2 # What percent of rows will have missing data missing.cols &lt;- c(&#39;nodegr&#39;, &#39;re75&#39;) # The columns we will add missing values to # Vectors indiciating which rows are treatment and control. treat.rows &lt;- which(lalonde$treat == 1) control.rows &lt;- which(lalonde$treat == 0) Add missingness to the existing data. For the not missing at random data treatment units will have twice as many missing values as the control group. set.seed(2112) for(i in missing.cols) { lalonde.mar[sample(nrow(lalonde), nrow(lalonde) * missing.rate), i] &lt;- NA lalonde.nmar[sample(treat.rows, length(treat.rows) * missing.rate * 2), i] &lt;- NA lalonde.nmar[sample(control.rows, length(control.rows) * missing.rate), i] &lt;- NA } The proportion of missing values for the first covariate prop.table(table(is.na(lalonde.mar[,missing.cols[1]]), lalonde$treat, useNA=&#39;ifany&#39;)) ## ## 0 1 ## FALSE 0.47640449 0.32359551 ## TRUE 0.10786517 0.09213483 prop.table(table(is.na(lalonde.nmar[,missing.cols[1]]), lalonde$treat, useNA=&#39;ifany&#39;)) ## ## 0 1 ## FALSE 0.4674157 0.2494382 ## TRUE 0.1168539 0.1662921 Create a shadow matrix. This is a logical vector where each cell is TRUE if the value is missing in the original data frame. shadow.matrix.mar &lt;- as.data.frame(is.na(lalonde.mar)) shadow.matrix.nmar &lt;- as.data.frame(is.na(lalonde.nmar)) Change the column names to include &quot;_miss&quot; in their name. names(shadow.matrix.mar) &lt;- names(shadow.matrix.nmar) &lt;- paste0(names(shadow.matrix.mar), &#39;_miss&#39;) Impute the missing values using the mice package set.seed(2112) mice.mar &lt;- mice(lalonde.mar, m=1) ## ## iter imp variable ## 1 1 nodegr re75 ## 2 1 nodegr re75 ## 3 1 nodegr re75 ## 4 1 nodegr re75 ## 5 1 nodegr re75 mice.nmar &lt;- mice(lalonde.nmar, m=1) ## ## iter imp variable ## 1 1 nodegr re75 ## 2 1 nodegr re75 ## 3 1 nodegr re75 ## 4 1 nodegr re75 ## 5 1 nodegr re75 Get the imputed data set. complete.mar &lt;- complete(mice.mar) complete.nmar &lt;- complete(mice.nmar) Estimate the propensity scores using logistic regression. lalonde.mar.glm &lt;- glm(treat~., data=cbind(treat=Tr, complete.mar, shadow.matrix.mar)) lalonde.nmar.glm &lt;- glm(treat~., data=cbind(treat=Tr, complete.nmar, shadow.matrix.nmar)) We see that the two indicator columns from the shadow matrix are statistically significant predictors suggesting that the data is not missing at random. summary(lalonde.mar.glm) ## ## Call: ## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.mar, ## shadow.matrix.mar)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6447 -0.4039 -0.3539 0.5617 0.7840 ## ## Coefficients: (6 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.918e-01 2.512e-01 2.356 0.0189 * ## age 1.477e-03 3.447e-03 0.428 0.6685 ## educ -5.422e-03 1.694e-02 -0.320 0.7490 ## black -5.421e-02 8.927e-02 -0.607 0.5440 ## hisp -1.971e-01 1.166e-01 -1.690 0.0918 . ## married 4.477e-02 6.602e-02 0.678 0.4980 ## nodegr -1.506e-01 7.564e-02 -1.991 0.0471 * ## re74 -5.212e-06 5.870e-06 -0.888 0.3751 ## re75 9.383e-06 9.865e-06 0.951 0.3421 ## age_missTRUE NA NA NA NA ## educ_missTRUE NA NA NA NA ## black_missTRUE NA NA NA NA ## hisp_missTRUE NA NA NA NA ## married_missTRUE NA NA NA NA ## nodegr_missTRUE 6.754e-02 5.868e-02 1.151 0.2504 ## re74_missTRUE NA NA NA NA ## re75_missTRUE -7.267e-03 5.866e-02 -0.124 0.9015 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.2419505) ## ## Null deviance: 108.09 on 444 degrees of freedom ## Residual deviance: 105.01 on 434 degrees of freedom ## AIC: 644.25 ## ## Number of Fisher Scoring iterations: 2 summary(lalonde.nmar.glm) ## ## Call: ## glm(formula = treat ~ ., data = cbind(treat = Tr, complete.nmar, ## shadow.matrix.nmar)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9081 -0.4024 -0.2388 0.5023 0.8679 ## ## Coefficients: (6 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.314e-01 2.422e-01 2.194 0.0287 * ## age 1.099e-04 3.243e-03 0.034 0.9730 ## educ -6.887e-03 1.639e-02 -0.420 0.6745 ## black -4.916e-02 8.364e-02 -0.588 0.5570 ## hisp -1.934e-01 1.105e-01 -1.751 0.0807 . ## married 4.126e-02 6.405e-02 0.644 0.5197 ## nodegr -1.703e-01 7.103e-02 -2.398 0.0169 * ## re74 -7.956e-06 5.694e-06 -1.397 0.1630 ## re75 1.587e-05 9.510e-06 1.669 0.0958 . ## age_missTRUE NA NA NA NA ## educ_missTRUE NA NA NA NA ## black_missTRUE NA NA NA NA ## hisp_missTRUE NA NA NA NA ## married_missTRUE NA NA NA NA ## nodegr_missTRUE 2.251e-01 4.956e-02 4.542 7.22e-06 *** ## re74_missTRUE NA NA NA NA ## re75_missTRUE 2.312e-01 4.917e-02 4.702 3.47e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.2172915) ## ## Null deviance: 108.090 on 444 degrees of freedom ## Residual deviance: 94.305 on 434 degrees of freedom ## AIC: 596.42 ## ## Number of Fisher Scoring iterations: 2 "],
["sensitivity-analysis.html", "Chapter 6 Sensitivity Analysis", " Chapter 6 Sensitivity Analysis require(rbounds) data(lalonde, package=&#39;Matching&#39;) Y &lt;- lalonde$re78 #the outcome of interest Tr &lt;- lalonde$treat #the treatment of interest attach(lalonde) #The covariates we want to match on X = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74) #The covariates we want to obtain balance on BalanceMat &lt;- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74, I(re74*re75)) detach(lalonde) gen1 &lt;- GenMatch(Tr=Tr, X=X, BalanceMat=BalanceMat, pop.size=50, data.type.int=FALSE, print=0, replace=FALSE) mgen1 &lt;- Match(Y=Y, Tr=Tr, X=X, Weight.matrix=gen1, replace=FALSE) summary(mgen1) ## ## Estimate... 1731.4 ## SE......... 729.55 ## T-stat..... 2.3732 ## p.val...... 0.017636 ## ## Original number of observations.............. 445 ## Original number of treated obs............... 185 ## Matched number of observations............... 185 ## Matched number of observations (unweighted). 185 psens(mgen1, Gamma=1.5, GammaInc=.1) ## ## Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value ## ## Unconfounded estimate .... 0.0291 ## ## Gamma Lower bound Upper bound ## 1.0 0.0291 0.0291 ## 1.1 0.0075 0.0868 ## 1.2 0.0017 0.1906 ## 1.3 0.0004 0.3330 ## 1.4 0.0001 0.4917 ## 1.5 0.0000 0.6411 ## ## Note: Gamma is Odds of Differential Assignment To ## Treatment Due to Unobserved Factors ## hlsens(mgen1, Gamma=1.5, GammaInc=.1, .1) ## ## Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate ## ## Unconfounded estimate .... 1362.872 ## ## Gamma Lower bound Upper bound ## 1.0 1362.900 1362.9 ## 1.1 687.870 1363.0 ## 1.2 407.570 1756.2 ## 1.3 131.370 2069.4 ## 1.4 -19.328 2390.3 ## 1.5 -291.830 2662.4 ## ## Note: Gamma is Odds of Differential Assignment To ## Treatment Due to Unobserved Factors ## "],
["bootstrapping.html", "Chapter 7 Bootstrapping", " Chapter 7 Bootstrapping "],
["non-binary-treatments.html", "Chapter 8 Non-Binary Treatments", " Chapter 8 Non-Binary Treatments require(TriMatch) data(tutoring) str(tutoring) ## &#39;data.frame&#39;: 1142 obs. of 17 variables: ## $ treat : Factor w/ 3 levels &quot;Control&quot;,&quot;Treat1&quot;,..: 1 1 1 1 1 2 1 1 1 1 ... ## $ Course : chr &quot;ENG*201&quot; &quot;ENG*201&quot; &quot;ENG*201&quot; &quot;ENG*201&quot; ... ## $ Grade : int 4 4 4 4 4 3 4 3 0 4 ... ## $ Gender : Factor w/ 2 levels &quot;FEMALE&quot;,&quot;MALE&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Ethnicity : Factor w/ 3 levels &quot;Black&quot;,&quot;Other&quot;,..: 2 3 3 3 3 3 3 3 1 3 ... ## $ Military : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ ESL : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ EdMother : int 3 5 1 3 2 3 4 4 3 6 ... ## $ EdFather : int 6 6 1 5 2 3 4 4 2 6 ... ## $ Age : num 48 49 53 52 47 53 54 54 59 40 ... ## $ Employment: int 3 3 1 3 1 3 3 3 1 3 ... ## $ Income : num 9 9 5 5 5 9 6 6 1 8 ... ## $ Transfer : num 24 25 39 48 23 ... ## $ GPA : num 3 2.72 2.71 4 3.5 3.55 3.57 3.57 3.43 2.81 ... ## $ GradeCode : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Level : Factor w/ 2 levels &quot;Lower&quot;,&quot;Upper&quot;: 1 1 1 1 1 2 1 1 1 1 ... ## $ ID : int 377 882 292 215 252 265 1016 282 39 911 ... table(tutoring$treat) ## ## Control Treat1 Treat2 ## 918 134 90 # Histogram of unadjusted grades tmp &lt;- as.data.frame(prop.table(table(tutoring$treat, tutoring$Grade), 1)) ggplot(tmp, aes(x=Var2, y=Freq, fill=Var1)) + geom_bar(position=&#39;dodge&#39;, stat=&#39;identity&#39;) + scale_y_continuous(labels = percent_format()) + xlab(&#39;Grade&#39;) + ylab(&#39;Percent&#39;) + scale_colour_hue(&#39;Treatment&#39;) ## Phase I # Note that the dependent variable is not included in the formula. The TriMatch # functions will replace the dependent variable depending on which pair is # being modeled. tutoring.formu &lt;- ~ Gender + Ethnicity + Military + ESL + EdMother + EdFather + Age + Employment + Income + Transfer + GPA # trips will estimate the propensity scores for each pairing of groups tutoring.tpsa &lt;- trips(tutoring, tutoring$treat, tutoring.formu) plot(tutoring.tpsa, sample=c(200)) # trimatch finds matched triplets. tutoring.matched &lt;- trimatch(tutoring.tpsa) # Partial exact matching tutoring.matched2 &lt;- trimatch(tutoring.tpsa, exact=tutoring$Level) # Plotting the results of trimatch is a subset of the triangle plot with only # points that were matched. There is also an additional parameter, rows, that # will overlay matched triplets. plot(tutoring.matched, rows=1, line.alpha=1, draw.segments=TRUE) ## Examine the unmatched students unmatched &lt;- unmatched(tutoring.matched) summary(unmatched) ## 819 (71.7%) of 1142 total data points were not matched. ## Unmatched by treatment: ## Control Treat1 Treat2 ## 795 (86.6%) 17 (12.7%) 7 (7.78%) plot(unmatched) ## Check balance multibalance.plot(tutoring.tpsa) balance.plot(tutoring.matched, tutoring$Age, label=&#39;Age&#39;) ## ## Friedman rank sum test ## ## data: Covariate and Treatment and ID ## Friedman chi-squared = 4.1498, df = 2, p-value = 0.1256 ## ## Repeated measures ANOVA ## ## Effect DFn DFd F p p&lt;.05 ges ## 2 Treatment 2 294 1.707234 0.1831598 0.006613137 balance.plot(tutoring.matched, tutoring$Military, label=&#39;Military&#39;) ## ## Friedman rank sum test ## ## data: Covariate and Treatment and ID ## Friedman chi-squared = 0.4, df = 2, p-value = 0.8187 # Create a grid of figures. bplots &lt;- balance.plot(tutoring.matched, tutoring[,all.vars(tutoring.formu)], legend.position=&#39;none&#39;, x.axis.labels=c(&#39;C&#39;,&#39;T1&#39;,&#39;T1&#39;), x.axis.angle=0) bplots[[&#39;Military&#39;]] # We can plot one at at time. summary(bplots) # Create a data frame with the statistical results ## Covariate Friedman Friedman.p Friedman.sig rmANOVA rmANOVA.p ## 1 Gender 2.16666667 0.33846543 NA NA ## 2 Ethnicity 0.05678233 0.97200807 NA NA ## 3 Military 0.40000000 0.81873075 NA NA ## 4 ESL 4.78571429 0.09136826 . NA NA ## 5 EdMother 1.55974843 0.45846368 0.76509335 0.4662146 ## 6 EdFather 0.02794411 0.98612510 0.06102055 0.9408158 ## 7 Age 4.14982578 0.12556736 1.70723419 0.1831598 ## 8 Employment 2.04048583 0.36050736 1.27194067 0.2818249 ## 9 Income 0.59582543 0.74236614 0.39251642 0.6757086 ## 10 Transfer 3.08717949 0.21361291 0.55080160 0.5770812 ## 11 GPA 1.37542662 0.50272433 0.49589373 0.6095348 ## rmANOVA.sig ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 plot(bplots, cols=3, byrow=FALSE) ## Phase II # The summary function performs a number of statistical tests including Friedman # rank sum test, repeated measures ANOVA, and if one or both of those tests have # p values less than 0.5 (the default, but configurable), then a pairwise Wilcox # test and three paired t-tests will also be performed. (sout &lt;- summary(tutoring.matched, tutoring$Grade)) ## $PercentMatched ## Control Treat1 Treat2 ## 0.1339869 0.8731343 0.9222222 ## ## $friedman.test ## ## Friedman rank sum test ## ## data: Outcome and Treatment and ID ## Friedman chi-squared = 17.404, df = 2, p-value = 0.0001663 ## ## ## $rmanova ## $rmanova$ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 Treatment 2 294 16.66293 1.396209e-07 * 0.06818487 ## ## $rmanova$`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 Treatment 0.8668353 2.946934e-05 * ## ## $rmanova$`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] ## 2 Treatment 0.8824842 6.035469e-07 * 0.8923995 5.333417e-07 ## p[HF]&lt;.05 ## 2 * ## ## ## $pairwise.wilcox.test ## ## Pairwise comparisons using Wilcoxon signed rank test ## ## data: out$Outcome and out$Treatment ## ## Treat1.out Treat2.out ## Treat2.out 0.0046 - ## Control.out 0.0165 1.9e-06 ## ## P value adjustment method: bonferroni ## ## $t.tests ## Treatments t df p.value sig mean.diff ## 1 Treat1.out-Treat2.out -3.095689 147 2.351743e-03 ** -0.3378378 ## 2 Treat1.out-Control.out 2.939953 147 3.813865e-03 ** 0.4459459 ## 3 Treat2.out-Control.out 5.443253 147 2.140672e-07 *** 0.7837838 ## ci.min ci.max ## 1 -0.5535076 -0.1221681 ## 2 0.1461816 0.7457103 ## 3 0.4992224 1.0683452 ## ## attr(,&quot;class&quot;) ## [1] &quot;trimatch.summary&quot; &quot;list&quot; ls(sout) ## [1] &quot;friedman.test&quot; &quot;pairwise.wilcox.test&quot; &quot;PercentMatched&quot; ## [4] &quot;rmanova&quot; &quot;t.tests&quot; # TODO: boxdiff.plot(tutoring.matched, tutoring$Grade, ordering=c(&#39;Treatment2&#39;,&#39;Treatment1&#39;,&#39;Control&#39;)) parallel.plot(tutoring.matched, tutoring$Grade) # The Loess plot is imperfect with three sets of propensity scores. There is a # model parameter to specify which model to use. Once we a model is selected # we have propensity scores for two of the three groups. We impute a propensity # score on that model&#39;s scale for the third group as the midpoint between # the other two propensity scores that unit was matched to. loess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method=&#39;loess&#39;) # Turn on 95% confidence interval (see also the level parameter) loess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method=&#39;loess&#39;) # We can also pass other parameters to the loess function. loess3.plot(tutoring.matched, tutoring$Grade, se=TRUE, method=&#39;loess&#39;, span=1) # This is a busy plot, but since all the lines are practically vertical, the # distance between each pair of propensity scores is minimal. loess3.plot(tutoring.matched, tutoring$Grade, se=FALSE, method=&#39;loess&#39;, plot.connections=TRUE) # The merge function will add the outcome to the matched triplet data frame. # This is useful for other approaches to analyzing the matched triplets. tmatch.out &lt;- merge(tutoring.matched, tutoring$Grade) head(tmatch.out) ## Treat1 Treat2 Control D.m3 D.m2 D.m1 Dtotal ## 1 368 39 331 0.007053754 0.001788577 0.0103932229 0.01923555 ## 2 800 1088 1105 0.018477707 0.000736057 0.0001821526 0.01939592 ## 3 286 655 853 0.016859948 0.004237243 0.0019476652 0.02304486 ## 4 158 279 365 0.003373585 0.009530680 0.0107118774 0.02361614 ## 5 899 209 100 0.001929173 0.013633300 0.0091835718 0.02474604 ## 6 1034 791 484 0.010538949 0.008541671 0.0092350273 0.02831565 ## Treat1.out Treat2.out Control.out ## 1 4 4 0 ## 2 4 4 3 ## 3 2 4 4 ## 4 4 4 4 ## 5 4 3 4 ## 6 4 4 4 "],
["multilevel-psa.html", "Chapter 9 Multilevel PSA", " Chapter 9 Multilevel PSA "],
["shiny-application.html", "A Shiny Application", " A Shiny Application library(psa) psa_shiny() Figure A.1: PSA Shiny Application "],
["propensity-score-ranges.html", "B Propensity Score Ranges", " B Propensity Score Ranges This function will create a data frame with three variables (a, b, c) for two groups. library(multilevelPSA) getSimulatedData &lt;- function(nvars = 3, ntreat = 100, treat.mean = 0.6, treat.sd = 0.5, ncontrol = 1000, control.mean = 0.4, control.sd = 0.5) { if (length(treat.mean) == 1) { treat.mean = rep(treat.mean, nvars) } if (length(treat.sd) == 1) { treat.sd = rep(treat.sd, nvars) } if (length(control.mean) == 1) { control.mean = rep(control.mean, nvars) } if (length(control.sd) == 1) { control.sd = rep(control.sd, nvars) } df &lt;- c(rep(0, ncontrol), rep(1, ntreat)) for (i in 1:nvars) { df &lt;- cbind(df, c(rnorm(ncontrol, mean = control.mean[1], sd = control.sd[1]), rnorm(ntreat, mean = treat.mean[1], sd = treat.sd[1]))) } df &lt;- as.data.frame(df) names(df) &lt;- c(&quot;treat&quot;, letters[1:nvars]) return(df) } 1:10 (100 treatments, 1000 control units) test.df1 &lt;- getSimulatedData(ntreat = 100, ncontrol = 1000) psranges1 &lt;- psrange(test.df1, test.df1$treat, treat ~ ., samples = seq(100, 1000, by = 100), nboot = 20) plot(psranges1) summary(psranges1) ## p ntreat ncontrol ratio min.mean min.sd min.median ## 1 10 100 100 1 0.15607122 0.0397388795 0.16058282 ## 21 20 100 200 2 0.08333485 0.0193815860 0.07989006 ## 41 30 100 300 3 0.05473508 0.0113995963 0.05285193 ## 61 40 100 400 4 0.03765264 0.0057528081 0.03807210 ## 81 50 100 500 5 0.03037681 0.0035860547 0.03110543 ## 101 60 100 600 6 0.02477044 0.0030294298 0.02485441 ## 121 70 100 700 7 0.02157335 0.0015089751 0.02112071 ## 141 80 100 800 8 0.01844567 0.0009961631 0.01828649 ## 161 90 100 900 9 0.01629556 0.0010700622 0.01611003 ## 181 100 100 1000 10 0.01453930 0.0000000000 0.01453930 ## min.se min.min min.max max.mean max.sd max.median ## 1 0.0088858836 0.05686622 0.21857512 0.8495815 0.05080280 0.8297464 ## 21 0.0043338544 0.05673333 0.12346914 0.7264830 0.03990986 0.7301801 ## 41 0.0025490272 0.03786787 0.08859258 0.6184533 0.05415051 0.6088459 ## 61 0.0012863670 0.02592885 0.04563137 0.5823261 0.03302572 0.5780575 ## 81 0.0008018662 0.02340058 0.03598691 0.5083236 0.03227982 0.4986569 ## 101 0.0006774011 0.01992831 0.03106816 0.4658278 0.02654592 0.4635782 ## 121 0.0003374171 0.01885545 0.02473837 0.4200557 0.01697799 0.4224061 ## 141 0.0002227488 0.01696225 0.02022493 0.3881060 0.01442410 0.3866870 ## 161 0.0002392732 0.01474694 0.01852492 0.3579822 0.01157948 0.3592441 ## 181 0.0000000000 0.01453930 0.01453930 0.3306953 0.00000000 0.3306953 ## max.se max.min max.max ## 1 0.011359852 0.7856924 0.9820647 ## 21 0.008924117 0.6500461 0.7722604 ## 41 0.012108422 0.5396722 0.6978683 ## 61 0.007384776 0.5220826 0.6608008 ## 81 0.007217986 0.4693094 0.6054163 ## 101 0.005935848 0.4048935 0.5112540 ## 121 0.003796394 0.3840361 0.4470703 ## 141 0.003225326 0.3567112 0.4166013 ## 161 0.002589251 0.3339493 0.3817098 ## 181 0.000000000 0.3306953 0.3306953 1:20 (100 treatments, 2000 control units) test.df2 &lt;- getSimulatedData(ncontrol = 2000) psranges2 &lt;- psrange(test.df2, test.df2$treat, treat ~ ., samples = seq(100, 2000, by = 100), nboot = 20) plot(psranges2) summary(psranges2) ## p ntreat ncontrol ratio min.mean min.sd min.median ## 1 5 100 100 1 0.103905726 0.0320556096 0.107142689 ## 21 10 100 200 2 0.045125876 0.0125382053 0.040303937 ## 41 15 100 300 3 0.034023946 0.0084957779 0.033066893 ## 61 20 100 400 4 0.026860409 0.0066813357 0.026095421 ## 81 25 100 500 5 0.020098057 0.0050673591 0.018844209 ## 101 30 100 600 6 0.017303447 0.0036902192 0.016874042 ## 121 35 100 700 7 0.014850775 0.0029970995 0.014724521 ## 141 40 100 800 8 0.011410420 0.0021469387 0.011602856 ## 161 45 100 900 9 0.010778674 0.0019240575 0.010209809 ## 181 50 100 1000 10 0.008542355 0.0012600263 0.008460578 ## 201 55 100 1100 11 0.008948169 0.0011741946 0.009028753 ## 221 60 100 1200 12 0.007678468 0.0011308141 0.007440696 ## 241 65 100 1300 13 0.007376001 0.0012135331 0.007069843 ## 261 70 100 1400 14 0.006403304 0.0007698200 0.006248771 ## 281 75 100 1500 15 0.006221750 0.0007498069 0.006060769 ## 301 80 100 1600 16 0.005658868 0.0005181636 0.005488641 ## 321 85 100 1700 17 0.005413312 0.0005867353 0.005226072 ## 341 90 100 1800 18 0.004848690 0.0003431279 0.004809666 ## 361 95 100 1900 19 0.004577525 0.0001190219 0.004593598 ## 381 100 100 2000 20 0.004375447 0.0000000000 0.004375447 ## min.se min.min min.max max.mean max.sd max.median ## 1 7.167852e-03 0.051733996 0.178661707 0.8951243 3.478463e-02 0.8907102 ## 21 2.803628e-03 0.027059959 0.067355664 0.8151360 4.919688e-02 0.8020895 ## 41 1.899714e-03 0.019288682 0.049139969 0.7498046 3.752869e-02 0.7476146 ## 61 1.493992e-03 0.017716227 0.044316797 0.7062473 4.725246e-02 0.7041710 ## 81 1.133096e-03 0.013626997 0.032692468 0.6571050 5.990009e-02 0.6711006 ## 101 8.251581e-04 0.011040057 0.025770446 0.6066183 5.451294e-02 0.5947387 ## 121 6.701718e-04 0.010300981 0.020151699 0.6020598 5.116134e-02 0.5879645 ## 141 4.800701e-04 0.008269876 0.014782980 0.5243244 4.815199e-02 0.5101789 ## 161 4.302323e-04 0.006437306 0.014494152 0.5387761 5.655747e-02 0.5551983 ## 181 2.817505e-04 0.007002938 0.011406644 0.5189611 6.217013e-02 0.5430891 ## 201 2.625579e-04 0.006892403 0.010947912 0.4666514 6.413993e-02 0.4372149 ## 221 2.528577e-04 0.006019265 0.010610583 0.4594839 7.352566e-02 0.4515450 ## 241 2.713542e-04 0.005574252 0.009946307 0.4440750 6.180433e-02 0.4679452 ## 261 1.721370e-04 0.005383043 0.007842719 0.4082777 6.136882e-02 0.3730742 ## 281 1.676619e-04 0.005379226 0.007813695 0.4459146 2.636240e-02 0.4467309 ## 301 1.158649e-04 0.005038693 0.006718842 0.4438479 1.264566e-02 0.4425390 ## 321 1.311980e-04 0.004800651 0.006998850 0.4088718 4.321938e-02 0.4230782 ## 341 7.672573e-05 0.004405349 0.005980909 0.3941516 4.277788e-02 0.4069567 ## 361 2.661411e-05 0.004344971 0.004747006 0.3956948 2.444378e-02 0.4000337 ## 381 0.000000e+00 0.004375447 0.004375447 0.3879546 1.000000e-15 0.3879546 ## max.se max.min max.max ## 1 0.007778081 0.8178474 0.9455989 ## 21 0.011000757 0.7470076 0.9194607 ## 41 0.008391669 0.6462721 0.8075923 ## 61 0.010565971 0.6124022 0.7838985 ## 81 0.013394067 0.5450304 0.7562037 ## 101 0.012189464 0.5325345 0.7153037 ## 121 0.011440024 0.5061709 0.6861969 ## 141 0.010767112 0.4664472 0.6429069 ## 161 0.012646634 0.4380242 0.6359416 ## 181 0.013901663 0.3933144 0.6091368 ## 201 0.014342124 0.3948888 0.5877631 ## 221 0.016440838 0.3471948 0.5679551 ## 241 0.013819868 0.3431424 0.5334311 ## 261 0.013722486 0.3374251 0.5003817 ## 281 0.005894812 0.3478395 0.4734096 ## 301 0.002827655 0.4097057 0.4613861 ## 321 0.009664146 0.3083789 0.4444204 ## 341 0.009565424 0.2898762 0.4292217 ## 361 0.005465795 0.2958516 0.4181276 ## 381 0.000000000 0.3879546 0.3879546 100 treatments, 1000 control units, equal means and standard deviations test.df3 &lt;- getSimulatedData(ncontrol = 1000, treat.mean = 0.5, control.mean = 0.5) psranges3 &lt;- psrange(test.df3, test.df3$treat, treat ~ ., samples = seq(100, 1000, by = 100), nboot = 20) plot(psranges3) summary(psranges3) ## p ntreat ncontrol ratio min.mean min.sd min.median ## 1 10 100 100 1 0.37856124 0.062570369 0.38668917 ## 21 20 100 200 2 0.23077488 0.035680170 0.22742177 ## 41 30 100 300 3 0.16465951 0.020164413 0.16509354 ## 61 40 100 400 4 0.13823882 0.009759935 0.13421685 ## 81 50 100 500 5 0.10864651 0.009213722 0.10929073 ## 101 60 100 600 6 0.09154158 0.007949813 0.09284312 ## 121 70 100 700 7 0.08432091 0.004808652 0.08480916 ## 141 80 100 800 8 0.07410177 0.003284654 0.07337204 ## 161 90 100 900 9 0.06640652 0.002323468 0.06669179 ## 181 100 100 1000 10 0.06074496 0.000000000 0.06074496 ## min.se min.min min.max max.mean max.sd max.median ## 1 0.0139911598 0.25991916 0.46897513 0.6278589 0.057904214 0.6284763 ## 21 0.0079783285 0.18438195 0.31135111 0.4485870 0.049873337 0.4453040 ## 41 0.0045088998 0.12532383 0.21044030 0.3532457 0.029085840 0.3483755 ## 61 0.0021823879 0.12649290 0.16120741 0.2823354 0.017448137 0.2812174 ## 81 0.0020602509 0.08674195 0.12257616 0.2493659 0.020231935 0.2449267 ## 101 0.0017776321 0.07884013 0.10530340 0.2175840 0.018089099 0.2123910 ## 121 0.0010752472 0.07337723 0.09189387 0.1824542 0.010232130 0.1827490 ## 141 0.0007344709 0.06736893 0.07891444 0.1657798 0.006436274 0.1645657 ## 161 0.0005195433 0.05946719 0.07004452 0.1499796 0.004882924 0.1499828 ## 181 0.0000000000 0.06074496 0.06074496 0.1372959 0.000000000 0.1372959 ## max.se max.min max.max ## 1 0.012947776 0.5366369 0.7331236 ## 21 0.011152017 0.3558560 0.5666023 ## 41 0.006503792 0.2963018 0.3940516 ## 61 0.003901522 0.2518835 0.3124219 ## 81 0.004523998 0.2247264 0.3114824 ## 101 0.004044846 0.1910216 0.2510128 ## 121 0.002287974 0.1675003 0.2009194 ## 141 0.001439195 0.1544713 0.1821468 ## 161 0.001091855 0.1403697 0.1593696 ## 181 0.000000000 0.1372959 0.1372959 100 treatments, 1000 control units, very little overlap test.df4 &lt;- getSimulatedData(ncontrol = 1000, treat.mean = 0.25, treat.sd = 0.3, control.mean = 0.75, control.sd = 0.3) psranges4 &lt;- psrange(test.df4, test.df4$treat, treat ~ ., samples = seq(100, 1000, by = 100), nboot = 20) plot(psranges4) summary(psranges4) ## p ntreat ncontrol ratio min.mean min.sd min.median ## 1 10 100 100 1 1.680737e-05 2.143585e-05 5.855399e-06 ## 21 20 100 200 2 3.577326e-06 5.292484e-06 9.104895e-07 ## 41 30 100 300 3 2.644752e-06 4.098021e-06 1.341479e-06 ## 61 40 100 400 4 1.846465e-06 1.387914e-06 1.254334e-06 ## 81 50 100 500 5 1.240739e-06 1.744646e-06 7.529383e-07 ## 101 60 100 600 6 8.224934e-07 5.510826e-07 7.753760e-07 ## 121 70 100 700 7 7.381685e-07 2.907928e-07 7.255144e-07 ## 141 80 100 800 8 6.653302e-07 1.868277e-07 7.013327e-07 ## 161 90 100 900 9 5.648245e-07 1.675210e-07 5.537482e-07 ## 181 100 100 1000 10 4.946927e-07 0.000000e+00 4.946927e-07 ## min.se min.min min.max max.mean max.sd ## 1 4.793202e-06 3.238000e-12 8.498606e-05 0.9999834 1.892844e-05 ## 21 1.183435e-06 7.483608e-08 1.826946e-05 0.9999611 6.293154e-05 ## 41 9.163453e-07 8.496377e-08 1.875137e-05 0.9999285 7.081525e-05 ## 61 3.103469e-07 5.167579e-07 5.246257e-06 0.9998835 8.973239e-05 ## 81 3.901147e-07 1.363531e-07 7.710479e-06 0.9999016 8.133311e-05 ## 101 1.232258e-07 1.658405e-07 2.710200e-06 0.9998672 6.880183e-05 ## 121 6.502324e-08 2.623251e-07 1.206922e-06 0.9998190 6.145607e-05 ## 141 4.177595e-08 2.944837e-07 9.645725e-07 0.9997837 5.158494e-05 ## 161 3.745883e-08 2.928769e-07 1.002138e-06 0.9997703 5.635962e-05 ## 181 0.000000e+00 4.946927e-07 4.946927e-07 0.9997408 0.000000e+00 ## max.median max.se max.min max.max ## 1 0.9999891 4.232528e-06 0.9999426 1.0000000 ## 21 0.9999893 1.407192e-05 0.9997798 0.9999987 ## 41 0.9999583 1.583477e-05 0.9997394 0.9999958 ## 61 0.9999148 2.006477e-05 0.9996972 0.9999742 ## 81 0.9999137 1.818664e-05 0.9996900 0.9999786 ## 101 0.9998865 1.538456e-05 0.9996680 0.9999559 ## 121 0.9998115 1.374199e-05 0.9997215 0.9999230 ## 141 0.9997739 1.153474e-05 0.9997011 0.9998901 ## 161 0.9997641 1.260239e-05 0.9996974 0.9998626 ## 181 0.9997408 0.000000e+00 0.9997408 0.9997408 100 treat, 1000 control, 10 covariates test.df5 &lt;- getSimulatedData(nvars = 10, ntreat = 100, ncontrol = 1000) psranges5 &lt;- psrange(test.df5, test.df5$treat, treat ~ ., samples = seq(100, 1000, by = 100), nboot = 20) plot(psranges5) summary(psranges5) ## p ntreat ncontrol ratio min.mean min.sd min.median ## 1 10 100 100 1 0.0104884856 6.498074e-03 0.0098358844 ## 21 20 100 200 2 0.0049186831 3.328767e-03 0.0041011637 ## 41 30 100 300 3 0.0026374317 1.052734e-03 0.0028785494 ## 61 40 100 400 4 0.0012844050 6.137909e-04 0.0011461637 ## 81 50 100 500 5 0.0014632821 4.623715e-04 0.0013831576 ## 101 60 100 600 6 0.0009285948 3.794626e-04 0.0008515216 ## 121 70 100 700 7 0.0008486325 2.752827e-04 0.0008668092 ## 141 80 100 800 8 0.0006482525 2.056092e-04 0.0005958190 ## 161 90 100 900 9 0.0005108171 7.469578e-05 0.0005210990 ## 181 100 100 1000 10 0.0004837078 0.000000e+00 0.0004837078 ## min.se min.min min.max max.mean max.sd ## 1 1.453013e-03 0.0009771476 0.0263330509 0.9911056 3.215265e-03 ## 21 7.443349e-04 0.0006605101 0.0122476785 0.9769135 9.676413e-03 ## 41 2.353985e-04 0.0005998227 0.0046293629 0.9676937 8.606312e-03 ## 61 1.372478e-04 0.0005957022 0.0028792261 0.9601771 1.307702e-02 ## 81 1.033894e-04 0.0006240486 0.0023289731 0.9476177 8.635115e-03 ## 101 8.485042e-05 0.0003882062 0.0017774698 0.9422121 9.014796e-03 ## 121 6.155508e-05 0.0003522386 0.0014522111 0.9323131 8.291693e-03 ## 141 4.597561e-05 0.0003615482 0.0011200497 0.9275224 8.438924e-03 ## 161 1.670248e-05 0.0003861774 0.0006429456 0.9172677 8.460324e-03 ## 181 0.000000e+00 0.0004837078 0.0004837078 0.9093487 1.000000e-15 ## max.median max.se max.min max.max ## 1 0.9917753 0.0007189551 0.9836504 0.9954776 ## 21 0.9752686 0.0021637117 0.9638761 0.9954879 ## 41 0.9677400 0.0019244300 0.9470855 0.9812785 ## 61 0.9635344 0.0029241096 0.9333551 0.9768242 ## 81 0.9508925 0.0019308704 0.9322614 0.9633041 ## 101 0.9434032 0.0020157697 0.9252255 0.9589773 ## 121 0.9299196 0.0018540790 0.9209121 0.9481534 ## 141 0.9290814 0.0018870008 0.9121234 0.9398998 ## 161 0.9163185 0.0018917860 0.9009513 0.9327219 ## 181 0.9093487 0.0000000000 0.9093487 0.9093487 "],
["references.html", "References", " References "]
]
